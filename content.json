{"meta":{"title":"CinKate's Blogs","subtitle":"长笛一声人倚楼~","description":"长笛一声人倚楼~","author":"CinKate","url":"http://renxingkai.github.io","root":"/"},"pages":[{"title":"","date":"2021-01-26T01:11:10.597Z","updated":"2021-01-26T01:11:10.597Z","comments":true,"path":"about/index.html","permalink":"http://renxingkai.github.io/about/index.html","excerpt":"","text":"个人信息姓名： 任星凯 性别： 男 出生年月： 1996/02 邮箱： renxingkai0101@163.com QQ： 179049243 教育经历 2018.09–至今 中南大学，计算机学院，硕士 2019.07–至今 国防科技大学，电子科学学院，联合培养 2014.09–2018.06 中北大学，物联网工程，学士 实习经历 2020.06-2020.07 贝壳找房 NLP算法工程师 2020.07-2020.09 联想研究院 AI Lab NLP算法工程师 比赛获奖 时间 比赛 结果 2021.01 2020年CCF BDCI 房产行业聊天问答匹配竞赛 3rd/2985 2020.09 2020年法研杯法律阅读理解竞赛 1st 2020.09 2020 年 CCL 小牛杯幽默情绪识别竞赛 2nd 2020.09 2020 年 CCKS 新冠知识图谱构建与问答评测 2nd 2020.09 2020 年百度人工智能开源大赛 10th/826 2020.08 2020 ICDM Knowledge Graph Contest : Specification 10th 2020.04 中国人工智能大赛·语言与知识技术竞赛（个人赛) 7th/738 2020.02 Kaggle Google QUEST Q&amp;A Labeling 5th/1571 金牌 2020.01 Kaggle TensorFlow 2.0 Question Answering 53rd/1233 银牌 2019.11 汽车论坛消费者用车体验内容的判别与标注 竞赛 5th/837 2019.10 莱斯杯军事阅读理解竞赛 16th/625 2019.10 CCF技术需求匹配竞赛 23rd/862 2019.05 2019年法研杯法律阅读理解竞赛 4th/148 2017.05 蓝桥杯程序设计竞赛(Java) 国家二等奖 2017.04 蓝桥杯程序设计竞赛(Java) 山西省一等奖 2016.11 华北五省计算机应用大赛 国家二等奖 2016.04 蓝桥杯程序设计竞赛(Java) 山西省一等奖 2015.09 全国大学生英语竞赛 三等奖 奖学金 时间 奖项 2020.09 中南大学研究生学业一等奖学金 2019.09 中南大学研究生学业二等奖学金 2018.09 中南大学研究生学业一等奖学金 2017.09 本科生国家奖学金 2017.01 中北大学综合素质一等奖学金 论文 Xingkai Ren, Ronghua Shi, Fangfang Li. Distill BERT to Traditional Models in Chinese Machine Reading Comprehension. AAAI Workshop, 2020"},{"title":"categories","date":"2019-03-19T18:33:09.000Z","updated":"2020-05-17T16:11:59.906Z","comments":true,"path":"categories/index.html","permalink":"http://renxingkai.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-03-19T17:12:23.000Z","updated":"2020-05-17T16:11:59.002Z","comments":true,"path":"tags/index.html","permalink":"http://renxingkai.github.io/tags/index.html","excerpt":"","text":""},{"title":"archives","date":"2019-03-19T17:20:15.000Z","updated":"2020-05-17T16:11:58.945Z","comments":true,"path":"archives/index.html","permalink":"http://renxingkai.github.io/archives/index.html","excerpt":"","text":""}],"posts":[{"title":"智能搜索和推荐系统第三章--知识图谱","slug":"SearchAndRec-Chapter03","date":"2021-05-08T15:45:42.000Z","updated":"2021-05-08T07:46:48.381Z","comments":true,"path":"2021/05/08/SearchAndRec-Chapter03/","link":"","permalink":"http://renxingkai.github.io/2021/05/08/SearchAndRec-Chapter03/","excerpt":"","text":"1.介绍在谷歌发布的文档中有明确的描述，知识图谱是一种用图模型来描述知识和建模世界万物之间关联关系的技术方法。知识图谱还是比较通用的语义知识的形式化描述框架，它用节点表示语义符号，用边表示语义之间的关系，如下图所示。在知识图谱中，人、事、物通常被称作实体或本体。 知识图谱的组成三要素包括：实体、关系和属性。 实体：又叫作本体（Ontology），指客观存在并可相互区别的事物，可以是具体的人、事、物，也可以是抽象的概念或联系。实体是知识图谱中最基本的元素。 关系：在知识图谱中，边表示知识图谱中的关系，用来表示不同实体间的某种联系。如图3-1所示，图灵和人工智能之间的关系，知识图谱和谷歌之间的关系，谷歌和深度学习之间的关系。 属性：知识图谱中的实体和关系都可以有各自的属性，如图下图所示。 2.知识图谱的架构知识图谱的架构涉及知识表示、知识获取、知识处理和知识利用等多个方面。一般情况下，知识图谱构建流程如下：首先确定知识表示模型，然后根据不同的数据来源选择不同的知识获取手段并导入相关的知识，接着利用知识推理、知识融合、知识挖掘等技术构建相应的知识图谱，最后根据不同应用场景设计知识图谱的表现方式，比如：语义搜索、智能推荐、智能问答等。 知识源：包括结构化数据、非结构化数据和半结构化数据。 信息抽取：就是从各种类型的数据源中提取实体、属性以及实体间的相互关系，在此基础上形成本体的知识表述。知识图谱的构建过程中存在大量的非结构化或者是半结构化数据，这些数据在知识图谱的构建过程中需要通过自然语言处理的方法进行信息抽取。从这些数据中，我们可以提取出实体、关系和属性。 知识融合：主要工作是把结构化的数据以及信息抽取提炼到的实体信息，甚至第三方知识库进行实体对齐和实体消歧。这一阶段的输出应该是从各个数据源融合的各种本体信息。 知识加工：知识加工阶段如图3-6所示，其中知识推理中重要的工作就是知识图谱的补全。常用的知识图谱的补全方法包括：基于本体推理的补全方法、相关的推理机制实现以及基于图结构和关系路径特征的补全方法。 从逻辑上，我们可以将知识图谱划分为两个层次：数据层和模式层。数据层可以是以事实为单位存储的数据库，可以选用的图数据库有RDF4j、Virtuoso、Neo4j等三元组。&lt;实体，关系，实体&gt;或者&lt;实体，属性，属性值&gt;可以作为基本的表达方式，存储在图数据库中。模式层建立在数据层之上，是知识图谱的核心。通常，通过本体库来管理数据层，本体库的概念相当于对象中“类”的概念。借助本体库，我们可以管理公理、规则和约束条件，规范实体、关系、属性这些具体对象间的关系。 3.关系抽取对于关系抽取，我们同样可以选择两种方式：第一种是基于模式或规则的方式。这种方式很好理解，比如，X表示一个人物实体，Y表示一个时间实体，那么X与Y的时间关系就有很多种形式。如“X出生于Y”“X死于Y”等。“出生”“死亡”就是X与Y之间的两种具体关系。这些关系可以通过模式被定义出来。 基于模式的关系抽取 基于学习的关系抽取。基于学习的抽取包括基于监督学习的方式、基于远程监督学习的方式及基于深度学习的方式。基于监督学习的方式需要大量的人工标注及文本特征工程，而基于深度学习的方式可以减少文本特征工程。","categories":[{"name":"搜索&推荐","slug":"搜索-推荐","permalink":"http://renxingkai.github.io/categories/搜索-推荐/"}],"tags":[{"name":"search&recommend","slug":"search-recommend","permalink":"http://renxingkai.github.io/tags/search-recommend/"}],"author":"CinKate"},{"title":"智能搜索和推荐系统第二章--搜索系统","slug":"SearchAndRec-Chapter02","date":"2021-04-26T21:44:20.000Z","updated":"2021-04-26T13:51:58.043Z","comments":true,"path":"2021/04/27/SearchAndRec-Chapter02/","link":"","permalink":"http://renxingkai.github.io/2021/04/27/SearchAndRec-Chapter02/","excerpt":"","text":"仅用于学习!!! 1.介绍搜索引擎是指根据一定的策略、运用特定的计算机程序从互联网上搜集信息，在对信息进行组织和处理后，将用户检索到的相关信息展示给用户，为用户提供检索服务。搜索引擎包括4个接口，分别是搜索器、索引器、检索器和用户接口。 搜索器的功能是在互联网中漫游，负责发现和搜集信息。 索引器的功能是理解搜索器所搜索的信息，从中抽取出索引项，输出用于表示文档以及生成文档库的索引表。 检索器的功能是根据用户的查询在索引库中快速检出文档，并进行文档与查询的相关度评价，对将要输出的结果进行排序，实现某种用户相关性反馈机制。 用户接口的功能是输入用户查询、显示查询结果、提供用户相关性反馈机制。 具体的搜索引擎架构示意图如图2-1所示。 2.搜索引擎的分类搜索引擎可以分为以下4类：全文搜索引擎、元搜索引擎、垂直搜索引擎和目录搜索引擎。下面对这4类搜索引擎进行具体介绍。 全文搜索引擎。计算机通过扫描文章中的每个词，对每个词建立索引，记录词汇在文章中出现的次数和位置信息。当用户进行查询时，计算机按照事先建立好的索引进行查找，并将结果反馈给用户。按照数据结构的不同，全文搜索可以分为结构化数据搜索和非结构化数据搜索。对于结构化数据，全文搜索一般是通过关系型数据库的方式进行存储和搜索，也可以建立索引。对于非结构化数据，全文搜索主要有两种方法：顺序扫描和全文检索。顺序扫描，顾名思义，按照顺序查询特定的关键字，这种方式耗时且低效；全文检索需要提取关键字并建立索引，因此，搜索到的信息过于庞杂，用户需要逐一浏览并甄别所需信息。在用户没有明确检索意图情况下，全文检索方式效率稍显不足。Google和百度都是典型的全文搜索引擎。 元搜索引擎。按照功能划分，搜索引擎可以分为元搜索引擎（Meta Search Engine）和独立搜索引擎（Independent Search Engine）。元搜索引擎是一种调用其他独立搜索引擎的搜索引擎，其能对多个独立搜索引擎进行整合、调用并优化结果。独立搜索引擎主要由网络爬虫、索引、链接分析和排序等部分组成；元搜索引擎由请求提交代理、检索接口代理、结果显示代理三部分组成，不需要维护庞大的索引数据库，也不需要爬取网页。元搜索引擎具体实现逻辑如图2-2所示。请求提交代理就是将请求分发给独立搜索引擎。元搜索引擎可以按照用户需求和偏好请求实际需要调用的独立搜索引擎，该方式能够有效提升用户查询的准确率和响应效率。检索接口代理是将查询内容转化成独立搜索引擎能够接受的模式，并且保证不会丢失必需的语义信息。结果显示代理是元搜索引擎按照用户的需求采用不同的排序方式对结果进行去重、排序。元搜索引擎常用的排序方式有：相关度排序、时间排序、搜索引擎排序等。元搜索引擎的整体工作流程如下：用户通过网络访问元搜索引擎并向服务器发出查询，服务器接收到查询内容后，先访问结果数据库，查询近期记录中是否存在相同的查询，如果存在，返回结果；如果没有，将查询进行处理后分发到多个独立搜索引擎，并集中各搜索引擎的查询结果，结合排序方式对结果进行排序，生成最终结果并返给用户，同时保存现有结果到数据库中，以备下次查询使用。保存的查询结果有一定的生存期，超过一定时间的记录就会被删除，以保证查询结果的时效性。 垂直搜索引擎。垂直搜索引擎是针对某个行业的专业搜索引擎，是搜索引擎的细分和延伸，对特定人群、特定领域、特殊需求提供服务。它的特点是专业、精确和深入。垂直搜索引擎将搜索范围缩小到极具针对性的具体信息。垂直搜索引擎的结构与通用搜索系统类似，主要由三部分构成：爬虫、索引和搜索。但垂直搜索的表现方式与Google、百度等搜索引擎在定位、内容、用户等方面存在一定的差异，所以它不是简单的行业搜索引擎。用户使用通用搜索引擎时，通常是通过关键字进行搜索，该搜索方式一般是语义上的搜索，返回的结果倾向于文章、新闻等，即相关知识。垂直搜索的关键字搜索是放到一个行业知识的上下文中，返回的结果是消息、条目。对于有购房需求的人来说，他们希望得到的信息是供求信息而不是关于房子的文章和新闻。 目录搜索引擎。目录搜索引擎是网站常用的搜索方式，类似于书本章节目录。该搜索方式是对网站信息整合处理并分目录呈现给用户，整合处理的过程一般需要人工维护，更新速度较慢，而且用户需要事先了解网站的基本内容，熟悉主要模块，所以应用场景越来越少。 3.推荐系统协同过滤目前，基于协同过滤的推荐是推荐系统中应用最广泛、最有效的推荐策略。它于20世纪90年代出现，促进了推荐系统的发展。协同过滤的基本思想是聚类。比如，如果周围很多朋友选择了某种商品，那么自己大概率也会选择该商品；或者用户选择了某种商品，当看到类似商品且其他人对该商品评价很高时，则购买这个商品的概率就会很高。协同过滤又分为三种：基于用户的协同过滤、基于项目的协同过滤和基于模型的协同过滤。 1）基于用户的协同过滤的基本思想是首先找到与目标用户兴趣相似的用户集合，然后找到这个集合中用户喜欢并且没有听说过的物品推荐给目标用户。下图是基于用户的协同过滤的实现逻辑。用户A喜欢商品A和商品C，用户C喜欢商品A、商品C和商品D，用户A和用户C具有相似的兴趣爱好，因此把商品D推荐给用户A。 2）基于项目的协同过滤的基本思想是基于所有用户对推荐对象的评价的推荐策略。如果大部分用户对一些推荐对象的评分较为相似，那么当前用户对这些推荐对象的评价也相似。然后，将相似推荐对象中用户未进行评价的商品推荐给用户。总之，基于项目的协同过滤就是根据用户对推荐对象的评价，发现对象间的相似度，根据用户的历史偏好将类似的商品推荐给该用户。图2-8是基于项目的协同过滤的实现逻辑。用户A喜欢商品A和商品C，用户B喜欢商品A、商品B和商品C，用户C喜欢商品A，通过这些用户的喜好可以判定商品A和商品C相似，喜欢商品A的用户同时也喜欢商品C，因此给喜欢商品A的用户C也推荐了商品C。 3）基于模型的协同过滤的基本思想是基于样本用户的喜好信息训练一个推荐模型，然后根据实时的用户喜好信息进行推荐。其和上述两种协同推荐的不同点在于先对已有数据应用统计和机器学习的方法得到模型，再进行预测。常用的方法有机器学习方法、统计模型、贝叶斯模型和线性回归模型等。 基于协同过滤推荐的优点有：1）可以使用在复杂的非结构化对象上；2）能够发现用户新的兴趣爱好，给用户带来惊喜；3）以用户为中心的自动推荐，随着用户数量的增加，用户体验也会越来越好。缺点在于：1）存在冷启动问题，即在没有大量用户数据的情况下，用户可能不满意获得的推荐结果；2）存在稀疏性问题，即用户大量增长的同时，评价差异性会越来越大，推荐对象也越来越多，导致大量的推荐对象没有经过用户评价，部分用户无法获得推荐结果，部分推荐对象无法被推荐。","categories":[{"name":"搜索&推荐","slug":"搜索-推荐","permalink":"http://renxingkai.github.io/categories/搜索-推荐/"}],"tags":[{"name":"search&recommend","slug":"search-recommend","permalink":"http://renxingkai.github.io/tags/search-recommend/"}],"author":"CinKate"},{"title":"No Answer is Better Than Wrong Answer A Reflection Model for Document Level Machine Reading Comprehension 笔记","slug":"emnlp2020-NoAnswerisBetter","date":"2021-04-19T17:29:46.000Z","updated":"2021-04-22T07:03:45.608Z","comments":true,"path":"2021/04/20/emnlp2020-NoAnswerisBetter/","link":"","permalink":"http://renxingkai.github.io/2021/04/20/emnlp2020-NoAnswerisBetter/","excerpt":"","text":"论文链接AbstractNatural Questions（NQ）数据集给机器阅读理解带来了新的挑战：答案不仅具有不同的粒度（长、短），而且具有更丰富的类型（包括无答案、是/否、单span和多span）。本文针对这一挑战，系统地处理了各种类型的答案。特别是，我们提出了一种新的方法称为Reflection Net，它利用两步训练过程来识别无答案和错误答案的情况。通过大量实验验证了该方法的有效性。在撰写论文时（5月）。2020年12月20日），我们的方法在长答案和短答案排行榜*上均获得前1名，F1得分分别为77.2和64.1。 1 IntroductionNatural Questions(NQ) 数据集的答案提供了两级粒度：长答案和短答案；因此需要模型去在文档和段落级别寻找答案，除了长短答案，还有无答案样本，多span短答案样本，YES/NO答案类型。一些研究人员提出了pipeline方式去抽取短答案，先对长答案进行抽取，然后再抽取短答案。虽然这种方法是合理的，但由于长答案和短答案是分开建模的，因此可能会失去它们之间固有的相关性。还有其他使用联合训练长短答案的方法，以前的方法已经被证明能有效地提高NQ任务的性能，但是很少有工作关注这个QA集合中丰富答案类型的挑战。我们注意到，51%的问题在NQ集中没有答案，因此，模型准确预测何时输出答案是至关重要的。对于其他答案类型，如多span答案或yes/no答案，尽管它们在NQ集合中的百分比很小，但不应忽略它们。相反，在实践中，更倾向于一种能够很好地处理各种答案类型的系统设计。 本文中，我们着重处理无答案类型，我们首先训练所有答案类型的MRC模型，然后，利用训练好的MRC模型对所有训练数据进行推理，训练出第二个模型，称为Reflection model，以预测的答案、上下文和MRC头部特征作为输入，预测出更准确的置信度，从而区分正确答案和错误答案。使用二阶段训练有三个原因： 首先，MRC置信度计算的常用方法是基于logits的启发式方法，这种方法不规范，不同问题之间的可比性不强 其次，在训练长文档MRC模型时，由于负实例比正实例多，所以对负实例进行了大量的下采样。但在预测时，MRC模型需要对所有实例进行推理。这种训练数据分布差异和预测结果表明，MRC模型可能会被一些负面实例所迷惑，并用高置信度的分数预测错误答案。 第三，MRC模型学习了问题的表示、类型和答案之间的关系，而答案不知道预测答案的正确性。 我们的第二阶段模型解决了这三个问题，类似于成为其名称来源的反射过程。通过大量实验验证了该方法的有效性。在撰写论文时（5月）。2020年12月20日），我们的方法在长答案和短答案排行榜*上均获得前1名，F1得分分别为77.2和64.1。 2 ApproachReflection模型结构如图1所示，由MRC模型和Reflection模型构成，分别用于答案预测和答案置信度预测。 2.1 MRC Model使用预训练模型作为MRC模型，滑动窗口用于处理长文本文章，然后将问题与文章片断组成一起去构造一个样本，正样本片段中包含答案，负样本片段中不含答案，由于长文本中负样本较多，我们对负样本使用下采样。 MRC模型的输出为span和答案类型，包含$l=(t,s,e,ms)$，t是答案类型，s和e是答案开始结束位置，答案类似是多span时，我们使用BIO去标注多答案(ms)，使用Transformer作为Encoder，最终得到$h(x)$ 答案类型分类使用[CLS]进行分类，单答案span使用最小化开始和结束的位置进行，多答案类型使用序列标注思想，直接将隐状态通过线性层进行分类BIO，并没使用CRF解码。最终MRC模型的loss: 除了预测答案，MRC模型需要输出置信分数， xs,xe,x1是被预测的开始、结束和[CLS]字符 2.2 Reflection ModeReflection Mode的目标是一个更精确的置信度得分，区分正确答案和两种错误答案（见第3.4节）。第一种方法是预测一个has-ans问题的错误答案，第二种方法是预测一个no-ans问题的任何答案 Training Data Generation为了生成Reflection Model的训练数据，我们使用训练后的MRC 模型去推理全量数据 对于属于每一个问题的所有实例(feature)，我们只根据其置信度得分选择预测答案前1名的实例。 所选实例、MRC预测答案、其对应的如下头部特征和正确性标签（如果预测答案与真实答案相同，则标签为1；否则为0）一起成为反射模型的训练案例。 Model Training 如图1(a)所示，我们初始化反射模型使用MRC训练好的参数，学习率比MRC模型的学习率小几倍。为了收到重要的MRC模型的状态信息，我们从MRC模型的top层抽取顶层特征当它预测答案时，如表2所示。 顶层特征与[cls]标记的隐藏表示连接，然后是用于最终置信度预测的隐藏层。 反射模型将所选实例x和预测答案作为输入。具体地说，我们创建了一个以答案类型和答案位置标记为元素的字典Ans。我们将应答类型标记添加到[cls]字符中，将位置标记添加到相应的位置字符中，并将空标记添加到其他字符中。 反射模型的隐藏层表示为： 然后和顶层特征拼接[CLS]字符表示$h^r(x_1)$，如下式： 最后获得的置信度分数和二分类损失如下： 式中，如果MRC模型的预测答案（基于x）正确，则y=1，否则为0。对于推理，MRC模型需要为每个问题预测一个文档的所有滑动窗口实例，而反射模型只需要推理一个包含MRC模型预测的最终答案的实例。因此反射模型的计算量很小 3 ExperimentsNQ 训练集307,373；验证集7,830;测试集7842 for lb. 3.1 Implementation先在SQuAD 2.0上ft一遍，再ft NQ 3.2 Baselines DocumentQA DecAtt + DocReader BERTjoint 4 Ablation Study 5 Related WorkMRCAnswer Verifier6 Conclusion本文提出了一种系统的方法来处理MRC中的丰富答案类型。特别是，我们开发了一个Reflection Model 来处理无答案/错误答案的情况。其关键思想是根据预测答案的内容、上下文和状态，训练第二阶段模型，预测预测答案的置信度。实验表明，该方法在NQ集上达到了最新的结果。由F1和R@P=90在长、短的回答上，我们的方法都超过了之前的SOTA。消融研究也证实了我们的方法的有效性。","categories":[{"name":"机器阅读理解","slug":"机器阅读理解","permalink":"http://renxingkai.github.io/categories/机器阅读理解/"}],"tags":[{"name":"MRC NQ","slug":"MRC-NQ","permalink":"http://renxingkai.github.io/tags/MRC-NQ/"}],"author":"CinKate"},{"title":"Multi-Task Learning with Multi-View Attention for Answer Selection and Knowledge 笔记","slug":"paper-Multi-TaskLearningwithMulti-ViewAttentionforAnswerSelectionandKnowledge","date":"2021-04-19T17:12:54.000Z","updated":"2021-04-19T09:15:17.033Z","comments":true,"path":"2021/04/20/paper-Multi-TaskLearningwithMulti-ViewAttentionforAnswerSelectionandKnowledge/","link":"","permalink":"http://renxingkai.github.io/2021/04/20/paper-Multi-TaskLearningwithMulti-ViewAttentionforAnswerSelectionandKnowledge/","excerpt":"","text":"论文链接Abstract答案选择和知识库问答（KBQA）是问答（QA）系统的两个重要任务。现有方法分别分开解决这两个任务，这需要大量的重复工作，而忽略了任务之间的丰富的相关信息。在本文中，我们基于以下动机，通过多任务学习（MTL）同时解决答案选择和KBQA任务。首先，答案选择和KBQA都可以视为排序问题，一个在文本级别，另一个在知识级别。其次，这两个任务可以互惠互利：答案选择可以结合知识库（KB）中的外部知识，而KBQA可以通过从答案选择中学习上下文信息来进行改进。为了实现共同学习这两个任务的目标，我们提出了一种新颖的多任务学习方案，该方案利用从各个角度学习到的多视图注意力来使这些任务能够相互交互以及学习更全面的句子表示形式。在多个真实的数据集上进行的实验证明了该方法的有效性，并提高了答案选择和KBQA的性能。同样，从不同的表示角度来看，多视图注意力方案在组合注意力信息方面被证明是有效的。 Introduction多任务学习在NLP领域是广泛的，然而QA中的MTL研究较少。本文中，我们探索了利用MTL去同时处理答案选择和KBQA，这两种任务都可以视为排序任务，一个在答案文本级别，另一个是知识级别。答案选择是在多个答案片段中选出一个最合适的答案，KBQA专注于在KB中抽取出相关的知识，去回答问题。大多多任务学习模块，划分共享层和特殊任务层，共享层在各个任务中共享信息，特殊任务层对不同任务是不同的。大多模型忽视了共享层和特殊任务层之间的交互，并且忽视了不同任务之间的交互。因此，本文提出了MTL从不同方面去学习多视角的attention，能够使不同的任务互相交互。具体而言，我们从任务特定的层中收集注意力信息，以在共享层中学习更全面的句子表示形式。 此外，多视图注意力机制通过结合单词级和知识级信息来增强句子的表示学习。 也就是说，通过使用多视图注意力机制，可以在不同任务之间共享和传输单词级和知识级的注意力信息。 根据实验，与单独答案选择和KBQA任务相比，联合学习可以显着提高每个任务的性能。 实验结果还表明了多视图注意力方案的有效性，并且每个视图的注意力都做出了贡献。本文主要贡献如下： 我们探索用于选择答案和知识库问题解答的多任务学习方法。 知识级别的KBQA任务可以改善答案选择任务，而词汇级别的答案选择任务可以增强KBQA任务。 我们提出了一种新颖的多任务学习方案，该方案利用多视图注意力机制来桥接不同的任务，该任务将特定于任务的层的重要信息集成到共享层中，并使模型能够交互式地学习单词级别和知识 级表示。 实验结果表明，答案选择和KBQA的多任务学习优于最新的单任务学习方法。 此外，基于多视图注意力的MTL方案进一步提高了性能。 Multi-Task Learning for Question Answering 图一为多任务QA网络的架构(包含AS和KBQA)。 Task-specific Encoder Layer 首先将预处理后的句子编码成向量表示。不同的QA任务在数据分布和底层表示上应该是不同的。因此，每个任务都配备了一个用于问答的特定任务编码器，每个特定任务编码器都包含一个单词编码器和一个知识编码器来学习完整的句子表示，如图2所示 Word Encoder. 输入为词向量，使用BiLSTM去捕获全文信息，得到Hw。 Knowledge Encoder. 由于知识序列是由一系列标记化的实体或关系名称组成的，因此后一种学习过程需要基于高级知识的表示。针对这一问题，我们将CNN应用到知识序列上，其中大小为n的filters在知识嵌入矩阵上滑动以捕获局部n-gram特征，每次移动都会计算一个隐藏层向量。由于实体长度不确定，因此，会使用不同大小的卷积核来计算，然后使用Dense层来获取知识表示Hk 将Hk和Hw进行拼接，$H_q=[H_{W_q}:H_{K_q}]$，$H_a=[H_{W_a}:H_{K_a}]$，分别为问题和答案的隐藏表示。 Shared Representation Learning Layer与任务特定编码层的输入相比，整句表示具有更丰富的语义，与其他任务的分布更相似。因此，我们整合来自所有任务的编码向量，并通过一个高级共享的Siamese-Bi-LSTM来生成最终的QA表示$S_q$和$S_a$，然后进行平均池化，并且使用了一些词级别特征$x_{ol}$，最终表示为$x=[s_q,s_a,x_{ol}]$ Task-specific Softmax Layer 使用softmax最终分类。 Multi-Task Learning 最小化以上目标函数，$\\lambda$为不同任务的权重 Multi-Task Model with Multi-View Attention为了增强潜在表征空间中不同QA任务之间的交互作用，我们提出了一种多视角注意机制，从任务特定层和共享层提取重要信息。 Multi-View Attention Scheme如图3所示，与其他注意共享方案不同的是，我们不仅从任务特定层提取注意力，而且还将来自共享层的信息进行组合。此外，我们从词汇和知识两个角度获得注意信息，因为词汇水平和知识水平的信息可能共同促进表示学习。具体来说，我们计算了五种注意观，包括词、知识、语义、知识语义和共同注意力。 Semantic View &amp; Knowledge Semantic View使用mean/max池化去获取句子的语义表示 ExperimentDatasets &amp; PreprocessingAS: YahooQA,TREC QAKBQA: SimpleQuestions,WebQSP Multi-Task Learning Results Related WorkAnswer SelectionMulti-Task LearningConclusion本文研究了同时解决答案选择和知识库问答问题的多任务学习方法。我们提出了一种新的多任务学习方案，该方案利用从不同角度学习的多视角注意，使这些任务能够相互作用，并学习更全面的句子表示，包括词视角、知识视角、语义视角、知识语义视角和共注意力视角。在几个广泛使用的QA基准数据集上进行的实验表明，答案选择和知识库问答的联合学习方法明显优于单任务学习方法。同时，多视角注意策略能有效地从不同的表征视角收集注意信息，提高整体表征学习效果。","categories":[{"name":"机器阅读理解","slug":"机器阅读理解","permalink":"http://renxingkai.github.io/categories/机器阅读理解/"}],"tags":[{"name":"MTL MRC","slug":"MTL-MRC","permalink":"http://renxingkai.github.io/tags/MTL-MRC/"}],"author":"CinKate"},{"title":"Text Style Transfer via Learning Style Instance Supported Latent Space 阅读笔记","slug":"ijcai2020-StyIns","date":"2021-02-06T21:41:25.000Z","updated":"2021-02-06T13:47:23.342Z","comments":true,"path":"2021/02/07/ijcai2020-StyIns/","link":"","permalink":"http://renxingkai.github.io/2021/02/07/ijcai2020-StyIns/","excerpt":"","text":"本文为biendata视频笔记，仅用来学习，侵删。 开源地址 Background风格转换：在不改变原来句子语义情况下，将原句子x和目标风格sj输入，生成含有目标风格sj的句子 Applications 文本润色，将非正式句子转为正式句子 评论、对话的自动生成 风格特征写作 Previous Paradigms风格转换，需要表示句子内容和target style。 解耦方式(Distentage)。将encoder之后的隐状态，拆出风格表示特征c和文本表示特征h，将h和target style同时输入到decoder中，生成y。这种方式的缺点：内容保留不太好；优点：风格表示容易控制、灵活的。 基于Attention的Seq2Seq。使用Seq2Seq结构，使用attention将x和y去对齐，并且在生成y时候，使用目标风格style embedding去监督生成对应风格的句子。cycle loss:先用目标风格sj监督x生成y，然后再用先前风格si将y再生成回去x’，x’和x计算相似loss。可以保证迁移之后的句子仍然保留原来的语义。同时使用判别器去判别迁移后的y是否真的有目标风格，促进风格迁移生成！优点：保留词级别的信息，几乎没有语义丢失。缺点：风格的信号比较差，风格迁移效果不太好。 Multi-Generator。通过不同generator来实现不同风格上的style生成。相当于每个decoder就代表了各自的风格。为了保留内容，在使用另一个decoder重新构造生成x’。优点：效果很好！缺点：资源需求比较多，需要多个seq2seq。 Locate and Replace。找出句子中和风格相关的词，进行替换，保留和风格无关的词。优点：较为精确，能够实现较好的风格转换和内容保留。缺点：需要一个风格词表；对于超出词义的风格迁移，效果不好，上升到更深的语义级别效果不好！ Motivation结合Attention-based Seq2Seq和潜在风格空间Latent Style Space，去构建更好的风格迁移和内容保留。 Methodology之前的VAE方法，假设每个句子是独立的，每个句子都去单独输出隐状态风格z，不能充分考虑全局信息 K为K个句子都包含同样的风格j。一个原句子encoder编码原句子x为H，一个风格encoder(将K个句子都包含同样的风格j的句子集)进行风格编码为z，一个decoder对H和z进行解码。 style encoder使用生成式流模型，假设初始为高斯分布z0，经过多次复杂变换之后，生成更为复杂的分布zt，能更充分表示风格。 TrainingUnsupervised Training Cycle Consistency Loss:先将x转为y再转回去x’，计算x’和x的交叉熵。 Reconstruction Loss:给定原句x和原句风格s，希望能重新生成原句x Adversarial Style Loss:提供style相关的判别对抗lossSemi-Supervised Training ExperimentDataset Metrics Results 人工评测 Cases Conclusion","categories":[{"name":"文本生成","slug":"文本生成","permalink":"http://renxingkai.github.io/categories/文本生成/"}],"tags":[{"name":"文本风格迁移","slug":"文本风格迁移","permalink":"http://renxingkai.github.io/tags/文本风格迁移/"}],"author":"CinKate"},{"title":"CoCon A Self Supervised Approach for Controlled Text Generation 阅读笔记","slug":"CoCon","date":"2021-02-01T16:30:22.000Z","updated":"2021-02-01T08:31:57.396Z","comments":true,"path":"2021/02/02/CoCon/","link":"","permalink":"http://renxingkai.github.io/2021/02/02/CoCon/","excerpt":"","text":"论文链接Abstract基于Transformer的LM展现了显著的自然语言生成能力。由于其巨大的潜力，控制这些LMs的生成文本正受到关注。虽然已经有了不少研究去控制被生成文本的高级属性(情感、主题等)，但仍然缺少对内容上词级别、短语级别的准确控制。本文中，我们提出了Content-Conditioner(CoCon)在细粒度级别，去使用目标内容控制一个语言模型的输出。在我们的自我监督的方法中，CoCon学习帮助LM完成一个部分观察到的文本序列，通过调节LM中保留的内容输入。通过实验，我们证明了CoCon可以自然地将目标内容合并到生成的文本中，并以Zero-shot的方式控制高级文本属性。 1 Introduction基于Transformer的LM在海量语料上进行训练，然后去预测下一个token通过对数似然损失(log-likehood)。控制LM的输出逐渐吸引了人们的注意。像从零开始训练一个修改过的LM来合并目标文本属性[这样的方法成本很高，而对特定属性的预训练LM进行微调则限制了文本控制的范围。PPLM没有修改LM的架构，通过属性来控制被生成的文本。尽管在控制诸如主题和情感之类的高级文本属性方面是有效的，但是相同的目标属性可能会生成在单词和短语级别具有显著不同的内容文本样本，从而为对LM生成的文本的内容进行更细粒度的控制留下一个gap。为了举例说明情绪控制的例子，一篇以“彼得”这样的名字的提示文本开头的文章后面可以是描述彼得性格的文本，关于他在某个特定时间正在做什么，甚至是关于“彼得广场”这样的非人类实体，以积极的方式。对LMs输出内容的更精细控制可以为生成符合事实或没有不适当内容的文本铺平道路。 我们提出的Content-Conditioner （CoCon），以缩小这一差距，通过指导预训练的LMs的文本输出同时包含目标内容。基本上，CoCon包括三个部分：1）编码器，2）解码器和3）CoCon块。CoCon采用预训练LM作为编码器和解码器，通过CoCon块将目标内容合并到编码文本表示中，然后将内容条件表示传递给解码器进行生成。为了训练CoCon块，我们提出了一种自监督学习方法，其中训练数据由预训练LM本身生成的文本样本组成。通过将每个文本序列分成两段（[xa；xb]），CoCon学会了通过将xb本身作为内容输入来帮助LM重建丢失的后段（xb）。 我们提出了CoCon的损失函数和content masking，在产生高质量文本的同时，对来自不同来源的内容进行限制。由于CoCon块的大小是LM的一小部分，并且没有对LM的权重进行微调，因此训练成本明显低于从头开始训练LM。对比于strong baselines，CoCon也可以进行高等级的文本属性控制(例如主题和情感)，通过zero-shot的方式。此外，CoCon在同化多个目标内容方面具有通用性，其内容调节的强度可以在推理过程中通过内容偏差项进行灵活的调整。本文中，我们使用GPT-2作为LM。我们主要贡献如下： 我们提出CoCon的基于条件的内容语言生成方法。 我们引入了一种自我监督学习方法，在这种方法中，CoCon在给定未来token的信息时学习完成文本序列。 通过消融试验和与PPLM和CTRL等强基线的比较，我们研究了CoCon如何有效地影响文本生成的内容，以及如何有竞争性地控制主题和情感等高级文本属性。 我们展示了CoCon在整合多种内容方面的多功能性，文本生成中内容调节的灵活性，以及它与其他控制方法（如PPLM）的互补性。 2 Related Work之前的属性控制的文本生成方法大多基于RL和GAN进行训练。与CoCon不同，这些方法中对预定属性的要求限制了生成文本的可能类型。 CTRL是最近的一种方法，它通过使用控制代码生成受控的流畅文本，这些控制代码是在生成过程中预先添加到文本中的元数据。虽然它使用GPT-2结构生成高质量的文本，但是它的控制代码在训练过程中也是预先确定的。 最接近我们的工作是PPLM，它试图通过相对较小的“可插拔”属性模型来控制已经预训练的LM上的文本。尽管PPLM的灵活设计也支持受控生成，而无需像CoCon中那样对LM进行再培训或微调，但我们的方法旨在将生成控制在更局部的内容级别，而不是高级的文本属性。另一个核心区别在于训练，CoCon的自监督学习免除了对标记数据的需要，例如用于训练PPLM的属性鉴别器模型的数据。PPLM中的加权解码试图通过在解码步骤中提高目标词的概率来控制输出文本，但已被证明产生不连贯文本。 文本风格转换是通过将文本从一种风格转换到另一种风格来控制文本属性的相关领域。一些这样的研究使用自动编码器来分离文本的风格和非风格的潜在表征。这种分离使得文本在保留大部分内容的同时，能够在潜在空间改变样式。另一项工作是识别与文本语料库中特定风格相关的n个属性标记，并通过替换它们来编辑文本的风格。从本质上说，风格转换改变现有的文本，而不是生成文本，需要预定义的属性。 3 Content Conditioner (CoCon)Motivation在基于语言模型的文本生成任务中，通常使用自回归的方式进行生成： 之前的控制文本生成任务中，p(x)可以通过目标属性或者控制代码进行控制文本的情感或者主题： 虽然这些方法生成是流畅的，并且可以很好地与目标属性对齐，但是输出文本${x_t,...,x_l}$是在全局属性（例如情绪/主题）级别上控制的，而不是在更局部的内容（例如单词/短语）级别上控制的。由于存在大量可能的${x_t,...,x_l}$候选项，这些候选项将与先验文本和目标属性很好地对齐，因此在随机字符采样过程中，生成的文本样本包含非常不同的内容。这激发了一种对输入目标内容c进行条件处理的方法，以便对文本生成进行更细粒度的控制： Model Architecture 我们提出的CoCon（图1）通过在我们的实验中加入一个基于预训练Transformer的语言模型（LM）GPT-2，在保持流畅性的同时控制生成文本的内容。 LM的生成可以分为两个独立的部分：编码器（enc）和解码器（dec）。编码器充当一个特征提取器，接收输入序列的嵌入并在断点处输出其中间表示，即$h_{:t−1}=enc(x_{:t−1})$。随后，解码器接收该表示并输出下一字符的logit，即$o_t＝dec(h_{:t−1})$，从而产生 从等式4中，我们可以看到表示（h）是控制下一个字符logits（o）的表示。实际上，我们通过CoCon块用目标内容输入（c）对h进行条件化，从而转换h 我们参数化CoCon模块将其作为一个单层的Transformer模块，其中包含一个attention层和 FFN层。与典型的LM attention层类似，Query（Q）、Key（K）、Value（V）矩阵通过表示$h_{:t−1}$上的线性变换来计算，其中$Q,K,V\\in R^{(t-1)xd}$，d是表示的维数。为了关注内容表示$h_{l_c}^{(c)}$，还计算内容键和值$K^{(c)},V^{(c)}\\in R^{l_cxd}$，并在计算CoCon注意输出之前concat到原始注意矩阵： 最后的CoCon输出通过位置前馈层进行计算。通过连接到t−1之前的表示并将其传递给解码器（dec），下一个logit以及随后的字符$\\hat x_t$现在由c进行调节： Multiple Content Inputs对于多个目标内容的输入，处理方法较为简单，将多个目标内容的K和V进行拼接再计算即可。 Strength of Content Conditioning在CoCon的注意机制中，我们可以通过偏置W（等式6）中与内容输入（c）相对应的注意权重来改变内容对输出文本的调节程度。更具体地说，可以通过注意对内容值$(V^{(c)})$的softmax加权来改变c对输出文本的影响。在生成期间，可以选择向内容注意权重$W_{:,:l_c}\\in R^{(t-1)xl_c}$添加正偏项（τ内容），以增加V（c）的影响，促进内容调节，而负偏项可以相反地降低内容调节效果。我们讨论了变化τ含量的例子在4.4节中。 3.1 Self-Supervised Learning我们可将一个序列$x=\\{x_1,...,x_{t-1},x_t,...,x_l\\}$分为两部分：$x^a=\\{x_1,...,x_{t-1}\\}$和$x^b=\\{x_t,...,x_{l}\\}$，即$x=[x^a;x^b]$。在现实世界中，$x^b$的替代品可能有很多，可以流利地从$x^a$开始。再加上文本采样的随机性，这意味着，如果没有关于$x^b$的信息，仅用LM从$x^a$重建完整x的概率可能很低。 Self Reconstruction Loss 自重构损失$L_{self}$就是让控制文本$c=x^b$，之后要生成的就是$x^b$自己，这一步是让模型能够学习结合控制文本的内容。 Null Content Loss 无内容损失$L_{null}$，即令$c=null$，这时候CoCon就退化为一个简单的语言模型，为了能够生成流畅的文本。 Cycle Reconstruction Loss 循环重构损失$L_{cycle}$通过两个不同文本互为控制文本来生成质量更高的文本。假设现在有两个不同的文本：$x=[p;q]$和$x&#39;=[p&#39;;q&#39;]$，进行如下操作： 首先将p’作为引导文本，将q作为控制文本，生成新文本$q^1=f(p&#39;,q)$。显然，q1的目的是在和p’保持语言流畅，且尽可能包含q的内容。 再将p作为引导文本，将q1作为控制文本，生成新文本$q^2=f(p,q^1)$，这是让q2和p保持流畅，同时尽可能包含q1的内容。 既然q2包含q1的内容，并且q1包含q的内容，要么就是q2包含q的内容，且要和p保持流程，此时就是q本身。 因此，循环重构损失就是以q为真值去优化q2，反过来也可以以q’为真值去优化q’2 Adversarial Loss 最后用到常用的对抗损失$L_{adv}$，让生成的文本更接近真实的文本。 Full Training 组合以上四个loss即为最终需要优化的目标： 4 Experiments本文在三个可控文本生成任务上进行实验，情感可控生成，主题可控生成，文本可控生成。 文本可控生成 文本引导的文本生成评估指标有BLEU、NIST、METEOR、PPL和Dist-1/2/3。由于没有现成的该任务的数据集，故本文随机从预训练的GPT-2中采样3000个样本，均匀分为三组，每组的控制文本c的长度为5,10,20个BPE长度，所生成的句子x长度都是100。把它们作为测试集。训练集则是随机从GPT-2所产生的句子中获取。此外，CoCon还在大小为250K的Webtext.上训练，以探究不同训练数据来源的影响。结果如下表所示。可以看到，CoCon在结合控制文本c.上比GPT2好太多。在这个实验。上，不加一些损失会有更低的PPL以及更高的BLEU、NIST、METEOR等值，比如去掉$L_{null}$或者$L_{adv}$模型的BLEU值会更高，这是因为这两个损失相比之下更关注生成文本的流畅度而不是与控制文本c相结合。 主题可控文本生成 在主题可控任务上，比较的基线模型有PPLM和CTRL，都是当前很强的模型。为验证主题相关性，训练一个分类器用于评估主题程度。下表是实验结果： 情感可控文本生成 情感可控生成任务采用二分类情感，下表是实验结果，和主题任务类似，CoCon在情感相关度上显著优于其他模型，这说明CoCon能够更加紧密地控制文本c。 实例分析 下图是不同的$t_{content}$生成的不同文本，显然，值越大就越和控制文本相关，但是越容易生成不相关的内容。 多个控制文本输入的结果实例： 5 Conclusion我们提出了CoCon为了细粒度的控制基于神经网络的生成文本。CoCon可以被有效训练通过一个自监督的方式，并且它与已经生成高质量文本的预训练语言模型兼容。通过我们的实验，CoCon被证明能够顺利地将目标内容合并到生成的文本中，并控制高级文本属性。这种新的控制方式为更真实、更安全的文本生成带来了希望，促进了神经文本生成在实际应用中的应用。","categories":[{"name":"文本生成","slug":"文本生成","permalink":"http://renxingkai.github.io/categories/文本生成/"}],"tags":[{"name":"文本风格迁移","slug":"文本风格迁移","permalink":"http://renxingkai.github.io/tags/文本风格迁移/"}],"author":"CinKate"},{"title":"PLUG AND PLAY LANGUAGE MODELS A SIMPLE APPROACH TO CONTROLLED TEXT GENERATION","slug":"iclr-2020-PPLM","date":"2021-01-28T21:44:04.000Z","updated":"2021-01-31T14:27:13.189Z","comments":true,"path":"2021/01/29/iclr-2020-PPLM/","link":"","permalink":"http://renxingkai.github.io/2021/01/29/iclr-2020-PPLM/","excerpt":"","text":"论文链接Abstract在大型文本语料库上训练的大型基于transformer的语言模型（LMs）已经显示出无与伦比的生成能力。然而，如果不修改模型结构或对特定属性数据进行微调，并且需要大量的再训练成本，那么控制生成语言的属性（例如，切换主题或情感）是困难的。我们提出了一个简单的可替代方案：可控制语言生成的即插即用语言模型（PPLM）。它将预训练的语言模型与一个或多个简单的属性分类器相结合，这些属性分类器无需对语言模型进行任何进一步的训练就可以指导文本生成。在我们提出的规范场景中，属性模型是简单的分类器，由用户指定的bag of words或单个学习层组成，其参数比LM少100000倍。采样需要前向和后向传递，其中来自属性模型的梯度推动LM的隐藏激活，从而指导生成。模型样本展示了对一系列主题和情感风格的控制，广泛的自动化和人工注释评估显示属性对齐和流畅性。PPLMs的灵活性在于，可以使用不同属性模型的任何组合来指导文本生成，这将允许本文给出的示例之外的多种创造性应用。 1 INTRODUCTION基于Transformer的PLMs在大量无标注的语料中训练，然后使用对数似然损失函数进行训练。然而，一旦这样的模型被训练，在不修改模型结构以允许额外的输入属性或对属性特定的数据进行微调的情况下，控制生成文本的属性就变得很困难。 可控的文本生成需要对$p(x|a)$进行建模，a是想要的可控属性，x是需要被生成的文本。然而，生成模型只需要去学习$p(x)$。在计算机视觉领域，Nguyen et al.（2017）的即插即用生成网络（PPGN）开发了一种生成具有不同属性的图像的机制，方法是将判别器（属性模型）$p(a|x)$与基本生成模型$p(x)$插在一起，并从结果的$p(x|a)\\propto p(a|x)p(x)$中采样，有效地创建条件生成模型从任何提供的属性模型动态建模。以类似的方式，我们提出了用于条件语言生成的即插即用语言模型（PPLM），它将一个或多个简单的属性模型p（a|x）组合在一起，可以是一个Bag-of-Words（BoW）或单层分类器的形式，也可以是一个预训练的无条件语言模型p（x）。我们从得到的组合模型中取样，在潜在的表示空间中遵循梯度，这种方式受到了（MALA）的启发。 优化是事后在激活空间中执行的，因此不需要重新训练或微调。文本控制是细粒度的，强度参数决定属性影响多大的强度；强度为0则完全恢复原始模型p（x）。这种设计允许极大的灵活性：用户可以将最先进的生成模型与任意数量的属性控制器结合起来，生成模型可能很大，而且很难训练。属性模型可能更易于训练或未经训练，并且在推理过程中可以灵活地组合多个控制器。在本文中，我们演示了使用GPT-2 345M参数模型作为通用语言模型p（x）的PPLM方法，但是该方法适用于任何基于transformer的文本生成器的任何表示空间，并且允许与任何属性模型p（a|x）组合。 我们演示了使用多个属性控制器的受控生成，这些控制器在生成过程中组装和组合，每个控制器具有不同的强度，充当一组“控制旋钮”，将生成的文本调整为所需的属性（参见表1中的示例），我们关键的贡献如下： 我们提出了用于控制语言生成的即插即用语言模型，讨论了它与现有工作的关系，以及如何从PPLM中进行采样。 我们证明了对一系列属性的文本生成控制，包括7个主题，每个主题使用一个BoW定义，以及1个简单的情感判别器。我们使用自动评估（分别训练的困惑和情绪模型）和人类评估（属性相关性和流畅性）来量化有效性。所有的评估都指向PPLMs生成属性控制的流畅文本的能力 我们对比了CTRL和GPT-2，我们的方法，没有任何LM训练，在属性相关性和流利性方面都是标杆，并且经常优于基线 我们表明，PPLM方法可以用来文本消除毒性，遵循负梯度模型训练，生成有毒内容是可能的，同时也可以用其检测毒性。我们还展示了PPLM如何用于结构受限的故事写作 2 RELATED WORKControlled generation当前的控制文本生成方法大多涉及到使用RL方法ft预训练模型，训练GAN或者训练条件生成模型。与我们方法不同之处，这些方法并不是即插即用，因为整个模型需要针对每个特定属性分别进行微调。我们的方法不需要对任何条件生成模型进行再训练，语言模型和条件模型都可以灵活组合。 Noisy Channel Modeling不少人使用香农噪声信道理论为了提升Seq2Seq建模，他们的方法翻译源语言句子y到一个目标语言句子x通过首先对前向模型的采样$p_{forward}(x|y)$，然后基于概率$p_{backward}(x|y) \\propto p(x)p(y|x)$重排样本。PPLM对样本进行打分使用相同的基本公式，但由于我们没有前向模型$p_{forward}(x|a)$，我们依赖于潜在空间的更新。作为一个基线，我们使用$p(x)$作为前向模型，然后重排序。我们将看到，在某些场景中工作得相当好，而在其他场景中工作得很差 Weighted decoding有的研究者使用判别器或者BoW去控制语言生成，为了考虑用于解码的评分函数，对解码过程进行了修改。See等人（2019）注意到，使用加权解码（WD）进行控制是困难的，通常会导致牺牲流畅性和连贯性。此外，Ghazvininejad et al.（2017）强烈依赖于从特定主题的一组关键字中取样，并且不允许以不必包括一组关键字的方式偏向于主题的生成。类似的，Baheti等人提出了一种解码策略用于在对话系统中生成感兴趣的回复，使用BoW和词向量，随机采样方法可以被使用去受限于模型的生成到确定的关键词和主题。我们使用带权重的解码作为基线。 Text Style Transfer在语言建模之外，语篇风格迁移是一个相关的研究课题。有的研究者训练VAE for 风格迁移依赖于学习区分风格和内容的潜在表示。Li et al.证明了一种基于条件生成模型的简单方法的有效性，该方法将与属性相关的n-gram替换为与所需属性对应的n-gram。我们的方法与上面方法一个关键的不同之处在于，我们使用了一个线下的判别器，基于此鉴别器执行优化。最近，Lample等人（2019年）采用了一种从无监督语言翻译到风格转换的方法，在这种方法中，去噪自动编码器的训练目标包括重建损失和回译损失的加权组合。虽然上述方法在风格转换任务上取得了令人印象深刻的成功，但主要的焦点是不受控制的语言生成，而且这些方法也不是即插即用的。 3 PLUG AND PLAY LANGUAGE MODELS 3.1 LANGUAGE MODELING WITH TRANSFORMERS给定一个句子$X={x_0,...,x_n}$，语言模型被训练去计算序列$p(X)$的无条件概率。这个概率可以应用递归链式法则进行表示为乘积形式： 本文中，我们使用Transformer进行语言分布的建模。令$H_t=[(K_t^{(1)},V_t^{(1)}),...,(K_t^{(l)},V_t^{(l)})]$为由key-value组成的历史矩阵，从第i层0到t的时间序列。 3.2 STEERING GENERATION: ASCENDING $log p(a|x)$为了去控制语言模型的输出，在每个生成步t，我们转移历史$H_t$在两个梯度方向上，一种倾向于在条件属性模型$p(a|x)$下，更高的对数似然属性a；另一种是倾向于未修改语言模型p（x）的更高对数似然。将这些因素与可变乘数结合起来，为我们提供了一个可控的“旋钮”，以指定的强度在给定的情感方向上引导生成。参数更新仅限于Ht，而不限于其他激活模型，因为未来的预测仅通过Ht依赖于过去（注意，Ht由在时间t之前生成的所有Tramsformer的key-value组成）。在Ht空间经过每一步会导致模型激活的逐渐变化——这可能被认为是对过去的逐渐重新解释——从而引导下一代朝着理想的方向发展。 将∆Ht作为对Ht的更新，这样，使用（Ht+∆Ht）生成的文本会改变生成文本的分布，从而更可能拥有所需的属性。∆Ht初始化为零，并使用属性模型的梯度进行更新，该属性模型测量生成的文本具有所需属性的程度（例如正性）。我们重写属性模型$p(a|x)$为$p(a|Ht+∆Ht)$然后基于以下公式去更新∆Ht： 3.3 ENSURING FLUENCY: ASCENDING $log p(x)$上一节中描述的方法能够生成针对特定判别器的文本，但如果不加以检查，当文本进入低概率区域时，将很快导致不切实际的敌对或愚蠢的示例（Szegedy et al.，2013；Nguyen et al.，2015）。为了解决这个问题，我们从两个方面使用了无条件语言模型，以确保流利度保持在或接近无条件语言模型的水平（这里是GPT-2）。 Kullback–Leibler (KL) Divergence除了上述步骤之外，我们还更新∆Ht以最小化修改和未修改语言模型的输出分布之间的KL差异。实际中，这是通过在使用渐变之前将数量相加来实现的，尽管可以将其可视化为两个单独的步骤，如图2所示。我们用一个标量λKL来缩放KL系数，在实践中，将这个超参数设置为0.01通常可以很好地在各种任务工作。 Post-norm Geometric Mean Fusion除了KL散度，我们还是用了后范数融合方法，这并不直接影响∆Ht；相反，它只是将生成的文本与无条件的p（x）LM分布联系起来。 3.4 SAMPLING AND RANKINGPPLM中的属性模型$p(a|x)$提供两种功能：首先，基于对数似然提供一个能排序想要样本的分数；其次，在潜在空间中执行更新的梯度上升方向。前者可用于生成r个样本，并对其进行排序，以选择最佳样本。除了使用更新的采样外，这还可以作为属性控制的附加方法。此外，为了避免重复性、低质量文本的问题（Holtzman et al.，2018），我们计算Dist-1、Dist-2和Dist-3分数的平均值（对于生成的文章），这是重复性的指标（Li et al.，2015），然后丢弃平均分数低于阈值τ的样本。 4 EXPERIMENTS, RESULTS, AND EVALUATION4.1 EVALUATION METHODS AND ABLATION STUDY我们评估两个属性：PPLM是否生成满足所需属性（主题或情感）的文本，以及当我们加强对属性的控制时，其文本质量是否恶化。注：我们可以随时将控制旋钮调低到零，以禁用属性控制，并达到原始模型的流畅性。如果需要，用户可以在推理时调整旋钮，直到在属性强度和流利性之间达到所选择的折衷。我们使用自动方法和人工注释器进行评估 4.2 BoW ATTRIBUTE MODELSBoW的一些结果 4.3 DISCRIMINATOR ATTRIBUTE MODELS当仅用BoW一些词难以表示的属性词，就可以使用判别器来进行属性控制。我们在输入句子x和相应标签yx的数据集上训练鉴别器，以下是结果。 4.4 LANGUAGE DETOXIFICATION使用大量互联网数据训练的语言模型反映了数据中存在的偏见和歧视。Wallace et al.（2019）最近的一篇论文进行了对抗性攻击，当给定一个经过仔细优化的触发器字符串作为前缀时，GPT-2会产生种族主义输出。他们还发现，当简单地使用“黑人”作为前缀时，2%的GPT-2样本包含明显的种族歧视。其他前缀（如“亚洲人”或“犹太人”）被提及，但没有报告百分比。我们进行实验并报告基线毒性百分比为10%（“亚洲人”）、12%（“犹太人”）和8%（“黑人”）。Wallace等人（2019年）发布的代码库产生了对抗性触发，平均毒性百分比为63.6%。更多细节见第S13节。 通过引入毒性分类器作为属性控制模型，并用负梯度更新潜在空间，PPLMs可以很容易地适应语言解毒。我们对来自毒性评论分类挑战（Jigsaw）的毒性数据训练了一个单层分类器，并表明与其他PPLM-Discrim方法具有相似的超参数设置，它在自然提示和对抗性触发条件下都能很好地工作。自然诱发的毒性百分比分别为6%、4%和10%，而对抗性诱发的毒性百分比则急剧下降到平均4.6%，具有统计学意义。注释程序和百分比和p值的完整表格的详细信息见表S23和第S13节。请注意，解毒语言的模型也可能被恶意用于生成有毒语言，这是我们在第S6节中简要讨论的主题 4.5 CONTROLLED STORY WRITING我们探索辅助性故事写作的受控生成（Peng et al.，2018；Luo et al.，2019；Yao et al.，2019；Fan et al.，2018）。使用不受控制的LMs辅助艺术创作可能很困难。为了有助于结构，我们使用了预先定义的故事骨架，通常用于即兴创作。我们用PPLM填充这些前缀之间的空白。见表S20和表S21中的示例。 5 CONCLUSION我们提出了PPLM，一个即插即用的方法可控的语言生成，容易组合到一个巨大的预训练LM和一个BoW模型，或者一个轻量级的容易去训练的判别器。PPLM实现了对属性的细粒度控制通过一个简单的基于梯度的采样机制。由于PPLMs在保持流利性的同时能够灵活地控制生成，因此它在支持下一代语言模型方面具有很大的潜力。","categories":[{"name":"文本生成","slug":"文本生成","permalink":"http://renxingkai.github.io/categories/文本生成/"}],"tags":[{"name":"文本风格迁移","slug":"文本风格迁移","permalink":"http://renxingkai.github.io/tags/文本风格迁移/"}],"author":"CinKate"},{"title":"Style Transfer from Non-Parallel Text by Cross-Alignment 阅读笔记","slug":"nips2017-style-transfer","date":"2021-01-26T09:09:18.000Z","updated":"2021-01-27T12:43:13.901Z","comments":true,"path":"2021/01/26/nips2017-style-transfer/","link":"","permalink":"http://renxingkai.github.io/2021/01/26/nips2017-style-transfer/","excerpt":"","text":"论文链接Abstract本文主要研究基于非平行文本的文体转换。这是一个包括机器翻译、破译和情感改写在内的一系列问题的例子。关键的挑战是将内容与风格等其他方面分开。我们假设一个共享的潜在内容分布在不同的文本语料库中，并提出了一种利用潜在表达的精确对齐来进行风格转换的方法。从一个语体转换过来的句子应该和从另一个语体转换过来的例句相匹配。我们证明了这种交叉比对方法在三个任务上的有效性：情感改写、单词替换密码的破译和词序恢复。 1 Introduction机器翻译、文本摘要类似的任务需要大量的平行语料进行训练，但是在一些NLP生成任务中，我们仅有非平行或单语言的数据。类似于文本破译或者风格迁移，在这些任务中，我们必须保留源语句的内容，但要使语句与所需的表示约束（例如，样式、明文/密文）保持一致。 我们的任务较有挑战性：我们只假设访问两个句子的语料库，尽管呈现的风格不同，但内容分布相同。我们的目标是证明这种内容的分布等价性，如果仔细加以利用，就足以让我们学会将一个文体中的句子映射到一个文体无关的内容向量，然后将其解码成一个内容相同但文体不同的句子。 本文中，我们提出了一个精细的句子级表示对齐方法在文本语料之间。我们通过学习一个编码器，它以一个句子和它的原始样式指示符作为输入，并将其映射到与样式无关的内容表示。然后将其传递给与样式相关的解码器进行渲染。实际上，更丰富的潜在内容表示更难在整个语料库中对齐，因此它们提供了更多的信息内容约束。此外，我们从交叉生成（风格转换）的句子中获取额外的信息，从而得到两个分布对齐约束。例如，风格转换成否定句的肯定句，作为一个总体，应该与给定的否定句相匹配。我们在图1中说明了这种交叉对齐 我们使用三个NLP任务去证明我们方法的有效性：情感改写、文本破译和词序恢复。在这三个任务中，模型都是使用非平行语料进行训练。 2 Related WorkStyle transfer in vision 抽取内容和风格特征等，使用GANS network(CoupledGANs、CycleGAN)。虽然我们的方法具有类似的高级体系结构，但自然语言的离散性不允许我们重用这些模型，因此需要开发新的方法。 Non-parallel transfer in natural language在NLP中的生成任务大多需要平行语料，类似于(机器翻译、摘要生成等)。我们的方法很接近于不使用平行语料，但是使用训练中的非直接信号进行辅助。(VAEs，操作隐状态的表示以控制对应的情感生成等)。虽然我们的模型建立在分布式交叉对齐的基础上，以实现风格转换和内容保留，但可以以相同的方式添加约束。 Adversarial training over discrete samples最近离散样本中的对抗训练，大多使用RNN。在我们的工作中，我们采用了Professor-forcing算法（Lamb等人，2016年），该算法最初是为了弥补教师在训练过程中的强迫和测试过程中的自我反馈之间的差距。这样的设计符合我们的风格迁移场景which called cross-alignment。 3 Formulation 这个命题基本上是说不同风格生成的X应该足够“独特”，否则风格之间的转换任务就没有很好的定义。虽然这看起来微不足道，但即使对于简单的数据分布，它也可能不适用。下面的例子说明了在不同的模型假设下，转移（和恢复）是如何变得可行或不可行的。正如我们将看到的，对于某一风格的Y，z的分布越复杂，恢复传递函数的可能性就越大，寻找传递函数就越容易。 3.1 Example 1: Gaussian 3.2 Example 2: Word substitution这里考虑另一个例子，当z是一个双语语言模型，风格y是一个正在使用的词汇表，它将每个“内容词”映射到它的表面形式（词汇形式）。如果我们观察同一语言z的两个实现x1和x2，那么转移和恢复问题就变成了推断x1和x2之间的单词对齐。 这就是一个简单的语言破译或者翻译版本。除此之外，回复问题仍然是很困难的。为了解释这一问题，假设$M_1$和$M_2$是$X_1$和$X_2$的概率估计，寻找单词对齐与去寻找一个排列矩阵P，满足$P^TM_1P≈M_2$是相似的，可以表示为一个优化问题： 同样的公式适用于给定M1和M2作为两个图的邻接矩阵的图同构（GI）问题，表明确定P的存在性和唯一性至少是GI困难的。幸运的是，如果M作为一个图足够复杂，那么搜索问题可能更容易处理。例如，如果作为一个集合的每个顶点的关联边的权重是唯一的，那么可以通过简单地匹配边集合来找到同构。这个假设在很大程度上适用于我们的场景，其中z是一个复杂的语言模型。我们在结果部分以经验证明了这一点。 上述例子表明，作为潜在内容变量的z应该承载最复杂的数据x，而作为潜在风格变量的y应该具有相对简单的效果。我们将在下一节中相应地构建模型。 4 Method由于图像中的数据是连续的，所以可以直接进行迁移学习，由于自然语言的离散性，我们不能直接进行训练；需要我们在潜在空间进行操作。由于x1和x2是给定潜在内容变量z之后条件独立。 这样表示建议我们使用一个自编码模型，特别地，一个风格迁移任务将x2迁移到x1涉及到两步：编码步和解码步。编码步骤负责推断x2的内容$z-p(z|x_2,y_2)$：解码步骤负责：生成对应的迁移事务从$p(x_1|y_1,z)$,本文中，我们使用神经网络近似学习和训练$p(z|x,y)$和$p(x|y,z)$。 正如我们的生成框架所设想的那样，为了使翻转风格成为一个有意义的迁移，$X_1$和$X_2$的内容空间必须是连续的。为了限制x1和x2是从相同的潜在内容分布p（z）生成的，一种选择是应用可变自动编码器（Kingma和Welling，2013）。VAE先使用一个先验的概率密度$p(z)$，例如$z-N(0,I)$，然后使用KL散度去标准化后验$p_E(z|x_1,y_1)$和$p_E(z|x_2,y_2)$ 然而，正如我们在上一节中所讨论的，将z限制为简单且均匀的分布，并将大部分复杂性推给解码器，对于非并行数据的风格迁移可能不是一个好的策略。相比之下，标准的自动编码器只是将重建误差最小化，鼓励z携带尽可能多的x信息。另一方面，它降低了p（x|y,z）中的熵，这有助于在y1和y2之间切换时产生有意义的风格转换。在不显式建模p（z）的情况下，仍然可以强制p（z|y1）和p（z|y2）的分布对齐。为此，我们介绍了自动编码器的两种受约束的变体。 4.1 Aligned auto-encoder省去了对p(z)作显式假设并使后验值与p(z)一致的VAEs，我们将$p_E(z|y_1)$和$p_E(z|y_2)$相互对齐，从而得到如下约束优化问题。 在实践中，拉格朗日松弛的原始问题，而不是优化。我们引入了一个对抗性鉴别器D来校准不同类型z的聚集后验分布（Makhzani et al.，2015）。D旨在区分这两种分布： 我们使用带有GRU单元的单层RNNs来实现编码器E和生成器G。E获取一个初始隐藏状态为y的输入句子x，并输出最后一个隐藏状态z作为其内容表示。G生成一个以潜在状态(y,z)为条件的句子x。为了对齐$z_1=E(x_1,y_1)$和$z_2=E(x_2,y_2)$的分布，鉴别器D是具有单个隐藏层和sigmoid输出层的前馈网络。 4.2 Cross-aligned auto-encoder第二种变体是交叉对齐自动编码器，它直接将一种风格的迁移样本与另一种风格的真实样本对齐。 在生成性假设下，$p(x_2|y_2)=\\int_{x_1}p(x_2|x_1;y_1;y_2)p(x_1|y_1)dx_1$，因此x2（从左侧取样）应表现出与被迁移的x1（从右侧取样）相同的分布，反之亦然。与我们的第一个模型类似，第二个模型使用两个判别器D1和D2来排列群体。D1的任务是区分真实x1和被迁移的x2，D2的任务是区分真实的x2和被迁移的x1。 对抗训练由G生成的离散样本由于梯度阻断无法进行反向传播，虽然很多人使用强化学习，但由于高方差的采样梯度导致收敛并不稳定。首先，我们不使用单个采样词作为生成器RNN的输入，而是使用词上的softmax分布。具体地说，在从G(y1,z2)迁移x2的生成过程中，假设在时间步t处输出logit向量是vt。我们将其峰值分布softmax（vt/γ）作为下一个输入，其中γ是温度参数。 其次，我们使用Professor Forcing（Lamb et al.，2016）来匹配隐藏状态序列，包含输出信息且平滑分布的输出词。也就是说，到判别器D1的输入是由实例x1强制的（1）G（y1；z1）教师的隐藏状态序列，或者由先前的软分布自馈的（2）G（y1；z2）教师的隐藏状态序列。 交叉对齐自动编码器的运行过程如图2所示。注意，交叉对齐加强了潜在变量z在生成器G的递归网络上的对齐。通过对齐整个隐藏状态序列，它可以防止z1和z2的初始不对齐在递归生成过程中传播，因此转移的句子可能会结束在远离目标的某个地方目标域。 我们使用卷积神经网络实现D1和D2序列分类（Kim，2014）。算法1给出了训练算法。 5 Experimental setup Sentiment modification 我们的第一个实验是以改变潜在情绪为目标的文本改写，这可以看作是否定句和肯定句之间的风格转换。我们在Yelp餐厅评论上进行实验，利用与每个评论相关的现成用户评分。按照标准惯例，评分高于三分的评审被认为是积极的，低于三分的评审被认为是消极的。当我们的模型在句子层次上运行时，数据集中的情感注释在文档层次上提供。我们假设文档中的所有句子都有相同的情感。这显然过于简单化了，因为有些句子（例如背景）是情绪中立的。考虑到这样的句子在长评论中更为常见，我们过滤掉超过10句的评论。我们进一步过滤剩下的句子，去掉那些超过15个单词的句子。结果数据集有25万个否定句和35万个肯定句。将出现少于5次的单词替换为“”标记后，词汇量为10K。作为基线模型，我们与Hu et al.（2017）的对照control-gen模型进行了比较。 为了定量评估转换的句子，我们采用了一种基于模型的评估标准，类似于用于图像传输的评估标准（Isola et al.，2016）。具体地说，我们根据预先训练好的情感分类器来衡量一个转移句有多少正确的情感。为此，我们使用Kim（2014）中描述的TextCNN模型。在我们简化的风格转换数据集上，它几乎达到了97.4%的完美准确率。 虽然定量评估提供了一些传输质量的指标，但它并没有涵盖这一代任务的所有方面。因此，我们还对从测试集2中随机选取的500个句子进行了两次人类评估。在第一次评估中，评委们被要求根据句子的流畅性和情感程度对生成的句子进行排名。流利程度从1分（不可读）到4分（完美），而情感类别是“积极”、“消极”或“两者都不是”（可能是矛盾的、中性的或无意义的）。在第二个评价中，我们对转移过程进行了比较评价。以随机顺序向注释者展示源句和系统的相应输出，并询问“哪一个转移句在语义上等同于具有相反情感的源句？”？”. 它们可以都令人满意，A/B更好，也可以都不令人满意。我们为每个问题收集两个标签。标签协议和冲突解决策略可在补充材料中找到。请注意，这两个评估不是多余的。例如，一个系统总是独立于源句生成语法正确、情感正确的句子，在第一个评估设置中得分较高，但在第二个评估设置中得分较低。 Word substitution decipherment我们的第二组实验涉及字词替换密码的破译，这在NLP文献中已有探讨（Dou和Knight，2012；Nuhn和Ney，2013）。这些密码将明文（自然语言）中的每个单词根据1:1替换密钥替换为密码令牌。解密任务是从密文中恢复明文。如果我们能够访问并行数据，这任务是微不足道的。然而，我们有兴趣考虑一个非并行破译方案。为了训练，我们选择200K个句子作为X1，并对不同的200K个句子集应用替换密码f得到X2。虽然这些句子是非平行的，但它们是从评论数据集中的相同分布中提取的。开发集和测试集有100K个平行句D1和D2。我们可以使用Bleu分数定量比较D1和转移（解密）D2（Papineni et al.，2002）。 显然，这个破译任务的难度取决于替换词的数量。因此，我们根据替换词汇表的百分比来报告模型性能。注意，转移模型不知道f是一个词替换函数。他们完全从数据分发中学习。 除了有不同的迁移模型外，我们还引入了一个简单的基于词频的解码基线。具体来说，我们假设X1和X2之间共享的单词不需要翻译。其余的单词根据频率进行映射，任意断开连接。最后，为了评估任务的难度，我们报告了在平行语料库上训练的机器翻译系统的准确性（Klein et al.，2017）。 Word order recovery我们最后的实验集中在单词排序任务，也称为bag翻译（Brown et al.，1990；Schmaltz et al.，2016）。通过学习原始英语句子X1和混叠英语句子X2之间的语体迁移函数，该模型可以用来恢复混叠句子的原始语序（或者反过来随机排列句子）。非平行训练数据和平行测试数据的构造过程与单词替换破译实验相同。同样，传输模型不知道f是一个随机函数，完全从数据中学习 6 Results 7 Conclusion之前的迁移学习都是使用平行语料，本文工作中，我们将其定义为翻译问题，并使用非平行的语料。我们的方法使用强制的分布表示对齐去优化神经网络，我们使用情感迁移任务、文本破译、单词排序等任务证明我们方法的有效性。本文也引出了一个问题：when can thejoint distribution p(x1; x2) be recovered given only marginal distributions?，我们认为解决此问题可以促进情感迁移在CV和NLP中的发展。","categories":[{"name":"文本生成","slug":"文本生成","permalink":"http://renxingkai.github.io/categories/文本生成/"}],"tags":[{"name":"文本风格迁移","slug":"文本风格迁移","permalink":"http://renxingkai.github.io/tags/文本风格迁移/"}],"author":"CinKate"},{"title":"Towards Fine-grained Text Sentiment Transfer 笔记","slug":"FGST","date":"2021-01-12T23:53:55.000Z","updated":"2021-01-19T08:27:34.718Z","comments":true,"path":"2021/01/13/FGST/","link":"","permalink":"http://renxingkai.github.io/2021/01/13/FGST/","excerpt":"","text":"论文链接Abstract本文专注于细粒度的情感迁移(FGST)，该任务的目标是在保留原始语义内容的同时，修改输入序列以满足给定的情感强度。不同于传统的情绪传递任务，它只反转文本的情绪极性（正/负），FTST任务需要更细致和细粒度的情绪控制。为了解决这个问题，我们提出了一个新的Seq2SentiSeq模型。具体地，通过高斯核层将数字情感强度值并入解码器以精细地控制输出的情感强度。此外，针对并行数据不足的问题，提出了一种循环强化学习算法来指导模型训练。在这个框架中，精心设计的奖励可以平衡情感转换和内容保存，同时不需要任何ground truth的输出。实验结果表明，该方法在自动评价和人工评价方面均优于现有方法。 1 Introduction为了对文本生成进行更细致、更精确的情感控制，我们转向细粒度文本情感转移（FTST），它在保持语义内容不变的情况下，修改一个序列以满足给定的情感强度。例子如下： FTST任务有两个主要挑战。首先，在生成句子时很难实现情感强度的细粒度控制。以前关于粗粒度文本情感迁移的工作通常对每个情感标签使用单独的解码器（Xu et al.，2018；Zhang et al.，2018b）或将每个情感标签嵌入单独的向量（Fu et al.，2018；Li et al.，2018）。然而，这些方法对于细粒度文本情感迁移是不可行的，因为目标情感强度值是一个实际值，而不是离散的标签。第二，平行数据在实践中是不可用的。换句话说，我们只能访问标有细粒度情绪评级或强度值的语料库。因此，在FTST任务中，我们不能通过ground truth输出来训练生成模型。 针对上述两大挑战，我们提出了两个相应的解决方案。首先，为了控制生成句子的情感强度，我们提出了一种新的情感强度控制序列对序列（Seq2Seq）模型Seq2SentiSeq。它通过高斯核层将情感强度值引入到传统的Seq2Seq模型中。通过这种方式，该模型可以鼓励在解码过程中生成情感强度接近给定强度值的词。其次，由于缺乏并行数据，我们不能直接用极大似然估计（MLE）对模型进行训练。因此，我们提出了一种循环强化学习算法来指导模型训练，而不需要任何并行数据。设计的奖励可以平衡情感迁移和内容保存，同时不需要任何地面真相输出。 本文主要贡献如下： 提出了一种情感强度控制的生成模型Seq2SentiSeq，通过高斯核层引入情感强度值，实现对生成句子的细粒度情感控制。 为了适应非并行数据，我们设计了一种循环强化学习算法CycleRL来指导模型的无监督训练。 实验表明，该方法在自动评价和人工评价两方面均优于现有系统。 2 Proposed Model2.1 Task Definition给定一个输入序列x和一个目标情绪强度值$v_y$，FTST任务的目标是生成一个序列y，该序列y不仅表达了目标情绪强度$v_y$，而且保留了输入x的原始语义内容。在不损失通用性的前提下，我们将情绪强度值$v_y$限制在0（最负）到1（最负）之间阳性。 2.2.1 Encoder使用双向RNN对句子进行编码，获取每个词的表示$h_i$。 2.2.2 Decoder给定输入句子的隐层表示$\\{h_i\\}_{i=1}^m$和目标情感强度值$v_y$，decoder目标去生成序列y,不仅与x描述相同的语义，同时表达出了情感$v_y$。 为了在解码过程中达到控制情感的目的，我们首先在原有的语义表示之外，在每个词中嵌入一个额外的情感表示。语义表征表征词的语义内容，情感表征词表征情感的强度。形式上，解码器在时间步t的隐藏状态st计算如下： 与传统的Seq2Seq模型（Bahdanau et al.，2014）类似，整个词汇表的语义概率分布计算如下 情绪概率度量生成序列的情绪强度与目标vy的接近程度。通常，每个词都有特定的情感强度。例如，单词“好”的强度约为0.6，“好”的强度约为0.7，“好”的强度约为0.8。然而，当涉及到之前生成的词时，当前生成词的情感强度可能完全不同。例如，短语“不好”的负强度约为0.3，而“非常好”的负强度约为0.9。也就是说，每个词在时间步t的情感强度应该由情感表示$E_s$和当前解码器状态$s_t$两者决定。因此，我们定义了一个情感预测的函数$g(E_s,s_t)$直观地说，为了实现情感的细粒度控制，情感强度更接近目标情感强度值$v_y$的词应该被赋予更高的概率。 受Luong et al.（2015）和Zhang et al.（2018a）的启发，为了支持情感强度接近vy的词，我们引入了一个高斯核层，该层以vy为中心放置高斯分布。具体而言，情绪概率表示为： 为了平衡情感转换和内容保存，将整个词汇表的最终概率分布pt定义为两个概率分布的混合 2.3 Training: Cycle Reinforcement LearningFTST任务的一个严重挑战是缺乏并行数据。由于ground truth输出y是不可观测的，因此不能直接用极大似然估计（MLE）进行训练。为此，我们设计了一种循环强化学习（CycleRL）算法。算法1概述了训练过程。两个奖励旨在鼓励改变情绪，但保留内容，而不需要平行数据。下面介绍Seq2SentiSeq模型的两个奖励和相应的梯度的定义。 2.3.1Reward Design我们为FTST的两个目标(sentiment transformation and content preservation)分别设计两个rewards，然后使用一个全局reward r去平衡两个目标和指导模型训练。 Reward for sentiment transformation.一个预先训练的情感评分器用来评估样本句子$\\hat{y}$与目标情感强度值$v_y$的匹配程度。具体而言，情绪转化的回报公式如下： Reward for content preservation.直观地说，如果模型在内容保存方面表现良好，则很容易对源输入x进行反向重构。因此，我们将内容保存的报酬设计为基于生成的文本$\\hat{y}$和源情感强度值$v_x$的模型重构x的概率。 Overall reward.最终整体的reward。 3 Experimental Setup3.1 Dataset我们在Yelp数据集5上进行实验，该数据集包含大量的产品评论。每一篇评论都被分配了一个从1到5的情绪等级。由于细粒度评分中人与人之间的标签不一致更为严重，因此我们对Jaccard相似度大于0.9的句子进行平均评分。然后，将平均评级标准化为0到1之间的情绪强度。其他数据预处理同沈等（2017）。最后，我们得到了总共64万个句子。我们随机分为630k训练，10k验证，500k测试。尽管训练数据集的情绪强度分布不均匀，但该框架由一个均匀的数据扩充组成，该扩充生成的句子强度来自区间[0，1]，步长为0.05，以指导模型训练（算法1中的步骤6）。 3.3 Baselines细粒度系统旨在修改输入句子以满足给定的情感强度。Liao et al.（2018）构建伪平行语料库来训练一个模型，该模型是一个修正的VAE和一个耦合组件的组合，该组件对伪平行数据进行建模，并具有三个额外的损失L。此外，我们还考虑了SCSeq2Seq（Zhang等人，2018a），这是在对话生成中提出的一种特异性控制的Seq2Seq模型。为了适应这种无监督的任务，提出的CycleRL训练算法被用来训练SC-Seq2Seq模型。 粗粒度系统旨在反转输入的情绪极性（正/负），这可以被视为情绪强度设置为低于平均值（负）或高于平均值（正）的特殊情况。我们将我们提出的方法与以下最先进的系统进行了比较：CrossAlign（Shen et al.，2017）、MultiDecoder（Fu et al.，2018）、DeleteRetrieve（Li et al.，2018）和Unpaired（Xu et al.，2018）。 3.4 Evaluation Metrics3.4.1 Automatic Evaluation Content (BLEU) Fluency (PPL) Sentiment 3.4.2 Human Evaluation4 Results and Discussion 5 Related Work近年来，关于无监督情绪传递的文献越来越多。这项任务的目的是翻转一个句子的情感极性，但保持其内容不变，没有平行数据。然而，对于情感的细粒度控制的研究却很少。Liao et al.（2018）通过启发式规则利用伪并行数据，从而将此任务转化为监督任务。然后提出了一种基于变分自动编码器（VAE）的模型，首先将内容因子和源情感因子分离，然后将内容因子和目标情感因子结合起来。然而，伪并行数据的质量并不理想，这严重影响了VAE模型的性能。与之不同的是，我们在训练过程中通过反译（Lample等人，2018b）动态更新伪并行数据（等式12）。 NLP的其他一些任务也对控制文本生成的细粒度属性感兴趣。例如，Zhang et al.（2018a）和Ke et al.（2018）提出控制对话生成的特异性和多样性。我们从这些文章中借鉴了一些想法，但我们的工作动机和提出的模式却与之相去甚远。主要区别在于：（1）由于情感依赖于局部语境，而特异性独立于局部语境，因此我们的模型中有一系列的设计来考虑局部语境（或先前生成的词）st（例如，公式1，公式3）。（2） 由于缺乏并行数据，我们提出了一种循环强化学习算法来训练所提出的模型（第2.3节）。 6 Conclusion我们提出了一个Seq2SentiSeq模型来控制生成句子的细粒度情感强度。为了在没有任何并行数据的情况下训练所提出的模型，我们设计了一种循环强化学习算法。我们将所提出的方法应用于Yelp评论数据集，获得了自动评估和人工评估的最新结果。","categories":[{"name":"文本生成","slug":"文本生成","permalink":"http://renxingkai.github.io/categories/文本生成/"}],"tags":[{"name":"文本风格迁移","slug":"文本风格迁移","permalink":"http://renxingkai.github.io/tags/文本风格迁移/"}],"author":"CinKate"},{"title":"图神经网络相关学习","slug":"graph-embedding","date":"2021-01-09T10:00:56.000Z","updated":"2021-01-11T03:09:40.943Z","comments":true,"path":"2021/01/09/graph-embedding/","link":"","permalink":"http://renxingkai.github.io/2021/01/09/graph-embedding/","excerpt":"","text":"只用于记录学习，图侵删。 图学习算法分类结构图 Deep Walk &amp; Node2vec GraphSAGE","categories":[{"name":"图神经网络","slug":"图神经网络","permalink":"http://renxingkai.github.io/categories/图神经网络/"}],"tags":[{"name":"GraphEmbedding","slug":"GraphEmbedding","permalink":"http://renxingkai.github.io/tags/GraphEmbedding/"}],"author":"CinKate"},{"title":"Style Transformer：Unpaired Text Style Transfer without Disentangled Latent Representation 笔记","slug":"style-transfomer-paper","date":"2021-01-06T21:31:33.000Z","updated":"2021-01-11T03:19:15.909Z","comments":true,"path":"2021/01/07/style-transfomer-paper/","link":"","permalink":"http://renxingkai.github.io/2021/01/07/style-transfomer-paper/","excerpt":"","text":"论文链接Abstract在未配对的文本风格转换中，对潜空间中的文本内容和风格进行消解是一种普遍现象。然而有两个主要的问题存在现在的大多数神经网络中：1)从一个句子的语义中完全剔除风格信息是很困难的；2)基于递归神经网络（RNN）的编码器和解码器，在潜在表示的中介下，不能很好地处理长期依赖的问题，导致非文体语义内容保存不好。在本文中，我们提出了Style Transfomer，它不需要假设源句的潜在表示，而是在Transformer中配置注意机制，以实现更好的风格转换和内容保留。 1.Introduction语篇风格转换的任务是改变语篇的风格属性（如情感），同时在语境中保留与风格无关的内容。由于文本风格定义比较模糊，构建相同内容但文本风格不一样的句子对是比较困难的。因此文本风格迁移研究主要关注在未成对的文本句子。 神经网络成为了主流的文本风格迁移工具，主要使用Seq2Seq结构，encoder负责编码句子到一个vector，decoder生成不同风格的文本但是保留了原来的内容。这些方法都在专注于如何分清潜在语义空间中的content和style。由于没有成对的句子，对抗loss被使用到潜在空间去促进潜在空间中的风格信息。 现在面临的一些困难： 在潜在语义空间中，区分content和style是困难的， 区分content和style是没有必要的，一个好的风格迁移器，其实不用区分content和style，最好进行直接改写并输出。 受限的vector表示，会使得较长句子编码损失信息 为了理清潜在空间中的content和style，所有的现存方法都假设输入句子被编码到一个固定大小的潜在空间。这些方法因此不能使用attention机制去加强输入句子中的信息。 大多数模型使用RNN去作为encoder和decoder,对于较长句子编码效果并不好。 本文使用Transfomer作为基础模块。贡献如下： 我们提出一个创新的训练算法，不假设需要理清输入句子的content和style，因此，模型可以使用attention机制去提升性能。 首次使用Transfomer在风格迁移任务中。 在两个数据集中我们模型效果都很好，特别地，在content保留中，Style Transfomer取得了较大性能的提升。 2.Related Work3.Style Transformer3.1 Problem Formalization假设有K种不同的数据集Di,可以定义K种不同的风格，文本风格迁移目标是给一个随机的自然语言句子x和一个想要的风格s，需要让机器重写句子x成为新的句子y,句子y包含新的风格s,同时又保留了x的文本信息(语义)。 3.2 Model Overview本文目标是学习一个映射函数f(x,s) ，x是一个自然语言句子，s是一个风格控制变量。函数f的输出是迁移之后的句子y。 3.3 Style Transformer NetworkTransformer是一个标准的encoder-decoder结构，对于一个输入句子x=(x_1,x_2,...,x_n) 编码器encoder将输入映射到连续表示z=(z_1,z_2,...,z_n) 解码器根据条件概率和自回归的方式生成输出句子：p_\\theta(y|x)=\\prod_&#123;t=1&#125;^m p_\\theta(y_t|z,y_1,...,y_&#123;t-1&#125;) 在每一时间步t，下一个token通过softmax分类器来计算：p_\\theta(y_t|z,y_1,...,y_&#123;t-1&#125;)=softmax(o_t) ot是Transfomer decoder的输出。 为了能够控制风格，我们在Transformer Encoder中加入了一个额外的风格向量:Encoder(x,s,\\theta_E) 因此，网络的输出变为:p_\\theta(y|x,s)=\\prod_&#123;t=1&#125;^m p_\\theta(y_t|z,y_1,...,y_&#123;t-1&#125;) 我们定义网络的预测输出为f_(x,x) 3.4 Discriminator Network因为大部分情况下缺少并行语料来进行有监督训练，我们提出了使用判别网络去从非平行语料中学习监督信息。我们构建了两种方法 为了保留内容信息，当我们将迁移语句$\\hat{y}=f_\\theta(x,\\hat{s})$送到带有原始标签s的Style Transfromer中时，我们训练网络重构原始输入语句x 对于风格控制，我们训练了一个判别网络去更好地控制被生成的句子。 判别网络是另一个TRM encoder,学习区分不同句子的风格。Style TRM网络接收风格监督信息从这个判别网络中。我们实验了两种判别网络： Conditional Discriminator 一个句子x和一个风格s被输入到判别网络$d_{\\phi}(x,s)$判别器需要输出句子x是否含有相关的风格。在判别器训练阶段，数据集x中的真是句子和重构句子$y=f_{\\theta}(x,s)$被标记为正样本，迁移的句子$\\hat{y}=f_\\theta(x,\\hat{s})$被标记为负样本。在Style TRM训练阶段，网络$f_\\theta$被训练最大化正样本概率当输入$\\hat{y}=f_\\theta(x,\\hat{s})$和$\\hat s$到判别器中。 Multi-class Discriminator 仅有一个句子x被输入到判别器$d_{\\phi}(x)$，判别器的目标是去回答此句话的风格。更具体地说，判别器是具有K+1类的分类器。前K类代表K种不同的风格，最后一类代表$f_\\theta(x,\\hat s)$生成的数据，也常被称为假样本。在判别器训练阶段，我们标记真实句子x和重构句子$y=f_{\\theta}(x,s)$为相关的风格，至于被迁移的句子$\\hat{y}=f_\\theta(x,\\hat{s})$被标记为类别0。在Style TRM学习阶段，我们训练网络$f_\\theta(x,\\hat{s})$最大化代表风格$\\hat s$的概率。 3.5 Learning Algorithm该模型的训练算法可分为两部分：判别器学习和风格变换网络学习。架构图如图2所示 3.5.1 Discriminator Learning我们训练我们的判别器来区分真实句字x和重构句子$y=f_{\\theta}(x,s)$与迁移句子$\\hat{y}=f_\\theta(x,\\hat{s})$。损失函数为交叉熵算法细节： 3.5.2 Style Transformer Learning根据$s=\\hat s$和$s!=\\hat s$分为两种情况。 Self Reconstruction 对于$s=\\hat s$，输入句子x和风格s都是来于相同数据集，可以直接训练Style TRM，去重构输入句子，通过最小化-log likehood: L_&#123;self&#125;(\\theta)=-p_&#123;\\theta&#125;(y=x|x,s) 对于$s!=\\hat s$，没有监督数据，不能直接获得loss，因此提出两种不同的训练loss： Cycle Reconstruction为了鼓励生成的句子保留输入句子x中的信息，我们将生成的句$\\hat{y}=f_\\theta(x,\\hat{s})$以x的风格输入给Style TRM，并训练我们的网络通过最小化负对数似然来重构原始输入句子： Style Controlling如果我们只训练我们的Style TRM从转移句$\\hat{y}=f_\\theta(x,\\hat{s})$重构输入句x，网络只能学习将输入复制到输出。为了处理这个退化问题，我们进一步为生成的句子添加了风格控制损失。即将网络生成的句子$\\hat y$输入到判别器中，以最大限度地提高$\\hat s$的概率。对于conditional discriminator，Style TRM的目标是在使用样式标签$\\hat s$输入到判别器时最小化类1的负对数似然性： 在multi-class discriminator的情况下，Style TRM以最小化对应类型的风格$\\hat s$的负对数似然： 组合loss之后，Style TRM学习过程如下： 3.5.3 Summarization and Discussion与GANs的训练过程类似（Goodfellow et al.，2014），在每个训练迭代中，我们首先执行$n_d$步鉴别器学习以获得更好的鉴别器，然后训练我们的Style TRM$n_f$步以提高其性能。算法3对训练过程进行了总结 由于自然语言的离散性，对于生成的句子$\\hat{y}=f_\\theta(x,\\hat{s})$，我们不能通过离散样本直接传播来自判别器的梯度。在我们的实验中，我们还观察到Gumbel-Softmax技巧会减慢模型的收敛速度，并且没有给模型带来太多的性能改进。基于以上原因，我们将fθ生成的softmax分布视为一个“软”生成句子，并将该分布输入给下游网络，以保持整个训练过程的连续性。当使用这种近似时，我们也将我们的解码器网络从贪婪解码切换到连续解码。也就是说，在每一个时间步，我们将整个softmax分布（等式（2））输入到网络，而不是将在先前预测步骤中具有最大概率的令牌馈送到网络。解码器利用这个分布，从输入的嵌入矩阵计算加权平均嵌入。 4 Experiment4.1 DatasetsYelp Review Dataset (Yelp) Yelp数据集是由Yelp数据集挑战提供的，由带有情绪标签（负面或正面）的餐馆和商业评论组成。在之前的工作之后，我们使用了Li等人（2018）提供的数据集。此外，它还为测试集提供人类参考句。 IMDb Movie Review Dataset (IMDb) IMDb数据集由在线用户撰写的影评组成。为了获得高质量的数据集，我们使用了Maas等人（2011）提供的极性电影评论。基于此数据集，我们通过以下步骤构建了一个高度极性的句子级风格转换数据集：1）在原始训练集上微调一个BERT（Devlin et al.，2018）分类器，在测试集上达到95%的准确率；2）将原始数据集中的每个评论分成几个句子；3） 通过我们微调的BERT分类器过滤掉置信阈值低于0.9的句子；4）删除不常见单词的句子。最后，这个数据集包含366K、4k和2k个句子，分别用于训练、验证和测试. 4.2 Evaluation目标转移句应该是一个流利的、内容完整的、具有目标风格的句子。为了评估不同模型的表现，我们在前人工作的基础上，比较了生成样本的三个不同维度：1）风格控制，2）内容保留(BLEU)，3）流畅性(perplexity)。 4.3 Training Details在所有的实验中，对于编码器、解码器和鉴别器，我们都使用了四层TRM，每层有四个注意头。Transformer中的隐藏大小、嵌入大小和位置编码大小都是256维。另一个包含256个隐藏单元的嵌入矩阵用来表示不同的风格，作为输入语句的额外标记输入到编码器中。并且位置编码不用于样式标记。对于鉴别器，类似于Radford et al.（2018）和Devlin et al.（2018），我们进一步在输入中添加标记，并将相应位置的输出向量馈送到表示鉴别器输出的softmax分类器中。 4.4 Experiment Results实验结果一些迁移例子 5.Conclusions and Future Work本文我们提出了Style Transfomer，实验结果在两个数据集中展示出了我们模型比之前的SOTA模型有竞争力的结果，特别地是，由于我们提出的方法没有假设一个分离的潜在表示来操纵句子的风格，我们的模型可以在两个数据集上得到更好的内容保留。","categories":[{"name":"文本生成","slug":"文本生成","permalink":"http://renxingkai.github.io/categories/文本生成/"}],"tags":[{"name":"文本风格迁移","slug":"文本风格迁移","permalink":"http://renxingkai.github.io/tags/文本风格迁移/"}],"author":"CinKate"},{"title":"常见的评估指标","slug":"metrics","date":"2021-01-06T19:29:52.000Z","updated":"2021-01-11T03:10:07.812Z","comments":true,"path":"2021/01/07/metrics/","link":"","permalink":"http://renxingkai.github.io/2021/01/07/metrics/","excerpt":"","text":"准确率，精确率，F1值，ROC，AUC，P-R，mAP(图来自网络，侵删)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://renxingkai.github.io/categories/机器学习/"}],"tags":[{"name":"评估指标","slug":"评估指标","permalink":"http://renxingkai.github.io/tags/评估指标/"}],"author":"CinKate"},{"title":"搜索中的NLP技术","slug":"searchandnlp","date":"2020-12-26T10:25:23.000Z","updated":"2021-01-06T11:33:26.871Z","comments":true,"path":"2020/12/26/searchandnlp/","link":"","permalink":"http://renxingkai.github.io/2020/12/26/searchandnlp/","excerpt":"","text":"搜索技术除了涉及基础的搜索算法，也涉及到很多NLP技术，本文转载于，只是做个学习记录，侵删。 推荐系统被捧为目前算法领域的主流，推荐系统不需要用户主动进行操作就能获取自己喜欢的东西，但是实际上，搜索系统在很长一段时间占据了重要位置，大到百度的大搜，小到音乐、视频、电商、应用商店等，都有各种各样的搜索引擎，这些搜索搜索能更为精准直接的满足用户需求，即使是推荐系统如日中天，目前也仍会有搜索的一席之地。今天我来为大家介绍，搜索系统中涉及的算法问题，也让大家了解，搜索中需要什么算法。 数据预处理模块这个很好理解吧，用户query进来，一般要做如下处理: 大小写转化。 删除标点符号（当然有的分析会保留标点，但是建议在这个场景下还是去掉更好）。 繁体转简体。 数字处理，转中文，转阿拉伯数字，甚至罗马字等，部分情况要删除数字。 长度截断。（总不能让用户输入一本西游记，然后你还在整吧？） 理解模块query理解其实是一个非常重要的模块，好的query理解能为后续的工作提供很大的支持。这部分往往不直接应对下游，而是按需使用，有点像辅助吧。 分词。基操。 关键词抽取。会涉及丢词、排序等功能。 命名实体识别。一方面协助用户识别，另一方面可能会涉及数据库查询匹配的内容。在垂搜中比较常见，大搜也有但是相比之下没那么精准和常见。 紧密度分析，避免由于切词出现错误导致词义偏移的问题，这个其实并不少见，尤其是在人名上，这个是别的精准度会很低，近期的如“武大靖”，会被分为“武大 靖”，“曾舜晞”直接被分为了3个字，挺头疼的。 非法信息过滤。例如黄色、暴力血腥、反动等。 改写模块改写模块其实非常关键，这是连接用户query和数据库底层数据的桥梁，数据库的存储有特定的形式，但是用户不会按照你的底层数据结构去写，例如，用户不见得会输入和平精英，而是吃鸡，数据库里可不见得会存吃鸡对吧，所以这块很重要。 同义词改写。上面的吃鸡就要改写为和平精英，这个需要通过同义词挖掘等方式去构造词典实现。 拼音改写。数据库是罗密欧与朱丽叶，但是用户输入的是罗密欧与朱莉业，拼音改写其实颇为常见，用户经常由于找不到需要的结果或者不知道应该需要哪个，于是直接输入后开始搜索。 前缀补全。非常常见，用户输入射雕，射雕英雄传就要出了，这个一般的方法也是构造词典，另外有一个很重要的需要了解的就是前缀树，这个能让你查询的时间非常低的水平（只和query长度本身有关）。 丢词和留词。结合上述关键词提取和命名实体识别完成，有些不必要的词汇需要被删除，例如“李小璐到底怎么了”，整个句子只有李小璐是关键词，其他词如果也通过and逻辑召回，就没有信息召回了，这时候其实可以直接删除或者将降级到or逻辑。留词和丢词相反，丢词如果是做减法，留词就是做加法。 近义词召回。这个召回不是从数据库中召回，而是召回近义词，具体的方法是通过embedding方法转化词汇，然后通过ball tree、simhash的方式召回与之意思相近的词汇，该模式虽然比较激进，但是能一定程度增加召回，有一定效果。 意图识别。与其说是意图，我更喜欢理解为这是直接针对底层数据结构产生的需要解决的问题，query这是一条，但是数据库有好几个，我们要去哪个数据库搜，这是需要识别的，而这个数据库的设计往往和品类、意图有关，找酒店、找景点都是不同的，所以此时就要进行意图识别，一般地方法是抽象为文本分类，但是很多时候语义本身是无法体现出真实意图的，例如少年的你，语义上其实很难分析出，有时候更复杂的会夹带一些实体识别、词性、词典之类的信息。 召回模块结合命名实体识别、改写结果，然后开始召回，模式比较多，包括但不限于下面形式： Elastic Search，著名的搜索平台ES，有些时候甚至简单的搜索平台直接用它管整个搜索引擎但是要精做，就只能把它当做底层的数据库和搜索平台。 关系数据库。MySQL之类的，但是在我的理解速度和并发性都不是特别好。 Redis。KV形式的数据库，速度很快，但是Key匹配必须是精确匹配，这需要改写模块具有很强的能力。 说白了，就是把用上面流程处理过的query放到数据库里面查，这个其实就是召回。 排序模块内容是召回回来了，其实怎么展现给用户呢？这里是需要深度挖掘用户的实际需求的，很多时候甚至需要做个性化，但是更多时候是我们能够得到的只有短短的一句用户query，那么，我们就要好好利用好这个query，来做好排序，让用户喜欢的（当然，还有广告商喜欢的哈哈哈）东西放在前面，实际上就是一个学习排序的问题了，这个我之前也做过一些简单的介绍，那么在特征层面，可以考虑如下的信息： query层面： 本身的embedding，后续迭代可以走elmo后逐渐形成的pre-train的模式。 词性、实体、词权重、offset等序列标注得到的结果。 document层面。 如果文档本身有文本，最好是标题类的，也把embeding引入，和query层面那种一样。如果只有query文本和document文本，其实就是个文本相似度模型了对吧。 综合层面： query和document的ctr、cqr、bm25，句向量余弦相似度等匹配信息。 其他层面： 意图识别结果。 模型上，DSSM系列似乎是比较流行的方法，但是提取一些特定的特征，有点的时候简单的LR、XGBOOST就能一定程度把问题解决好了。 而在排序模块中，还会涉及一些硬规则等。 搜索引导模块query suggestion是一个上述搜索过程非常类似的模块，只不过处理的结果大部分是放在离线，在线是指一个查字典的过程，那么离线部分，其实是做了一整个搜索过程的：预处理、query理解、改写、召回、排序。 预处理：和上面的预处理类似，不赘述。 query理解，和上述内容类似，但是有的时候会简化，直接进入改写模块。 改写模块，会进行容忍度更高的前缀匹配，去找回一些用户可能会喜欢的内容，这时候往往还会带上统计特征，从整体层面上看用户喜欢的内容，毕竟不同时间用户喜欢的可能不同，例如对热点新闻的偏好。 召回和排序模块。召回模块不是召回数据了，而是召回相似的doc信息，也可以是相似的历史用户query，尤其是后者，如果是后者，则要确认历史用户query是有结果的。 其他辅助模块显然，整个搜索系统远远不止这些内容，在算法视角下，其实还需要很多辅助模块协助我们进行算法开发。 日志模块。无论是线上的运行日志，还是线下的模型实验和离线模型运行，都需要日志协助，对于线上运行和离线模型运行而言，出现错误可以方便追溯，找到ERROR出现的时间和具体问题，而线下的模型试验能方便我们计算运行时间、找到bug，而非每次训练模型的时候功亏一篑才来找问题。 实验模块。由于算法策略存在很多不确定性，无论是算法内存占用和时间，还是算法实际结果，因此需要利用AB实验的方式来验证，快速进行分析，对有问题的算法及时下线检查原因。 数据分析和报表模块。结合实验模块，需要对日常用户数据进行分析，这个比较好理解，不赘述。 特征生产模块。特征生产涉及两块，一个是线上的实时计算，对于一些实时数据，是需要即时生成的，例如微博热搜里面就需要一些诸如5分钟搜索量之类的特征，这个只能从实时计算模块中获取；另一个是离线模块，为了进行离线模型训练，需要定时将生肉（原始数据）转化为特征，方便进行下一步计算，如果这个能变成日常任务，那开发人员就不需要临时造轮子执行，还要长时间等待。 定时任务模块。很多任务其实是定时开始的，报表生成、特征生产，甚至是一些实时任务（说白了就是短时间内的计算），因此有定时任务模块去管理这些定时任务，将会非常方便。 模型管理模块。首先一个项目中可能存在大量模型，然后因为各个模型训练需要花费大量资源，还要结合AB试验，另外还有模型的更新和保存机制（防止模型加载失败导致的无模型可用），因此需要构建模型模块统一管理模型，这个不是每个部门都有，或者说每个模型都是各自管理，但是有一个比较好管理模型平台是非常高效的。 数据平台。额，简单地说就是写SQL的地方，但是也有类似ETL之类的内容，和特征生产模块很接近。 小结搜索作为一个完整地系统，难度甚至比搜索要困难，在更多的场景下，搜索系统只能针对短短几个字的query进行分析，从而在海量数据中找到用户需要的东西，而由于是用户主动输入的，所以用户的预期非常高，但因为是用户主动的行为，所以不会太过复杂，甚至可能会有各种各样的歧义，只有详细分析和挖掘才能得到，因此在我看来，搜索这个事情非常具有挑战性，即使这个东西已经发展多年，年龄比推荐系统更大，还是很多可挖掘的地方。","categories":[{"name":"搜索","slug":"搜索","permalink":"http://renxingkai.github.io/categories/搜索/"}],"tags":[{"name":"search","slug":"search","permalink":"http://renxingkai.github.io/tags/search/"}],"author":"CinKate"},{"title":"Git常用复习","slug":"git-learn","date":"2020-12-23T20:55:42.000Z","updated":"2020-12-23T12:57:43.570Z","comments":true,"path":"2020/12/24/git-learn/","link":"","permalink":"http://renxingkai.github.io/2020/12/24/git-learn/","excerpt":"","text":"版本控制的分类 本地版本控制，如RCS。最常见，最简单的版本控制方法。 集中版本控制，如SVN。所有的版本数据都保存在服务器上，协同开发者从服务器上同步更新或上传自己的修改。必须联网。。。 分布式版本控制，如Git。所有版本信息仓库全部同步在本地的每个用户中，可以在本地查看所有版本历史，可以在本地离线提交，有网了之后push到服务器。但是每个人都有全部代码，存在安全隐患。 Git原理 git add files 将本地文件添加到暂存区stage，git commit 将暂存区文件提交到本地的git仓库，git push将本地仓库文件提交到远程仓库，如下图左边，右边对应反向操作 Workspace：工作目录，平时存放代码的位置 Index/Stage：暂存区，用于临时存放你的改动，事实上只是一个文件，保存即将提交到文件列表信息 Repository：仓库区(或本地仓库)，就是安全存放数据的位置，这里面有你提交到所有版本的数据。其中HEAD指向最新放入仓库的版本 Remote：远程仓库，托管代码的服务器，可以简单地认为是你项目组中的一台电脑用于远程数据交换 Git提交时忽略的文件在主目录下建立”.gitinore”文件，此文件以下规则常用#为注释*.txt #忽略所有.txt结尾的文件,这样的话，上传就不会被选中!lib.txt #但lib.txt除外/temp #仅忽略项目根目录下的TODO文件，不包括其他目录tempbuild/ #忽略build/目录下的所有文件doc/*.txt #会忽略 doc/notes.txt 但不包括 doc/server/arch.txt Git分支#列出所有本地分支git branch#列出所有远程分支git branch -r#新建一个分支git branch [branch-name]#虽然新建了但是默认还是在master分支下显示#新建一个分支并切换到该分支git checkout -b [branch-name]#合并指定分支到当前分支git merge [branch]#删除分支git branch -d [branch-name]#删除远程分支git push origin --delete [branch-name]git branch -dr [remote/branch] 多个分支如果并行执行，就会导致代码不会冲突，会同时存在多个版本！ web-api -A组开发 web-admin -B组开发 B会调用A web-app -C组开发 C会调用B和A的代码 如果冲突了要进行协商！ 如果同一个文件在合并分支时都被修改了，则会引起冲突：解决的办法是我们可以修改冲突文件后重新提交！选择要保留他的代码还是你的代码！ master主分支应该非常稳定，用来发布新版本，一般情况下不允许在上面工作，工作一般情况下在新建的dev分支上工作，工作完后，比如上要发布，或者说dev分值代码稳定后可以合并到主分支master上来。","categories":[{"name":"后台学习","slug":"后台学习","permalink":"http://renxingkai.github.io/categories/后台学习/"}],"tags":[{"name":"git","slug":"git","permalink":"http://renxingkai.github.io/tags/git/"}],"author":"CinKate"},{"title":"Docker基础学习","slug":"docker-learning","date":"2020-12-01T11:07:19.000Z","updated":"2020-12-22T09:10:06.146Z","comments":true,"path":"2020/12/01/docker-learning/","link":"","permalink":"http://renxingkai.github.io/2020/12/01/docker-learning/","excerpt":"","text":"Docker的常用命令帮助命令docker version #显示docker的版本信息docker info #显示docker的系统信息，包括镜像和容器的数量docker 命令 --help #帮助命令 帮助文档的地址：https://docs.docker.com/engine/reference/commandline/docker/ 镜像命令docker images #查看所有本地主机上的镜像docker images #查看所有本地主机上的镜像[root@VM-0-15-centos ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEhello-world latest bf756fb1ae65 11 months ago 13.3kB#解释REPOSITORY #镜像的仓库TAG #镜像的标签IMAGE ID #镜像的IDCREATED #镜像的创建时间SIZE #镜像的大小#可选项Options: -a, --all #列出所有镜像 --digests Show digests -q, --quiet #仅显示镜像的ID docker search #搜索镜像[root@VM-0-15-centos ~]# docker search tensorflowNAME DESCRIPTION STARS OFFICIAL AUTOMATEDtensorflow/tensorflow Official Docker images for the machine learn… 1799 jupyter/tensorflow-notebook Jupyter Notebook Scientific Python Stack w/ … 248 tensorflow/serving Official images for TensorFlow Serving (http… 102 rocm/tensorflow Tensorflow with ROCm backend support 58 xblaster/tensorflow-jupyter Dockerized Jupyter with tensorflow 54 [OK]floydhub/tensorflow tensorflow 26 [OK]bitnami/tensorflow-serving Bitnami Docker Image for TensorFlow Serving 14 [OK]opensciencegrid/tensorflow-gpu TensorFlow GPU set up for OSG 12 docker pull #下载镜像#docker pull mysql 镜像名[:tag][root@VM-0-15-centos ~]# docker pull mysqlUsing default tag: latest #不写tag，默认latestlatest: Pulling from library/mysql852e50cd189d: Pull complete #分层下载,docker 镜像的核心 联合文件系统29969ddb0ffb: Pull complete a43f41a44c48: Pull complete 5cdd802543a3: Pull complete b79b040de953: Pull complete 938c64119969: Pull complete 7689ec51a0d9: Pull complete a880ba7c411f: Pull complete 984f656ec6ca: Pull complete 9f497bce458a: Pull complete b9940f97694b: Pull complete 2f069358dc96: Pull complete Digest: sha256:4bb2e81a40e9d0d59bd8e3dc2ba5e1f2197696f6de39a91e90798dd27299b093 #签名Status: Downloaded newer image for mysql:latestdocker.io/library/mysql:latest #真实地址#指定版本docker pull mysql:5.7[root@VM-0-15-centos ~]# docker pull mysql:5.75.7: Pulling from library/mysql852e50cd189d: Already exists 29969ddb0ffb: Already exists a43f41a44c48: Already exists 5cdd802543a3: Already exists b79b040de953: Already exists 938c64119969: Already exists 7689ec51a0d9: Already exists 36bd6224d58f: Pull complete cab9d3fa4c8c: Pull complete 1b741e1c47de: Pull complete aac9d11987ac: Pull complete Digest: sha256:8e2004f9fe43df06c3030090f593021a5f283d028b5ed5765cc24236c2c4d88eStatus: Downloaded newer image for mysql:5.7docker.io/library/mysql:5.7 docker rmi #删除镜像#按docker id镜像删除[root@VM-0-15-centos ~]# docker rmi -f ae0658fdbad5 Untagged: mysql:5.7Untagged: mysql@sha256:8e2004f9fe43df06c3030090f593021a5f283d028b5ed5765cc24236c2c4d88eDeleted: sha256:ae0658fdbad5fb1c9413c998d8a573eeb5d16713463992005029c591e6400d02Deleted: sha256:a2cf831f4221764f4484ff0df961b54f1f949ed78220de1b24046843c55ac40fDeleted: sha256:0a7adcc95a91b1ec2beab283e0bfce5ccd6df590bd5a5e894954fcf27571e7f5Deleted: sha256:0fae465cbacf7c99aa90bc286689bc88a35d086f37fd931e03966d312d5dfb10Deleted: sha256:23af125b9e54a94c064bdfacc2414b1c8fba288aff48308e8117beb08b38cb19#递归删除所有的镜像[root@VM-0-15-centos ~]# docker rmi -f $(docker images -aq)Untagged: mysql:latestUntagged: mysql@sha256:4bb2e81a40e9d0d59bd8e3dc2ba5e1f2197696f6de39a91e90798dd27299b093Deleted: sha256:dd7265748b5dc3211208fb9aa232cef8d3fefd5d9a2a80d87407b8ea649e571cDeleted: sha256:aac9a624212bf416c3b41a62212caf12ed3c578d6b17b0f15be13a7dab56628dDeleted: sha256:1bf3ce09276e9e128108b166121e5d04abd16e7de7473b53b3018c6db0cf23ffDeleted: sha256:24c6444cea460c3cc2f4e0385e3e97819a0672a54a361921f95d4582583abd59Deleted: sha256:77585ebe3eaa035694084b3c5937fe82b8972aae1e6c6070fc4d7bc391d10928Deleted: sha256:1cfd539163ceb17f7bb85a0da968714fe9258b75dbf73f5ad45392a45cfd34b7Deleted: sha256:c37f414ac8d2b5e5d39f159a6dffd30b279c1268f30186cee5da721e451726eaDeleted: sha256:955b3c214bccf3ee2a7930768137fd7ed6a72677334be67a07c78a622abd318aDeleted: sha256:a2e35a0fdb20100365e2fb26c65357fcf926ac7990bf9074a51cbac5a8358d7eDeleted: sha256:8c3a028fc66f360ce6ce6c206786df68fac4c24257474cbe4f67eda0ac21efd6Deleted: sha256:0a6d37fabaceb4faa555e729a7d97cb6ee193cb97789a213907a3d3c156d7e35Deleted: sha256:579519c51de1afe1e29d284b1741af239a307975197cf6ce213a70068d923231Deleted: sha256:f5600c6330da7bb112776ba067a32a9c20842d6ecc8ee3289f1a713b644092f8Untagged: hello-world:latestUntagged: hello-world@sha256:e7c70bb24b462baa86c102610182e3efcb12a04854e8c582838d92970a09f323Deleted: sha256:bf756fb1ae65adf866bd8c456593cd24beb6a0a061dedf42b26a993176745f6b 容器命令有了镜像才能创建容器，linux，下载一个centos镜像来测试 [root@VM-0-15-centos ~]# docker pull centos 新建容器并启动 docker run [可选参数] image#参数说明--name=&quot;Name&quot; #容器名字-d #使用后台方式运行，类似nohup-it #使用交互方式运行，进入容器查看内容-p #指定容器端口 -p 8080:8080 -p 主机端口:容器端口 (常用) -p 容器端口 容器端口-P #随机指定端口 启动并进入容器 [root@VM-0-15-centos ~]# docker run -it centos /bin/bash[root@1faa1a0b849e /]# [root@1faa1a0b849e /]# lsbin etc lib lost+found mnt proc run srv tmp vardev home lib64 media opt root sbin sys usr#退出容器[root@1faa1a0b849e /]# exitexit[root@VM-0-15-centos ~]# ls 列出所有运行中的容器 #列出当前在运行的容器[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES#列出所有运行过的容器[root@VM-0-15-centos ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1faa1a0b849e centos &quot;/bin/bash&quot; 4 minutes ago Exited (0) 55 seconds ago frosty_cori0d482f3a4d60 bf756fb1ae65 &quot;/hello&quot; 34 hours ago Exited (0) 34 hours ago nifty_visvesvarayae05f6bfe06e0 bf756fb1ae65 &quot;/hello&quot; 34 hours ago Exited (0) 34 hours ago amazing_diffie8a40e14a2c36 bf756fb1ae65 &quot;/hello&quot; 34 hours ago Exited (0) 34 hours ago goofy_liskov#列出最近创建的容器[root@VM-0-15-centos ~]# docker ps -n=1CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1faa1a0b849e centos &quot;/bin/bash&quot; 5 minutes ago Exited (0) About a minute ago frosty_cori#只显示容器的编号[root@VM-0-15-centos ~]# docker ps -aq1faa1a0b849e0d482f3a4d60e05f6bfe06e08a40e14a2c36 退出容器 exit #直接容器停止并退出CTRL+P+Q #容器不停止并退出[root@VM-0-15-centos ~]# docker run -it centos /bin/bash[root@e5a4a59f28ff /]# [root@VM-0-15-centos ~]# [root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe5a4a59f28ff centos &quot;/bin/bash&quot; 21 seconds ago Up 20 seconds agitated_ritchie 删除容器 docker rm 容器id #删除指定的容器,不能删除正在运行的容器，如要删除，加-f#递归删除所有docker容器docker rm -f $(docker ps -aq) #删除所有容器docker ps -a -q|xargs docker rm[root@VM-0-15-centos ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe5a4a59f28ff centos &quot;/bin/bash&quot; 5 minutes ago Up 5 minutes agitated_ritchiec4fd9794c39a centos &quot;/bin/bash&quot; 5 minutes ago Exited (0) 5 minutes ago dreamy_lumiere1faa1a0b849e centos &quot;/bin/bash&quot; 16 minutes ago Exited (0) 12 minutes ago frosty_cori0d482f3a4d60 bf756fb1ae65 &quot;/hello&quot; 34 hours ago Exited (0) 34 hours ago nifty_visvesvarayae05f6bfe06e0 bf756fb1ae65 &quot;/hello&quot; 34 hours ago Exited (0) 34 hours ago amazing_diffie8a40e14a2c36 bf756fb1ae65 &quot;/hello&quot; 34 hours ago Exited (0) 34 hours ago goofy_liskov[root@VM-0-15-centos ~]# docker rm e5a4a59f28ffError response from daemon: You cannot remove a running container e5a4a59f28ffc678ab9ea5ed7a9af825ff053e1816cd94c90880b3d2a87997df. Stop the container before attempting removal or force remove[root@VM-0-15-centos ~]# docker rm -f $(docker ps -aq)e5a4a59f28ffc4fd9794c39a1faa1a0b849e0d482f3a4d60e05f6bfe06e08a40e14a2c36[root@VM-0-15-centos ~]# docker ps -aq[root@VM-0-15-centos ~]# 启动和停止容器的操作 docker start 容器id #启动 docker restart 容器id #重启docker stop 容器id #停止当前运行的容器docker kill 容器id #强制停止当前容器 常用其他命令后台启动容器 docker run -d centos #后台启动 #docker ps 时候，发现centos会停止，因为容器后台运行，就必须要一个前台进程，docker发现没有应用，就会自动停止 查看日志 docker logs -f -t --tail 10 容器id #发现没有日志 查看容器中的进程信息 docker top 容器id [root@VM-0-15-centos ~]# docker run -it centos /bin/bash[root@ba393184f315 /]# [root@VM-0-15-centos ~]# [root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESba393184f315 centos &quot;/bin/bash&quot; 9 seconds ago Up 8 seconds wizardly_cerf[root@VM-0-15-centos ~]# docker top ba393184f315UID PID PPID C STIME TTY TIME CMDroot 25856 25840 0 09:46 pts/0 00:00:00 /bin/bash 查看镜像的元数据 docker inspect 容器id [root@VM-0-15-centos ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESba393184f315 centos &quot;/bin/bash&quot; 3 minutes ago Up 3 minutes wizardly_cerf[root@VM-0-15-centos ~]# docker inspect ba393184f315[ &#123; &quot;Id&quot;: &quot;ba393184f3157b86b24aa040b24b4e169c3d1aed0ca56edee01596a7f833fd73&quot;, &quot;Created&quot;: &quot;2020-12-01T01:46:02.700180427Z&quot;, &quot;Path&quot;: &quot;/bin/bash&quot;, &quot;Args&quot;: [], &quot;State&quot;: &#123; &quot;Status&quot;: &quot;running&quot;, &quot;Running&quot;: true, &quot;Paused&quot;: false, &quot;Restarting&quot;: false, &quot;OOMKilled&quot;: false, &quot;Dead&quot;: false, &quot;Pid&quot;: 25856, &quot;ExitCode&quot;: 0, &quot;Error&quot;: &quot;&quot;, &quot;StartedAt&quot;: &quot;2020-12-01T01:46:02.955068318Z&quot;, &quot;FinishedAt&quot;: &quot;0001-01-01T00:00:00Z&quot; &#125;, &quot;Image&quot;: &quot;sha256:0d120b6ccaa8c5e149176798b3501d4dd1885f961922497cd0abef155c869566&quot;, &quot;ResolvConfPath&quot;: &quot;/var/lib/docker/containers/ba393184f3157b86b24aa040b24b4e169c3d1aed0ca56edee01596a7f833fd73/resolv.conf&quot;, &quot;HostnamePath&quot;: &quot;/var/lib/docker/containers/ba393184f3157b86b24aa040b24b4e169c3d1aed0ca56edee01596a7f833fd73/hostname&quot;, &quot;HostsPath&quot;: &quot;/var/lib/docker/containers/ba393184f3157b86b24aa040b24b4e169c3d1aed0ca56edee01596a7f833fd73/hosts&quot;, &quot;LogPath&quot;: &quot;/var/lib/docker/containers/ba393184f3157b86b24aa040b24b4e169c3d1aed0ca56edee01596a7f833fd73/ba393184f3157b86b24aa040b24b4e169c3d1aed0ca56edee01596a7f833fd73-json.log&quot;, &quot;Name&quot;: &quot;/wizardly_cerf&quot;, &quot;RestartCount&quot;: 0, &quot;Driver&quot;: &quot;overlay2&quot;, &quot;Platform&quot;: &quot;linux&quot;, &quot;MountLabel&quot;: &quot;&quot;, &quot;ProcessLabel&quot;: &quot;&quot;, &quot;AppArmorProfile&quot;: &quot;&quot;, &quot;ExecIDs&quot;: null, &quot;HostConfig&quot;: &#123; &quot;Binds&quot;: null, &quot;ContainerIDFile&quot;: &quot;&quot;, &quot;LogConfig&quot;: &#123; &quot;Type&quot;: &quot;json-file&quot;, &quot;Config&quot;: &#123;&#125; &#125;, &quot;NetworkMode&quot;: &quot;default&quot;, &quot;PortBindings&quot;: &#123;&#125;, &quot;RestartPolicy&quot;: &#123; &quot;Name&quot;: &quot;no&quot;, &quot;MaximumRetryCount&quot;: 0 &#125;, &quot;AutoRemove&quot;: false, &quot;VolumeDriver&quot;: &quot;&quot;, &quot;VolumesFrom&quot;: null, &quot;CapAdd&quot;: null, &quot;CapDrop&quot;: null, &quot;Capabilities&quot;: null, &quot;Dns&quot;: [], &quot;DnsOptions&quot;: [], &quot;DnsSearch&quot;: [], &quot;ExtraHosts&quot;: null, &quot;GroupAdd&quot;: null, &quot;IpcMode&quot;: &quot;private&quot;, &quot;Cgroup&quot;: &quot;&quot;, &quot;Links&quot;: null, &quot;OomScoreAdj&quot;: 0, &quot;PidMode&quot;: &quot;&quot;, &quot;Privileged&quot;: false, &quot;PublishAllPorts&quot;: false, &quot;ReadonlyRootfs&quot;: false, &quot;SecurityOpt&quot;: null, &quot;UTSMode&quot;: &quot;&quot;, &quot;UsernsMode&quot;: &quot;&quot;, &quot;ShmSize&quot;: 67108864, &quot;Runtime&quot;: &quot;runc&quot;, &quot;ConsoleSize&quot;: [ 0, 0 ], &quot;Isolation&quot;: &quot;&quot;, &quot;CpuShares&quot;: 0, &quot;Memory&quot;: 0, &quot;NanoCpus&quot;: 0, &quot;CgroupParent&quot;: &quot;&quot;, &quot;BlkioWeight&quot;: 0, &quot;BlkioWeightDevice&quot;: [], &quot;BlkioDeviceReadBps&quot;: null, &quot;BlkioDeviceWriteBps&quot;: null, &quot;BlkioDeviceReadIOps&quot;: null, &quot;BlkioDeviceWriteIOps&quot;: null, &quot;CpuPeriod&quot;: 0, &quot;CpuQuota&quot;: 0, &quot;CpuRealtimePeriod&quot;: 0, &quot;CpuRealtimeRuntime&quot;: 0, &quot;CpusetCpus&quot;: &quot;&quot;, &quot;CpusetMems&quot;: &quot;&quot;, &quot;Devices&quot;: [], &quot;DeviceCgroupRules&quot;: null, &quot;DeviceRequests&quot;: null, &quot;KernelMemory&quot;: 0, &quot;KernelMemoryTCP&quot;: 0, &quot;MemoryReservation&quot;: 0, &quot;MemorySwap&quot;: 0, &quot;MemorySwappiness&quot;: null, &quot;OomKillDisable&quot;: false, &quot;PidsLimit&quot;: null, &quot;Ulimits&quot;: null, &quot;CpuCount&quot;: 0, &quot;CpuPercent&quot;: 0, &quot;IOMaximumIOps&quot;: 0, &quot;IOMaximumBandwidth&quot;: 0, &quot;MaskedPaths&quot;: [ &quot;/proc/asound&quot;, &quot;/proc/acpi&quot;, &quot;/proc/kcore&quot;, &quot;/proc/keys&quot;, &quot;/proc/latency_stats&quot;, &quot;/proc/timer_list&quot;, &quot;/proc/timer_stats&quot;, &quot;/proc/sched_debug&quot;, &quot;/proc/scsi&quot;, &quot;/sys/firmware&quot; ], &quot;ReadonlyPaths&quot;: [ &quot;/proc/bus&quot;, &quot;/proc/fs&quot;, &quot;/proc/irq&quot;, &quot;/proc/sys&quot;, &quot;/proc/sysrq-trigger&quot; ] &#125;, &quot;GraphDriver&quot;: &#123; &quot;Data&quot;: &#123; &quot;LowerDir&quot;: &quot;/var/lib/docker/overlay2/78d46a1dd3114ea4472382c70d189091005ab80b93b328fdf6917ea374e09135-init/diff:/var/lib/docker/overlay2/9977c7654d8482400f51116d18f3c0562764ac2c1082c8de455e689c10593b30/diff&quot;, &quot;MergedDir&quot;: &quot;/var/lib/docker/overlay2/78d46a1dd3114ea4472382c70d189091005ab80b93b328fdf6917ea374e09135/merged&quot;, &quot;UpperDir&quot;: &quot;/var/lib/docker/overlay2/78d46a1dd3114ea4472382c70d189091005ab80b93b328fdf6917ea374e09135/diff&quot;, &quot;WorkDir&quot;: &quot;/var/lib/docker/overlay2/78d46a1dd3114ea4472382c70d189091005ab80b93b328fdf6917ea374e09135/work&quot; &#125;, &quot;Name&quot;: &quot;overlay2&quot; &#125;, &quot;Mounts&quot;: [], &quot;Config&quot;: &#123; &quot;Hostname&quot;: &quot;ba393184f315&quot;, &quot;Domainname&quot;: &quot;&quot;, &quot;User&quot;: &quot;&quot;, &quot;AttachStdin&quot;: true, &quot;AttachStdout&quot;: true, &quot;AttachStderr&quot;: true, &quot;Tty&quot;: true, &quot;OpenStdin&quot;: true, &quot;StdinOnce&quot;: true, &quot;Env&quot;: [ &quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot; ], &quot;Cmd&quot;: [ &quot;/bin/bash&quot; ], &quot;Image&quot;: &quot;centos&quot;, &quot;Volumes&quot;: null, &quot;WorkingDir&quot;: &quot;&quot;, &quot;Entrypoint&quot;: null, &quot;OnBuild&quot;: null, &quot;Labels&quot;: &#123; &quot;org.label-schema.build-date&quot;: &quot;20200809&quot;, &quot;org.label-schema.license&quot;: &quot;GPLv2&quot;, &quot;org.label-schema.name&quot;: &quot;CentOS Base Image&quot;, &quot;org.label-schema.schema-version&quot;: &quot;1.0&quot;, &quot;org.label-schema.vendor&quot;: &quot;CentOS&quot; &#125; &#125;, &quot;NetworkSettings&quot;: &#123; &quot;Bridge&quot;: &quot;&quot;, &quot;SandboxID&quot;: &quot;913e925d94cac8aaa4ead2ed29197af7d07d4fbfd35625642019116a42cff156&quot;, &quot;HairpinMode&quot;: false, &quot;LinkLocalIPv6Address&quot;: &quot;&quot;, &quot;LinkLocalIPv6PrefixLen&quot;: 0, &quot;Ports&quot;: &#123;&#125;, &quot;SandboxKey&quot;: &quot;/var/run/docker/netns/913e925d94ca&quot;, &quot;SecondaryIPAddresses&quot;: null, &quot;SecondaryIPv6Addresses&quot;: null, &quot;EndpointID&quot;: &quot;9da28d1e996685ffa1a39d26fe042919d661c4b4c8267f3470f2042ee78810ed&quot;, &quot;Gateway&quot;: &quot;172.18.0.1&quot;, &quot;GlobalIPv6Address&quot;: &quot;&quot;, &quot;GlobalIPv6PrefixLen&quot;: 0, &quot;IPAddress&quot;: &quot;172.18.0.2&quot;, &quot;IPPrefixLen&quot;: 16, &quot;IPv6Gateway&quot;: &quot;&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:12:00:02&quot;, &quot;Networks&quot;: &#123; &quot;bridge&quot;: &#123; &quot;IPAMConfig&quot;: null, &quot;Links&quot;: null, &quot;Aliases&quot;: null, &quot;NetworkID&quot;: &quot;df77c62f666112b12946dbbbc16f1dcdbbcc8548e3fa56519fd062e5331a0c08&quot;, &quot;EndpointID&quot;: &quot;9da28d1e996685ffa1a39d26fe042919d661c4b4c8267f3470f2042ee78810ed&quot;, &quot;Gateway&quot;: &quot;172.18.0.1&quot;, &quot;IPAddress&quot;: &quot;172.18.0.2&quot;, &quot;IPPrefixLen&quot;: 16, &quot;IPv6Gateway&quot;: &quot;&quot;, &quot;GlobalIPv6Address&quot;: &quot;&quot;, &quot;GlobalIPv6PrefixLen&quot;: 0, &quot;MacAddress&quot;: &quot;02:42:ac:12:00:02&quot;, &quot;DriverOpts&quot;: null &#125; &#125; &#125; &#125;] 进入当前运行的容器 通常容器都是后台运行的，有时我们需要进入容器修改一些配置 docker exec -it 容器id bashShell [root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESba393184f315 centos \"/bin/bash\" 11 minutes ago Up 11 minutes wizardly_cerf[root@VM-0-15-centos ~]# docker exec -it ba393184f315 /bin/bash[root@ba393184f315 /]# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 01:46 pts/0 00:00:00 /bin/bashroot 14 0 0 01:57 pts/1 00:00:00 /bin/bashroot 27 14 0 01:57 pts/1 00:00:00 ps -ef docker attach 容器id [root@VM-0-15-centos ~]# docker attach ba393184f315[root@ba393184f315 /]# docker exec 进入容器后，开启新的终端，可以在里面操作docker attach 进入容器正在执行的终端 从容器拷贝文件到主机 docker cp 容器id:容器内路径 目的的主机路径 [root@VM-0-15-centos home]# docker attach b67b64394534[root@b67b64394534 /]# cd /home [root@b67b64394534 home]# ls[root@b67b64394534 home]# touch rx.java[root@b67b64394534 home]# read escape sequence[root@VM-0-15-centos home]# [root@VM-0-15-centos home]# docker cp b67b64394534:/home/rx.java /home[root@VM-0-15-centos home]# lsrx.java rxk.java Test1-Docker部属nginx#下载Nginx[root@VM-0-15-centos ~]# docker pull nginxUsing default tag: latestlatest: Pulling from library/nginx852e50cd189d: Pull complete 571d7e852307: Pull complete addb10abd9cb: Pull complete d20aa7ccdb77: Pull complete 8b03f1e11359: Pull complete Digest: sha256:6b1daa9462046581ac15be20277a7c75476283f969cb3a61c8725ec38d3b01c3Status: Downloaded newer image for nginx:latestdocker.io/library/nginx:latest#查看目前运行镜像[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb67b64394534 centos \"/bin/bash\" 10 hours ago Up 10 hours suspicious_sutherland[root@VM-0-15-centos ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEnginx latest bc9a0695f571 6 days ago 133MBcentos latest 0d120b6ccaa8 3 months ago 215MB#以后台方式运行镜像，并且将nginx的80端口映射到本地3344端口，同时给此镜像一个名字--name nginx01[root@VM-0-15-centos ~]# docker run -d --name nginx01 -p 3344:80 nginx5a3e8a8e16e5e1f46b8bbe6c23662aebd3c13dc5cacfd63fb24d0703c1235819[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5a3e8a8e16e5 nginx \"/docker-entrypoint.…\" 2 minutes ago Up 2 minutes 0.0.0.0:3344-&gt;80/tcp nginx01b67b64394534 centos \"/bin/bash\" 10 hours ago Up 10 hours suspicious_sutherland#请求本地3344端口，显示nginx，成功运行[root@VM-0-15-centos ~]# curl localhost:3344&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;#进入容器[root@VM-0-15-centos ~]# docker exec -it nginx01 /bin/bashroot@5a3e8a8e16e5:/# whereis nginxnginx: /usr/sbin/nginx /usr/lib/nginx /etc/nginx /usr/share/nginx#关闭容器[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5a3e8a8e16e5 nginx \"/docker-entrypoint.…\" 33 minutes ago Up 33 minutes 0.0.0.0:3344-&gt;80/tcp nginx01b67b64394534 centos \"/bin/bash\" 10 hours ago Up 10 hours suspicious_sutherland[root@VM-0-15-centos ~]# docker stop nginx01nginx01[root@VM-0-15-centos ~]# docker stop centosError response from daemon: No such container: centos[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb67b64394534 centos \"/bin/bash\" 10 hours ago Up 10 hours suspicious_sutherland[root@VM-0-15-centos ~]# docker stop b67b64394534b67b64394534 Test2-Docker部属tomcat#下载并启动tomcat (加了--rm 意味着用完就会删除)[root@VM-0-15-centos ~]# docker run -it --rm tomcat:9.0Unable to find image 'tomcat:9.0' locally9.0: Pulling from library/tomcat756975cb9c7e: Pull complete d77915b4e630: Pull complete 5f37a0a41b6b: Pull complete 96b2c1e36db5: Pull complete 27a2d52b526e: Pull complete a867dba77389: Pull complete 0939c055fb79: Pull complete 0b0694ce0ae2: Pull complete 81a5f8099e05: Pull complete c3d7917d545e: Pull complete#正常下载[root@VM-0-15-centos ~]# docker pull tomcat:9.09.0: Pulling from library/tomcatDigest: sha256:a319b10d8729817c7ce0bcc2343a6f97711c7870395019340d96b6aafd6ccbeaStatus: Image is up to date for tomcat:9.0docker.io/library/tomcat:9.0#将tomcat 8080端口映射到外部3355端口[root@VM-0-15-centos ~]# docker run -d -p 3355:8080 --name tomcat01 tomcataf2d7437747112b51f83c556b1790503f41e2632bbbe965e00a8bff3d3268c92#进入tomcat[root@VM-0-15-centos ~]# docker exec -it tomcat01 /bin/bashroot@af2d74377471:/usr/local/tomcat# lsBUILDING.txt LICENSE README.md RUNNING.txt conf logs temp webapps.distCONTRIBUTING.md NOTICE RELEASE-NOTES bin lib native-jni-lib webapps work Test3-Docker部属ES+kibanaES暴露的端口较多，十分耗内存，ES的数据一般挂载在安全目录 # --net somenetwork网络配置docker run -d --name elasticsearch --net somenetwork -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" elasticsearch:tag#下载并启动ESdocker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" elasticsearch:7.6.2#启动之后 服务器会较为卡顿#查看CPU和内存状态#docker stats #增加内存限制 -e环境配置修改docker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -e ES_JAVA_OPTS=\"-Xms64m -Xmx512m\" elasticsearch:7.6.2CONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDSdb5428acc88d elasticsearch02 0.77% 358.6MiB / 1.795GiB 19.51% 656B / 0B 6.41MB / 729kB 43^C[root@VM-0-15-centos ~]# curl localhost:9200&#123; \"name\" : \"db5428acc88d\", \"cluster_name\" : \"docker-cluster\", \"cluster_uuid\" : \"MqrmYUWDTT2jYZJHxsVU0A\", \"version\" : &#123; \"number\" : \"7.6.2\", \"build_flavor\" : \"default\", \"build_type\" : \"docker\", \"build_hash\" : \"ef48eb35cf30adf4db14086e8aabd07ef6fb113f\", \"build_date\" : \"2020-03-26T06:34:37.794943Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.4.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" &#125;, \"tagline\" : \"You Know, for Search\"&#125; 可视化启动portainer [root@VM-0-15-centos ~]# docker run -d -p 8088:9000 --restart=always -v /var/run/docker.sock:/var/run/docker.sock --privileged=true portainer/portainerUnable to find image 'portainer/portainer:latest' locallylatest: Pulling from portainer/portainerd1e017099d17: Pull complete 717377b83d5c: Pull complete Digest: sha256:f8c2b0a9ca640edf508a8a0830cf1963a1e0d2fd9936a64104b3f658e120b868Status: Downloaded newer image for portainer/portainer:latestcb19cef5927877f6e10e15013b81fffdaefd5cb2e426f34819fc869c109ab7b3 commit镜像#先启动tomcat 并确保在运行[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES675a7261e3b7 tomcat \"catalina.sh run\" 55 seconds ago Up 54 seconds 0.0.0.0:8080-&gt;8080/tcp heuristic_curie#进入tomcat[root@VM-0-15-centos ~]# docker exec -it 675a7261e3b7 /bin/bash#将webapps中的东西复制上去root@675a7261e3b7:/usr/local/tomcat# cd webappsroot@675a7261e3b7:/usr/local/tomcat/webapps# lsroot@675a7261e3b7:/usr/local/tomcat/webapps# cd ..root@675a7261e3b7:/usr/local/tomcat# cp -r webapps.dist/* webappsroot@675a7261e3b7:/usr/local/tomcat# cd webappsroot@675a7261e3b7:/usr/local/tomcat/webapps# lsROOT docs examples host-manager managerroot@675a7261e3b7:/usr/local/tomcat/webapps# exit[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES675a7261e3b7 tomcat \"catalina.sh run\" 7 minutes ago Up 7 minutes 0.0.0.0:8080-&gt;8080/tcp heuristic_curie#commit一个镜像,-a是作者,-m是描述[root@VM-0-15-centos ~]# docker commit -a=\"cinkate\" -m=\"add webapps\" 675a7261e3b7 tomcat02:1.0sha256:80b6126c75b93053ee013c1a724a10fb958ddab21e5b0f3f10a58dbb996c74eb[root@VM-0-15-centos ~]# docker imgaesdocker: 'imgaes' is not a docker command.See 'docker --help'#查看是否commit成功[root@VM-0-15-centos ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEtomcat02 1.0 80b6126c75b9 8 seconds ago 654MBnginx latest bc9a0695f571 8 days ago 133MBtomcat 9.0 e0bd8b34b4ea 2 weeks ago 649MBtomcat latest e0bd8b34b4ea 2 weeks ago 649MBcentos latest 0d120b6ccaa8 3 months ago 215MBportainer/portainer latest 62771b0b9b09 4 months ago 79.1MBelasticsearch 7.6.2 f29a1ee41030 8 months ago 791MB docker容器数据卷即使删除了容器，数据还是存在的，类似于数据持久化容器之间应该有一个数据共享的技术！Docker容器中产生的数据，同步到本地！ 这就是卷技术！目录的挂载，将容器的目录，挂载到Linux上面！ 总结：为了容器的持久化和同步操作~，容器间也是可以数据共享！ 使用数据卷 方式一：直接使用命令来挂载 #-v 主机目录，容器内目录#-p 主机端口，容器内端口[root@VM-0-15-centos ~]# docker run -it -v#将centos /home下的目录 挂载到 主机下/home/ceshi下[root@VM-0-15-centos home]# docker run -it -v /home/ceshi:/home centos /bin/bash[root@081342dcbd4f /]# #另一个terminal(本机的home目录)[root@VM-0-15-centos ~]# cd /home[root@VM-0-15-centos home]# lsceshi rx.java rxk.java#查看元信息，挂载成功 Mounts[root@VM-0-15-centos home]# docker inspect 081342dcbd4f[ &#123; \"Id\": \"081342dcbd4f81f7d9d8495b8741dc2270b88d89123f5f422802cece726e75ee\", \"Created\": \"2020-12-03T13:39:28.974545204Z\", \"Path\": \"/bin/bash\", \"Args\": [], \"State\": &#123; \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 1314, \"ExitCode\": 0, \"Error\": \"\", \"StartedAt\": \"2020-12-03T13:39:29.238082253Z\", \"FinishedAt\": \"0001-01-01T00:00:00Z\" &#125;, \"Image\": \"sha256:0d120b6ccaa8c5e149176798b3501d4dd1885f961922497cd0abef155c869566\", \"ResolvConfPath\": \"/var/lib/docker/containers/081342dcbd4f81f7d9d8495b8741dc2270b88d89123f5f422802cece726e75ee/resolv.conf\", \"HostnamePath\": \"/var/lib/docker/containers/081342dcbd4f81f7d9d8495b8741dc2270b88d89123f5f422802cece726e75ee/hostname\", \"HostsPath\": \"/var/lib/docker/containers/081342dcbd4f81f7d9d8495b8741dc2270b88d89123f5f422802cece726e75ee/hosts\", \"LogPath\": \"/var/lib/docker/containers/081342dcbd4f81f7d9d8495b8741dc2270b88d89123f5f422802cece726e75ee/081342dcbd4f81f7d9d8495b8741dc2270b88d89123f5f422802cece726e75ee-json.log\", \"Name\": \"/clever_lovelace\", \"RestartCount\": 0, \"Driver\": \"overlay2\", \"Platform\": \"linux\", \"MountLabel\": \"\", \"ProcessLabel\": \"\", \"AppArmorProfile\": \"\", \"ExecIDs\": null, \"HostConfig\": &#123; \"Binds\": [ \"/home/ceshi:/home\" ], \"ContainerIDFile\": \"\", \"LogConfig\": &#123; \"Type\": \"json-file\", \"Config\": &#123;&#125; &#125;, \"NetworkMode\": \"default\", \"PortBindings\": &#123;&#125;, \"RestartPolicy\": &#123; \"Name\": \"no\", \"MaximumRetryCount\": 0 &#125;, \"AutoRemove\": false, \"VolumeDriver\": \"\", \"VolumesFrom\": null, \"CapAdd\": null, \"CapDrop\": null, \"Capabilities\": null, \"Dns\": [], \"DnsOptions\": [], \"DnsSearch\": [], \"ExtraHosts\": null, \"GroupAdd\": null, \"IpcMode\": \"private\", \"Cgroup\": \"\", \"Links\": null, \"OomScoreAdj\": 0, \"PidMode\": \"\", \"Privileged\": false, \"PublishAllPorts\": false, \"ReadonlyRootfs\": false, \"SecurityOpt\": null, \"UTSMode\": \"\", \"UsernsMode\": \"\", \"ShmSize\": 67108864, \"Runtime\": \"runc\", \"ConsoleSize\": [ 0, 0 ], \"Isolation\": \"\", \"CpuShares\": 0, \"Memory\": 0, \"NanoCpus\": 0, \"CgroupParent\": \"\", \"BlkioWeight\": 0, \"BlkioWeightDevice\": [], \"BlkioDeviceReadBps\": null, \"BlkioDeviceWriteBps\": null, \"BlkioDeviceReadIOps\": null, \"BlkioDeviceWriteIOps\": null, \"CpuPeriod\": 0, \"CpuQuota\": 0, \"CpuRealtimePeriod\": 0, \"CpuRealtimeRuntime\": 0, \"CpusetCpus\": \"\", \"CpusetMems\": \"\", \"Devices\": [], \"DeviceCgroupRules\": null, \"DeviceRequests\": null, \"KernelMemory\": 0, \"KernelMemoryTCP\": 0, \"MemoryReservation\": 0, \"MemorySwap\": 0, \"MemorySwappiness\": null, \"OomKillDisable\": false, \"PidsLimit\": null, \"Ulimits\": null, \"CpuCount\": 0, \"CpuPercent\": 0, \"IOMaximumIOps\": 0, \"IOMaximumBandwidth\": 0, \"MaskedPaths\": [ \"/proc/asound\", \"/proc/acpi\", \"/proc/kcore\", \"/proc/keys\", \"/proc/latency_stats\", \"/proc/timer_list\", \"/proc/timer_stats\", \"/proc/sched_debug\", \"/proc/scsi\", \"/sys/firmware\" ], \"ReadonlyPaths\": [ \"/proc/bus\", \"/proc/fs\", \"/proc/irq\", \"/proc/sys\", \"/proc/sysrq-trigger\" ] &#125;, \"GraphDriver\": &#123; \"Data\": &#123; \"LowerDir\": \"/var/lib/docker/overlay2/519ce3c698154cf997249d71ab41e9203f413522d25e777dc5f8544f2ba22c9e-init/diff:/var/lib/docker/overlay2/9977c7654d8482400f51116d18f3c0562764ac2c1082c8de455e689c10593b30/diff\", \"MergedDir\": \"/var/lib/docker/overlay2/519ce3c698154cf997249d71ab41e9203f413522d25e777dc5f8544f2ba22c9e/merged\", \"UpperDir\": \"/var/lib/docker/overlay2/519ce3c698154cf997249d71ab41e9203f413522d25e777dc5f8544f2ba22c9e/diff\", \"WorkDir\": \"/var/lib/docker/overlay2/519ce3c698154cf997249d71ab41e9203f413522d25e777dc5f8544f2ba22c9e/work\" &#125;, \"Name\": \"overlay2\" &#125;, \"Mounts\": [ &#123; \"Type\": \"bind\", \"Source\": \"/home/ceshi\", \"Destination\": \"/home\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"rprivate\" &#125; ], \"Config\": &#123; \"Hostname\": \"081342dcbd4f\", \"Domainname\": \"\", \"User\": \"\", \"AttachStdin\": true, \"AttachStdout\": true, \"AttachStderr\": true, \"Tty\": true, \"OpenStdin\": true, \"StdinOnce\": true, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], \"Cmd\": [ \"/bin/bash\" ], \"Image\": \"centos\", \"Volumes\": null, \"WorkingDir\": \"\", \"Entrypoint\": null, \"OnBuild\": null, \"Labels\": &#123; \"org.label-schema.build-date\": \"20200809\", \"org.label-schema.license\": \"GPLv2\", \"org.label-schema.name\": \"CentOS Base Image\", \"org.label-schema.schema-version\": \"1.0\", \"org.label-schema.vendor\": \"CentOS\" &#125; &#125;, \"NetworkSettings\": &#123; \"Bridge\": \"\", \"SandboxID\": \"7cc48bb3849fdd8ca238a8ba1f09409e01e7c50856abbccf23d59caaf5997c8d\", \"HairpinMode\": false, \"LinkLocalIPv6Address\": \"\", \"LinkLocalIPv6PrefixLen\": 0, \"Ports\": &#123;&#125;, \"SandboxKey\": \"/var/run/docker/netns/7cc48bb3849f\", \"SecondaryIPAddresses\": null, \"SecondaryIPv6Addresses\": null, \"EndpointID\": \"6bcd4d80b78f3f8ddeb0ee5468ba2e738509eafd745ca2a5e969ff0fa6dceb08\", \"Gateway\": \"172.18.0.1\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"IPAddress\": \"172.18.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"MacAddress\": \"02:42:ac:12:00:02\", \"Networks\": &#123; \"bridge\": &#123; \"IPAMConfig\": null, \"Links\": null, \"Aliases\": null, \"NetworkID\": \"df77c62f666112b12946dbbbc16f1dcdbbcc8548e3fa56519fd062e5331a0c08\", \"EndpointID\": \"6bcd4d80b78f3f8ddeb0ee5468ba2e738509eafd745ca2a5e969ff0fa6dceb08\", \"Gateway\": \"172.18.0.1\", \"IPAddress\": \"172.18.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"MacAddress\": \"02:42:ac:12:00:02\", \"DriverOpts\": null &#125; &#125; &#125; &#125;]#容器的home目录下[root@081342dcbd4f home]# touch test.java[root@081342dcbd4f home]# lstest.java#主机挂载的ceshi目录下[root@VM-0-15-centos home]# cd ceshi/[root@VM-0-15-centos ceshi]# lstest.java 关闭容器之后，在主机中挂载的目录中创建，修改文件，都是可以同步到容器中的目录。 Test4–安装MySQL#获取镜像[root@VM-0-15-centos home]# docker pull mysql:5.75.7: Pulling from library/mysql852e50cd189d: Already exists 29969ddb0ffb: Pull complete a43f41a44c48: Pull complete 5cdd802543a3: Pull complete b79b040de953: Pull complete 938c64119969: Pull complete 7689ec51a0d9: Pull complete 36bd6224d58f: Pull complete cab9d3fa4c8c: Pull complete 1b741e1c47de: Pull complete aac9d11987ac: Pull complete Digest: sha256:8e2004f9fe43df06c3030090f593021a5f283d028b5ed5765cc24236c2c4d88eStatus: Downloaded newer image for mysql:5.7docker.io/library/mysql:5.7#运行容器，需要做数据挂载# -d后台启动# -p端口映射# -v 挂载配置文件 和 数据文件# -e 修改环境变量，修改密码# --name 重命名[root@VM-0-15-centos home]# docker run -d -p 3310:3306 -v /home/mysql/conf:/etc/mysql/conf.d -v /home/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 --name mysql01 mysql:5.72f81782daf7d49e73365da7def107120a8f79c71b9c5bb21a4fc082f2ebef664#启动成功后，使用本地数据库管理软件可以进行正常连接 假设将容器删除，挂载在本地的数据卷不会丢失，实现了容器的持久化技术。 具名和匿名挂载#匿名挂载-v不写本地路径，会匿名进行挂载docker run -d -P --name nginx01 -v /etc/nginx nginx#具名挂载docker run -d -P --name nginx02 -v juming-nginx:/etc/nginx nginx#通过-v 卷名:容器内路径#查看这个卷所有docker容器内的卷，没有指定目录的情况下：都是在/var/lib/docker/volumes/xxxx/_data通过具名挂载，可以方便的找到一个卷，大多数情况使用具名挂载 如何确定是具名还是匿名挂载？还是指定路径挂载？ -v 容器内路径 #匿名挂载-v 卷名:容器内路径 #具名挂载-v /宿主机路径:容器内路径 #指定路径挂载 拓展 #通过 -v 容器内路径:ro rw 可以改变读写权限#ro readonly#rw readwrite#一旦设定了容器权限，容器对我们挂载出来的内容就有限定了！#ro 说明这个路径只能通过宿主机来操作，容器内部无法操作！docker run -d -P --name nginx02 -v juming-nginx:/etc/nginx:ro nginxdocker run -d -P --name nginx02 -v juming-nginx:/etc/nginx:rw nginx 初始Dockerfile 方式二：Dockerfile就是用来构建docker镜像的构建文件！通过这个脚本，可以生成镜像，脚本为一个个命令，每个命令都是一层。 #构建一个rxk/centos[root@VM-0-15-centos home]# cd docker-test-volume/[root@VM-0-15-centos docker-test-volume]# ls[root@VM-0-15-centos docker-test-volume]# pwd/home/docker-test-volume[root@VM-0-15-centos docker-test-volume]# [root@VM-0-15-centos docker-test-volume]# nano dockerfile1#文件中的内容[root@VM-0-15-centos docker-test-volume]# cat dockerfile1 FROM centosVOLUME [\"volume01\",\"volume02\"]CMD echo \"-----end-------\"CMD /bin/bash[root@VM-0-15-centos docker-test-volume]# docker build -f dockerfile1 -t /rxk/centos .invalid argument \"/rxk/centos\" for \"-t, --tag\" flag: invalid reference formatSee 'docker build --help'.[root@VM-0-15-centos docker-test-volume]# docker build -f dockerfile1 -t rxk/centos .Sending build context to Docker daemon 2.048kBStep 1/4 : FROM centos ---&gt; 0d120b6ccaa8Step 2/4 : VOLUME [\"volume01\",\"volume02\"] ---&gt; Running in 634e59a80996Removing intermediate container 634e59a80996 ---&gt; 02a6fde9ba35Step 3/4 : CMD echo \"-----end-------\" ---&gt; Running in 3fe4feb278dfRemoving intermediate container 3fe4feb278df ---&gt; d64085501c38Step 4/4 : CMD /bin/bash ---&gt; Running in 09d704b706feRemoving intermediate container 09d704b706fe ---&gt; 7735159af50cSuccessfully built 7735159af50cSuccessfully tagged rxk/centos:latest#查看当前镜像[root@VM-0-15-centos docker-test-volume]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZErxk/centos latest 7735159af50c 5 minutes ago 215MBtomcat02 1.0 80b6126c75b9 7 hours ago 654MBnginx latest bc9a0695f571 8 days ago 133MBmysql 5.7 ae0658fdbad5 12 days ago 449MBtomcat 9.0 e0bd8b34b4ea 2 weeks ago 649MBtomcat latest e0bd8b34b4ea 2 weeks ago 649MBcentos latest 0d120b6ccaa8 3 months ago 215MBportainer/portainer latest 62771b0b9b09 4 months ago 79.1MBelasticsearch 7.6.2 f29a1ee41030 8 months ago 791MB#进入自己[root@VM-0-15-centos docker-test-volume]# docker run -it 7735159af50c /bin/bash#可以看到挂载的两个volume卷[root@d6463f570ce4 /]# ls -ltotal 56lrwxrwxrwx 1 root root 7 May 11 2019 bin -&gt; usr/bindrwxr-xr-x 5 root root 360 Dec 3 15:21 devdrwxr-xr-x 1 root root 4096 Dec 3 15:21 etcdrwxr-xr-x 2 root root 4096 May 11 2019 homelrwxrwxrwx 1 root root 7 May 11 2019 lib -&gt; usr/liblrwxrwxrwx 1 root root 9 May 11 2019 lib64 -&gt; usr/lib64drwx------ 2 root root 4096 Aug 9 21:40 lost+founddrwxr-xr-x 2 root root 4096 May 11 2019 mediadrwxr-xr-x 2 root root 4096 May 11 2019 mntdrwxr-xr-x 2 root root 4096 May 11 2019 optdr-xr-xr-x 91 root root 0 Dec 3 15:21 procdr-xr-x--- 2 root root 4096 Aug 9 21:40 rootdrwxr-xr-x 11 root root 4096 Aug 9 21:40 runlrwxrwxrwx 1 root root 8 May 11 2019 sbin -&gt; usr/sbindrwxr-xr-x 2 root root 4096 May 11 2019 srvdr-xr-xr-x 13 root root 0 Dec 3 15:21 sysdrwxrwxrwt 7 root root 4096 Aug 9 21:40 tmpdrwxr-xr-x 12 root root 4096 Aug 9 21:40 usrdrwxr-xr-x 20 root root 4096 Aug 9 21:40 vardrwxr-xr-x 2 root root 4096 Dec 3 15:21 volume01drwxr-xr-x 2 root root 4096 Dec 3 15:21 volume02#这个卷Volume一定和外部有一个同步的目录[root@d6463f570ce4 volume01]# touch container.txt[root@d6463f570ce4 volume01]# lscontainer.txt[root@d6463f570ce4 volume01]# [root@VM-0-15-centos docker-test-volume]# [root@VM-0-15-centos docker-test-volume]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESd6463f570ce4 7735159af50c \"/bin/bash\" 7 minutes ago Up 7 minutes zen_faraday[root@VM-0-15-centos docker-test-volume]# docker inspect d6463f570ce4[ &#123; \"Id\": \"d6463f570ce423c10082cce5414bf5ba70b8cb9f0ab47df998fde77b8e965881\", \"Created\": \"2020-12-03T15:21:34.913373526Z\", \"Path\": \"/bin/bash\", \"Args\": [], \"State\": &#123; \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 15638, \"ExitCode\": 0, \"Error\": \"\", \"StartedAt\": \"2020-12-03T15:21:35.202804066Z\", \"FinishedAt\": \"0001-01-01T00:00:00Z\" &#125;, \"Image\": \"sha256:7735159af50c4ff09863f0a97c37447043ac0eca6657913a4e33c65b0be4a24d\", \"ResolvConfPath\": \"/var/lib/docker/containers/d6463f570ce423c10082cce5414bf5ba70b8cb9f0ab47df998fde77b8e965881/resolv.conf\", \"HostnamePath\": \"/var/lib/docker/containers/d6463f570ce423c10082cce5414bf5ba70b8cb9f0ab47df998fde77b8e965881/hostname\", \"HostsPath\": \"/var/lib/docker/containers/d6463f570ce423c10082cce5414bf5ba70b8cb9f0ab47df998fde77b8e965881/hosts\", \"LogPath\": \"/var/lib/docker/containers/d6463f570ce423c10082cce5414bf5ba70b8cb9f0ab47df998fde77b8e965881/d6463f570ce423c10082cce5414bf5ba70b8cb9f0ab47df998fde77b8e965881-json.log\", \"Name\": \"/zen_faraday\", \"RestartCount\": 0, \"Driver\": \"overlay2\", \"Platform\": \"linux\", \"MountLabel\": \"\", \"ProcessLabel\": \"\", \"AppArmorProfile\": \"\", \"ExecIDs\": null, \"HostConfig\": &#123; \"Binds\": null, \"ContainerIDFile\": \"\", \"LogConfig\": &#123; \"Type\": \"json-file\", \"Config\": &#123;&#125; &#125;, \"NetworkMode\": \"default\", \"PortBindings\": &#123;&#125;, \"RestartPolicy\": &#123; \"Name\": \"no\", \"MaximumRetryCount\": 0 &#125;, \"AutoRemove\": false, \"VolumeDriver\": \"\", \"VolumesFrom\": null, \"CapAdd\": null, \"CapDrop\": null, \"Capabilities\": null, \"Dns\": [], \"DnsOptions\": [], \"DnsSearch\": [], \"ExtraHosts\": null, \"GroupAdd\": null, \"IpcMode\": \"private\", \"Cgroup\": \"\", \"Links\": null, \"OomScoreAdj\": 0, \"PidMode\": \"\", \"Privileged\": false, \"PublishAllPorts\": false, \"ReadonlyRootfs\": false, \"SecurityOpt\": null, \"UTSMode\": \"\", \"UsernsMode\": \"\", \"ShmSize\": 67108864, \"Runtime\": \"runc\", \"ConsoleSize\": [ 0, 0 ], \"Isolation\": \"\", \"CpuShares\": 0, \"Memory\": 0, \"NanoCpus\": 0, \"CgroupParent\": \"\", \"BlkioWeight\": 0, \"BlkioWeightDevice\": [], \"BlkioDeviceReadBps\": null, \"BlkioDeviceWriteBps\": null, \"BlkioDeviceReadIOps\": null, \"BlkioDeviceWriteIOps\": null, \"CpuPeriod\": 0, \"CpuQuota\": 0, \"CpuRealtimePeriod\": 0, \"CpuRealtimeRuntime\": 0, \"CpusetCpus\": \"\", \"CpusetMems\": \"\", \"Devices\": [], \"DeviceCgroupRules\": null, \"DeviceRequests\": null, \"KernelMemory\": 0, \"KernelMemoryTCP\": 0, \"MemoryReservation\": 0, \"MemorySwap\": 0, \"MemorySwappiness\": null, \"OomKillDisable\": false, \"PidsLimit\": null, \"Ulimits\": null, \"CpuCount\": 0, \"CpuPercent\": 0, \"IOMaximumIOps\": 0, \"IOMaximumBandwidth\": 0, \"MaskedPaths\": [ \"/proc/asound\", \"/proc/acpi\", \"/proc/kcore\", \"/proc/keys\", \"/proc/latency_stats\", \"/proc/timer_list\", \"/proc/timer_stats\", \"/proc/sched_debug\", \"/proc/scsi\", \"/sys/firmware\" ], \"ReadonlyPaths\": [ \"/proc/bus\", \"/proc/fs\", \"/proc/irq\", \"/proc/sys\", \"/proc/sysrq-trigger\" ] &#125;, \"GraphDriver\": &#123; \"Data\": &#123; \"LowerDir\": \"/var/lib/docker/overlay2/16864c6fe5942b10aff31e1156dbb08a055e31975f8725d25a205bd9f76b6f23-init/diff:/var/lib/docker/overlay2/9977c7654d8482400f51116d18f3c0562764ac2c1082c8de455e689c10593b30/diff\", \"MergedDir\": \"/var/lib/docker/overlay2/16864c6fe5942b10aff31e1156dbb08a055e31975f8725d25a205bd9f76b6f23/merged\", \"UpperDir\": \"/var/lib/docker/overlay2/16864c6fe5942b10aff31e1156dbb08a055e31975f8725d25a205bd9f76b6f23/diff\", \"WorkDir\": \"/var/lib/docker/overlay2/16864c6fe5942b10aff31e1156dbb08a055e31975f8725d25a205bd9f76b6f23/work\" &#125;, \"Name\": \"overlay2\" &#125;, \"Mounts\": [ &#123; \"Type\": \"volume\", \"Name\": \"4bb7c13d437e16eb97bfcc71aa2faac18594ded837fcefc6d266e8ad36845c6a\", \"Source\": \"/var/lib/docker/volumes/4bb7c13d437e16eb97bfcc71aa2faac18594ded837fcefc6d266e8ad36845c6a/_data\", \"Destination\": \"volume01\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" &#125;, &#123; \"Type\": \"volume\", \"Name\": \"0eca9d146b2fd63f434657516305c20241da68936914059527e728865109e3f1\", \"Source\": \"/var/lib/docker/volumes/0eca9d146b2fd63f434657516305c20241da68936914059527e728865109e3f1/_data\", \"Destination\": \"volume02\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" &#125; ], \"Config\": &#123; \"Hostname\": \"d6463f570ce4\", \"Domainname\": \"\", \"User\": \"\", \"AttachStdin\": true, \"AttachStdout\": true, \"AttachStderr\": true, \"Tty\": true, \"OpenStdin\": true, \"StdinOnce\": true, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], \"Cmd\": [ \"/bin/bash\" ], \"Image\": \"7735159af50c\", \"Volumes\": &#123; \"volume01\": &#123;&#125;, \"volume02\": &#123;&#125; &#125;, \"WorkingDir\": \"\", \"Entrypoint\": null, \"OnBuild\": null, \"Labels\": &#123; \"org.label-schema.build-date\": \"20200809\", \"org.label-schema.license\": \"GPLv2\", \"org.label-schema.name\": \"CentOS Base Image\", \"org.label-schema.schema-version\": \"1.0\", \"org.label-schema.vendor\": \"CentOS\" &#125; &#125;, \"NetworkSettings\": &#123; \"Bridge\": \"\", \"SandboxID\": \"cf151107d8fa56fd220695eed265b431618350222be583761b13354867477e39\", \"HairpinMode\": false, \"LinkLocalIPv6Address\": \"\", \"LinkLocalIPv6PrefixLen\": 0, \"Ports\": &#123;&#125;, \"SandboxKey\": \"/var/run/docker/netns/cf151107d8fa\", \"SecondaryIPAddresses\": null, \"SecondaryIPv6Addresses\": null, \"EndpointID\": \"1c1bab6bce8a119f4731a2555a39256791d792539b260d2fd230aea73940d75b\", \"Gateway\": \"172.18.0.1\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"IPAddress\": \"172.18.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"MacAddress\": \"02:42:ac:12:00:02\", \"Networks\": &#123; \"bridge\": &#123; \"IPAMConfig\": null, \"Links\": null, \"Aliases\": null, \"NetworkID\": \"df77c62f666112b12946dbbbc16f1dcdbbcc8548e3fa56519fd062e5331a0c08\", \"EndpointID\": \"1c1bab6bce8a119f4731a2555a39256791d792539b260d2fd230aea73940d75b\", \"Gateway\": \"172.18.0.1\", \"IPAddress\": \"172.18.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"MacAddress\": \"02:42:ac:12:00:02\", \"DriverOpts\": null &#125; &#125; &#125; &#125;] 数据卷容器不同容器之间同步数据 #docker01已经启动了[root@VM-0-15-centos _data]# docker run -it --name docker02 --volumes-from docker01 rxk/centos#--volumes-from类似于 docker02继承docker01#即使docker01被删了，docker02中卷的文件也是存在的，类似硬链接 DockerFiledockerfile是用来构建docker镜像的文件！命令参数脚本！ 构建步骤： 1.编写一个dockerfile文件 2.docker build构建成为一个镜像 3.docker run运行镜像 4.docker push发布镜像(DockerHub、阿里云镜像仓库！) 很多官方镜像都是基础包，很多功能没有，通常会自己搭建自己的镜像。 DockerFile的构建过程基础知识 1.每个保留关键字(指令)都必须是大写字母 2.执行从上到下顺序执行 3.# 表示注释 4.每一个指令都会创建提交一个新的镜像层 并提交 DockerFile:构建文件，定义了一切的步骤，源代码。 DockerImages:通过DockerFile构建生成的镜像，最终发布和运行的产品 Docker容器：容器就是镜像运行起来提供服务的 DockerFile的指令FROM # 基础镜像，一切从这里开始构建MAINTAINER #镜像是谁写的，姓名+邮箱RUN #镜像构建的时候需要运行的命令ADD #步骤：构建基于tomcat的镜像，进行添加！ADD为添加内容WORKDIR #镜像的工作目录VOLUME #挂载的容器卷EXPOSE #指定暴露端口CMD #指定这个容器启动时候要运行的命令，只有最后一个会生效，可被替代ENTRYPOINT #指定这个容器启动的时候要运行的命令，可以追加命令ONBUILD #当构建一个被继承DockerFile， 这个时候就会运行ONBUILD命令。触发指令COPY #类似ADD命令 ，将文件拷贝到镜像中ENV #构建时候设置环境变量 构建自己的centos 构建自己的centos #1.编写dockerfile的文件FROM centosMAINTAINER kuangshen&lt;179049243@qq.com&gt;ENV MYPATH /usr/localWORKDIR $MYPATHRUN yum -y install vimRUN yum -y install net-toolsEXPOSE 80CMD echo $MYPATHCMD echo \"----end----\"CMD /bin/bash#2.通过文件构建镜像# docker build -f dockerfile文件路径 -t 镜像名:[tag].#docker build -f my-dockerfile -t mycentos:0.1 .[root@VM-0-15-centos dockerfile]# docker build -f my-dockerfile -t mycentos:0.1 .Sending build context to Docker daemon 2.048kBStep 1/10 : FROM centos ---&gt; 0d120b6ccaa8Step 2/10 : MAINTAINER kuangshen&lt;179049243@qq.com&gt; ---&gt; Running in 3dd9d64bfbffRemoving intermediate container 3dd9d64bfbff ---&gt; e56a7b5f722bStep 3/10 : ENV MYPATH /usr/local ---&gt; Running in 7707c6ee0441Removing intermediate container 7707c6ee0441 ---&gt; b67dab755631Step 4/10 : WORKDIR $MYPATH ---&gt; Running in 1797c746ab6bRemoving intermediate container 1797c746ab6b ---&gt; 3ed5689097b4Step 5/10 : RUN yum -y install vim ---&gt; Running in 5791c022d132CentOS-8 - AppStream 969 kB/s | 6.2 MB 00:06 CentOS-8 - Base 950 kB/s | 2.3 MB 00:02 CentOS-8 - Extras 11 kB/s | 8.1 kB 00:00 Dependencies resolved.================================================================================ Package Arch Version Repository Size================================================================================Installing: vim-enhanced x86_64 2:8.0.1763-15.el8 AppStream 1.4 MInstalling dependencies: gpm-libs x86_64 1.20.7-15.el8 AppStream 39 k vim-common x86_64 2:8.0.1763-15.el8 AppStream 6.3 M vim-filesystem noarch 2:8.0.1763-15.el8 AppStream 48 k which x86_64 2.21-12.el8 BaseOS 49 kTransaction Summary================================================================================Install 5 PackagesTotal download size: 7.8 MInstalled size: 30 MDownloading Packages:(1/5): gpm-libs-1.20.7-15.el8.x86_64.rpm 327 kB/s | 39 kB 00:00 (2/5): vim-filesystem-8.0.1763-15.el8.noarch.rp 827 kB/s | 48 kB 00:00 (3/5): which-2.21-12.el8.x86_64.rpm 316 kB/s | 49 kB 00:00 (4/5): vim-enhanced-8.0.1763-15.el8.x86_64.rpm 1.2 MB/s | 1.4 MB 00:01 (5/5): vim-common-8.0.1763-15.el8.x86_64.rpm 983 kB/s | 6.3 MB 00:06 --------------------------------------------------------------------------------Total 974 kB/s | 7.8 MB 00:08 warning: /var/cache/dnf/AppStream-02e86d1c976ab532/packages/gpm-libs-1.20.7-15.el8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID 8483c65d: NOKEYCentOS-8 - AppStream 1.6 MB/s | 1.6 kB 00:00 Importing GPG key 0x8483C65D: Userid : \"CentOS (CentOS Official Signing Key) &lt;security@centos.org&gt;\" Fingerprint: 99DB 70FA E1D7 CE22 7FB6 4882 05B5 55B3 8483 C65D From : /etc/pki/rpm-gpg/RPM-GPG-KEY-centosofficialKey imported successfullyRunning transaction checkTransaction check succeeded.Running transaction testTransaction test succeeded.Running transaction Preparing : 1/1 Installing : which-2.21-12.el8.x86_64 1/5 Installing : vim-filesystem-2:8.0.1763-15.el8.noarch 2/5 Installing : vim-common-2:8.0.1763-15.el8.x86_64 3/5 Installing : gpm-libs-1.20.7-15.el8.x86_64 4/5 Running scriptlet: gpm-libs-1.20.7-15.el8.x86_64 4/5 Installing : vim-enhanced-2:8.0.1763-15.el8.x86_64 5/5 Running scriptlet: vim-enhanced-2:8.0.1763-15.el8.x86_64 5/5 Running scriptlet: vim-common-2:8.0.1763-15.el8.x86_64 5/5 Verifying : gpm-libs-1.20.7-15.el8.x86_64 1/5 Verifying : vim-common-2:8.0.1763-15.el8.x86_64 2/5 Verifying : vim-enhanced-2:8.0.1763-15.el8.x86_64 3/5 Verifying : vim-filesystem-2:8.0.1763-15.el8.noarch 4/5 Verifying : which-2.21-12.el8.x86_64 5/5 Installed: gpm-libs-1.20.7-15.el8.x86_64 vim-common-2:8.0.1763-15.el8.x86_64 vim-enhanced-2:8.0.1763-15.el8.x86_64 vim-filesystem-2:8.0.1763-15.el8.noarch which-2.21-12.el8.x86_64 Complete!Removing intermediate container 5791c022d132 ---&gt; 05dc9bde286eStep 6/10 : RUN yum -y install net-tools ---&gt; Running in 900fc6c0b10eLast metadata expiration check: 0:00:15 ago on Thu Dec 10 15:07:33 2020.Dependencies resolved.================================================================================ Package Architecture Version Repository Size================================================================================Installing: net-tools x86_64 2.0-0.52.20160912git.el8 BaseOS 322 kTransaction Summary================================================================================Install 1 PackageTotal download size: 322 kInstalled size: 942 kDownloading Packages:net-tools-2.0-0.52.20160912git.el8.x86_64.rpm 969 kB/s | 322 kB 00:00 --------------------------------------------------------------------------------Total 181 kB/s | 322 kB 00:01 Running transaction checkTransaction check succeeded.Running transaction testTransaction test succeeded.Running transaction Preparing : 1/1 Installing : net-tools-2.0-0.52.20160912git.el8.x86_64 1/1 Running scriptlet: net-tools-2.0-0.52.20160912git.el8.x86_64 1/1 Verifying : net-tools-2.0-0.52.20160912git.el8.x86_64 1/1 Installed: net-tools-2.0-0.52.20160912git.el8.x86_64 Complete!Removing intermediate container 900fc6c0b10e ---&gt; 3f8796ecbdeaStep 7/10 : EXPOSE 80 ---&gt; Running in 37e6f3792548Removing intermediate container 37e6f3792548 ---&gt; 2d37cf00b36bStep 8/10 : CMD echo $MYPATH ---&gt; Running in 7e4ff55e4b24Removing intermediate container 7e4ff55e4b24 ---&gt; d74bce644e74Step 9/10 : CMD echo \"----end----\" ---&gt; Running in fc3301142b73Removing intermediate container fc3301142b73 ---&gt; 2c7e307a481cStep 10/10 : CMD /bin/bash ---&gt; Running in 4e5c6edd147cRemoving intermediate container 4e5c6edd147c ---&gt; 071c7fb7d0f9Successfully built 071c7fb7d0f9Successfully tagged mycentos:0.1#3.测试镜像[root@VM-0-15-centos dockerfile]# docker run -it mycentos:0.1[root@5b68abc14a5a local]# pwd/usr/local[root@5b68abc14a5a local]# vimsh: wq: command not foundshell returned 127Press ENTER or type command to continuesh: wq: command not foundshell returned 127Press ENTER or type command to continue 镜像构建历史查看 [root@VM-0-15-centos dockerfile]# docker history 071c7fb7d0f9IMAGE CREATED CREATED BY SIZE COMMENT071c7fb7d0f9 6 minutes ago /bin/sh -c #(nop) CMD [\"/bin/sh\" \"-c\" \"/bin… 0B 2c7e307a481c 6 minutes ago /bin/sh -c #(nop) CMD [\"/bin/sh\" \"-c\" \"echo… 0B d74bce644e74 6 minutes ago /bin/sh -c #(nop) CMD [\"/bin/sh\" \"-c\" \"echo… 0B 2d37cf00b36b 6 minutes ago /bin/sh -c #(nop) EXPOSE 80 0B 3f8796ecbdea 6 minutes ago /bin/sh -c yum -y install net-tools 23.1MB 05dc9bde286e 6 minutes ago /bin/sh -c yum -y install vim 57.7MB 3ed5689097b4 7 minutes ago /bin/sh -c #(nop) WORKDIR /usr/local 0B b67dab755631 7 minutes ago /bin/sh -c #(nop) ENV MYPATH=/usr/local 0B e56a7b5f722b 7 minutes ago /bin/sh -c #(nop) MAINTAINER kuangshen&lt;1790… 0B 0d120b6ccaa8 4 months ago /bin/sh -c #(nop) CMD [\"/bin/bash\"] 0B &lt;missing&gt; 4 months ago /bin/sh -c #(nop) LABEL org.label-schema.sc… 0B &lt;missing&gt; 4 months ago /bin/sh -c #(nop) ADD file:538afc0c5c964ce0d… 215MB 实战：Dockerfile制作tomcat镜像1、准备镜像文件tomcat压缩包，jdk的压缩包！2、编写dockerfile文件。 Docker0网络详解#删除所有镜像[root@VM-0-15-centos ~]# docker rmi -f $(docker images -aq)Untagged: rxk/centos:latestDeleted: sha256:7735159af50c4ff09863f0a97c37447043ac0eca6657913a4e33c65b0be4a24dDeleted: sha256:d64085501c38257f9828c1dc9717f26e20df9bcb8d1b958b7f49ea5bb0425ae0Deleted: sha256:02a6fde9ba357c11d75c6d828c8fee80e1e5e77c4aedb3a83c7e302fc1745d40Untagged: tomcat02:1.0Deleted: sha256:80b6126c75b93053ee013c1a724a10fb958ddab21e5b0f3f10a58dbb996c74ebDeleted: sha256:20e9e09986f8d937efa6f2ad283aa9a1b837cc5cc5d4d76e6105f83e38e8f3deUntagged: nginx:latestUntagged: nginx@sha256:6b1daa9462046581ac15be20277a7c75476283f969cb3a61c8725ec38d3b01c3Deleted: sha256:bc9a0695f5712dcaaa09a5adc415a3936ccba13fc2587dfd76b1b8aeea3f221cUntagged: mysql:5.7Untagged: mysql@sha256:8e2004f9fe43df06c3030090f593021a5f283d028b5ed5765cc24236c2c4d88eDeleted: sha256:ae0658fdbad5fb1c9413c998d8a573eeb5d16713463992005029c591e6400d02Untagged: tomcat:9.0Untagged: tomcat:latestUntagged: tomcat@sha256:a319b10d8729817c7ce0bcc2343a6f97711c7870395019340d96b6aafd6ccbeaDeleted: sha256:e0bd8b34b4ea904874e55eae50e8987815030d140f9773a4b61759f4f85bf38dUntagged: portainer/portainer:latestUntagged: portainer/portainer@sha256:f8c2b0a9ca640edf508a8a0830cf1963a1e0d2fd9936a64104b3f658e120b868Deleted: sha256:62771b0b9b0973a3e8e95595534a1240d8cfd968d30ec82dc0393ce0a256c5f3Untagged: elasticsearch:7.6.2Untagged: elasticsearch@sha256:1b09dbd93085a1e7bca34830e77d2981521a7210e11f11eda997add1c12711faDeleted: sha256:f29a1ee41030e3963026369105f3bee76d75fdecbeca07932ac054126be7bff9Error response from daemon: conflict: unable to delete 071c7fb7d0f9 (cannot be forced) - image is being used by running container 5b68abc14a5aError response from daemon: conflict: unable to delete 3f8796ecbdea (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete d74bce644e74 (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete 2d37cf00b36b (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete 2c7e307a481c (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete 05dc9bde286e (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete 3ed5689097b4 (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete e56a7b5f722b (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete b67dab755631 (cannot be forced) - image has dependent child imagesError: No such image: 02a6fde9ba35Error: No such image: d64085501c38Error: No such image: e0bd8b34b4eaError response from daemon: conflict: unable to delete 0d120b6ccaa8 (cannot be forced) - image has dependent child images 三个网络 #启动一个tomcat[root@VM-0-15-centos ~]# docker run -d -P --name tomcat02 tomcat942539fdda0c018ffa3750661373e624c4f2226f5d9532323d5c42c1daea98db[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES942539fdda0c tomcat \"catalina.sh run\" 3 seconds ago Up 3 seconds 0.0.0.0:32768-&gt;8080/tcp tomcat02#查看容器的内部网络地址[root@VM-0-15-centos ~]# docker exec -it tomcat02 ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever112: eth0@if113: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.18.0.2/16 brd 172.18.255.255 scope global eth0 valid_lft forever preferred_lft forever linux可以ping通docker容器内部 原理 1.我们每安装一个docker容器，docker就会给docker容器分配一个ip，我们只要安装了docker，就会有一个网卡，docker0，使用桥接模式，使用技术是veth-pair技术！ 再次测试ip addr: 2.再启动一个容器测试，又会多一对网卡，这样一对对的网卡，其实是veth-pair技术，就是一对的虚拟设备，他们都是成对出现，一段连着协议，一段彼此相连，veth-pair充当一个桥梁，链接各种虚拟网络设备 3.测试下tomcat02和tomcat03能否ping通，确实可以ping通 容器和容器之间是可以互相ping通的 [root@VM-0-15-centos ~]# docker exec -it tomcat03 ping 172.18.0.2PING 172.18.0.2 (172.18.0.2) 56(84) bytes of data.64 bytes from 172.18.0.2: icmp_seq=1 ttl=64 time=0.076 ms64 bytes from 172.18.0.2: icmp_seq=2 ttl=64 time=0.054 ms64 bytes from 172.18.0.2: icmp_seq=3 ttl=64 time=0.053 ms^C--- 172.18.0.2 ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 1001msrtt min/avg/max/mdev = 0.053/0.061/0.076/0.010 ms 结论：tomcat02和tomcat03共用一个路由器，docker0 所有容器不指定网络的情况下，都是由docker0路由的，docker会给我们容器分配一个默认的可用IP 当把docker容器删除之后，ip addr之后 容器ip就没了 [root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESbe137857cf4e tomcat \"catalina.sh run\" 29 minutes ago Up 29 minutes 0.0.0.0:32769-&gt;8080/tcp tomcat03942539fdda0c tomcat \"catalina.sh run\" 37 minutes ago Up 37 minutes 0.0.0.0:32768-&gt;8080/tcp tomcat02[root@VM-0-15-centos ~]# docker rm -f be137857cf4ebe137857cf4e[root@VM-0-15-centos ~]# docker rm -f 942539fdda0c942539fdda0c[root@VM-0-15-centos ~]# ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 52:54:00:83:af:57 brd ff:ff:ff:ff:ff:ff inet 172.17.0.15/20 brd 172.17.15.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::5054:ff:fe83:af57/64 scope link valid_lft forever preferred_lft forever3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:18:37:f9:06 brd ff:ff:ff:ff:ff:ff inet 172.18.0.1/16 brd 172.18.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:18ff:fe37:f906/64 scope link valid_lft forever preferred_lft forever Docker –link 容器互联使用服务名去ping通tomcat docker run -d -P tomcat03 --link tomcat02 tomcatdocker exec -it tomcat03 ping tomcat02#此时3能ping 通2，但是2 ping不通3 Docker自定义网络查看docker所有网络 [root@VM-0-15-centos ~]# docker network lsNETWORK ID NAME DRIVER SCOPEdf77c62f6661 bridge bridge localc916a7eaedff host host local00b73cedd526 none null local 网络模式 bridge:桥接 bridge(默认) none:不配置网络 host:和宿主机共享网络 container:容器内网络连通(使用较少) 创建自己的自定义网络 –driver bridge 默认桥接 –subnet 192.168.0.0/16 子网地址 –gateway 192.168.0.1 网关地址 [root@VM-0-15-centos ~]# docker network create --driver bridge --subnet 192.168.0.0/16 --gateway 192.168.0.1 mynet[root@VM-0-15-centos ~]# docker network lsNETWORK ID NAME DRIVER SCOPEdf77c62f6661 bridge bridge localc916a7eaedff host host localf5100a69756f mynet bridge local00b73cedd526 none null local[root@VM-0-15-centos ~]# docker network inspect mynet[ &#123; \"Name\": \"mynet\", \"Id\": \"f5100a69756fbedf28ab9ad7ecbf354417328efc06d7f0bfae40a0d4c7d33de4\", \"Created\": \"2020-12-22T11:03:05.693800981+08:00\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": &#123; \"Driver\": \"default\", \"Options\": &#123;&#125;, \"Config\": [ &#123; \"Subnet\": \"192.168.0.0/16\", \"Gateway\": \"192.168.0.1\" &#125; ] &#125;, \"Internal\": false, \"Attachable\": false, \"Ingress\": false, \"ConfigFrom\": &#123; \"Network\": \"\" &#125;, \"ConfigOnly\": false, \"Containers\": &#123;&#125;, \"Options\": &#123;&#125;, \"Labels\": &#123;&#125; &#125;]#启动两个容器[root@VM-0-15-centos ~]# docker run -d -P --name tomcat-net-01 --network mynet tomcat8fc4551bbad9d836cea86a3feb957596af6e1310781eadee1baacbb177d84a33[root@VM-0-15-centos ~]# docker run -d -P --name tomcat-net-02 --network mynet tomcat01a3d2a47af6a79689dc85e0a110d576cbdfa8bb979d25d7f493b13157b3e26a[root@VM-0-15-centos ~]# docker network inspect mynet[ &#123; \"Name\": \"mynet\", \"Id\": \"f5100a69756fbedf28ab9ad7ecbf354417328efc06d7f0bfae40a0d4c7d33de4\", \"Created\": \"2020-12-22T11:03:05.693800981+08:00\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": &#123; \"Driver\": \"default\", \"Options\": &#123;&#125;, \"Config\": [ &#123; \"Subnet\": \"192.168.0.0/16\", \"Gateway\": \"192.168.0.1\" &#125; ] &#125;, \"Internal\": false, \"Attachable\": false, \"Ingress\": false, \"ConfigFrom\": &#123; \"Network\": \"\" &#125;, \"ConfigOnly\": false, \"Containers\": &#123; \"01a3d2a47af6a79689dc85e0a110d576cbdfa8bb979d25d7f493b13157b3e26a\": &#123; \"Name\": \"tomcat-net-02\", \"EndpointID\": \"25be3c90c1e1a4b74f5fc9e10a59c50029ca7ff3bf692d7de636738edb3457b3\", \"MacAddress\": \"02:42:c0:a8:00:03\", \"IPv4Address\": \"192.168.0.3/16\", \"IPv6Address\": \"\" &#125;, \"8fc4551bbad9d836cea86a3feb957596af6e1310781eadee1baacbb177d84a33\": &#123; \"Name\": \"tomcat-net-01\", \"EndpointID\": \"a83c09df1a4595735453dafdde352123dd024707400716fbab256c7e3c02ea4a\", \"MacAddress\": \"02:42:c0:a8:00:02\", \"IPv4Address\": \"192.168.0.2/16\", \"IPv6Address\": \"\" &#125; &#125;, \"Options\": &#123;&#125;, \"Labels\": &#123;&#125; &#125;]#再次测试，不再使用--link 也是可以连接通的，不管使用ip还是服务名[root@VM-0-15-centos ~]# docker exec -it tomcat-net-01 ping 192.168.0.3PING 192.168.0.3 (192.168.0.3) 56(84) bytes of data.64 bytes from 192.168.0.3: icmp_seq=1 ttl=64 time=0.085 ms64 bytes from 192.168.0.3: icmp_seq=2 ttl=64 time=0.055 ms64 bytes from 192.168.0.3: icmp_seq=3 ttl=64 time=0.053 ms^C--- 192.168.0.3 ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 1000msrtt min/avg/max/mdev = 0.053/0.064/0.085/0.016 ms[root@VM-0-15-centos ~]# docker exec -it tomcat-net-01 ping tomcat-net-02PING tomcat-net-02 (192.168.0.3) 56(84) bytes of data.64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=1 ttl=64 time=0.050 ms64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=2 ttl=64 time=0.052 ms64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=3 ttl=64 time=0.053 ms^C--- tomcat-net-02 ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 2msrtt min/avg/max/mdev = 0.050/0.051/0.053/0.008 ms 我们自定义的网络docker 都已经帮我们维护好了对应的关系，推荐这样使用网络！ 好处：不同的集群使用不同的网络，保证集群是安全和健康 网络连通把一个容器连接到网络上 #将tomcat01连接到mynet网络下，一个容器两个ip docker network connect mynet tomcat01 Redis集群实战通过脚本创建六个redis配置 for port in $(seq 1 6); \\do \\mkdir -p /mydata/redis/node-$&#123;port&#125;/conftouch /mydata/redis/node-$&#123;port&#125;/conf/redis.confcat &lt;&lt; EOF &gt;/mydata/redis/node-$&#123;port&#125;/conf/redis.confport 6379bind 0.0.0.0cluster-enabled yescluster-config-file nodes.confcluster-node-timeout 5000cluster-announce-ip 172.38.0.1$&#123;port&#125;cluster-announce-port 6379cluster-announce-bus-port 16379daemonize noappendonly yesEOFdone 启动redis docker run -p 6371:6379 -p 16371:16379 --name redis-1 -v /mydata/redis/node-1/data:/data -v /mydata/redis/node-1/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.11 redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.confdocker run -p 6372:6379 -p 16372:16379 --name redis-2 -v /mydata/redis/node-2/data:/data -v /mydata/redis/node-2/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.12 redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.confdocker run -p 6373:6379 -p 16373:16379 --name redis-3 -v /mydata/redis/node-3/data:/data -v /mydata/redis/node-3/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.13 redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.confdocker run -p 6374:6379 -p 16374:16379 --name redis-4 -v /mydata/redis/node-4/data:/data -v /mydata/redis/node-4/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.14 redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.confdocker run -p 6375:6379 -p 16375:16379 --name redis-5 -v /mydata/redis/node-5/data:/data -v /mydata/redis/node-5/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.15 redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.confdocker run -p 6376:6379 -p 16376:16379 --name redis-6 -v /mydata/redis/node-6/data:/data -v /mydata/redis/node-6/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.16 redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.conf#正常启动之后[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe1535aac14ce redis:5.0.9-alpine3.11 \"docker-entrypoint.s…\" 22 seconds ago Up 22 seconds 0.0.0.0:6376-&gt;6379/tcp, 0.0.0.0:16376-&gt;16379/tcp redis-6fcf18ac7033d redis:5.0.9-alpine3.11 \"docker-entrypoint.s…\" 31 seconds ago Up 30 seconds 0.0.0.0:6375-&gt;6379/tcp, 0.0.0.0:16375-&gt;16379/tcp redis-5b17f66d41588 redis:5.0.9-alpine3.11 \"docker-entrypoint.s…\" 37 seconds ago Up 37 seconds 0.0.0.0:6374-&gt;6379/tcp, 0.0.0.0:16374-&gt;16379/tcp redis-4afb2f4e6484e redis:5.0.9-alpine3.11 \"docker-entrypoint.s…\" 44 seconds ago Up 44 seconds 0.0.0.0:6373-&gt;6379/tcp, 0.0.0.0:16373-&gt;16379/tcp redis-3ef59db5b0cc0 redis:5.0.9-alpine3.11 \"docker-entrypoint.s…\" 54 seconds ago Up 53 seconds 0.0.0.0:6372-&gt;6379/tcp, 0.0.0.0:16372-&gt;16379/tcp redis-20eca360704d7 redis:5.0.9-alpine3.11 \"docker-entrypoint.s…\" About a minute ago Up About a minute 0.0.0.0:6371-&gt;6379/tcp, 0.0.0.0:16371-&gt;16379/tcp redis-1#进入redis-1[root@VM-0-15-centos ~]# docker exec -it redis-1 /bin/sh/data # #创建redis集群[root@VM-0-15-centos ~]# docker exec -it redis-1 /bin/sh/data # redis-cli --cluster create 172.38.0.11:6379 172.38.0.12:6379 172.38.0.13:6379 172.38.0.14:6379 172.38.0.15:6379 172.38.0.16:6379 --cluster-replicas 1&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Master[0] -&gt; Slots 0 - 5460Master[1] -&gt; Slots 5461 - 10922Master[2] -&gt; Slots 10923 - 16383Adding replica 172.38.0.15:6379 to 172.38.0.11:6379Adding replica 172.38.0.16:6379 to 172.38.0.12:6379Adding replica 172.38.0.14:6379 to 172.38.0.13:6379M: f70e64b741bad779e2b545b1429b47b2732a9a13 172.38.0.11:6379 slots:[0-5460] (5461 slots) masterM: de19aadcb9d25a77ca0e5a23f1c45f31a6ece84d 172.38.0.12:6379 slots:[5461-10922] (5462 slots) masterM: 43b45750adfaea90314c724dd1b800621f6b40bf 172.38.0.13:6379 slots:[10923-16383] (5461 slots) masterS: 13b65b47fcac7126bf6ca15a322037571c90464f 172.38.0.14:6379 replicates 43b45750adfaea90314c724dd1b800621f6b40bfS: 03e58c163b80baf32990d6faf3a104d97e14f1e4 172.38.0.15:6379 replicates f70e64b741bad779e2b545b1429b47b2732a9a13S: b80d23b1516da408f9f9bad22007ce4389e529ca 172.38.0.16:6379 replicates de19aadcb9d25a77ca0e5a23f1c45f31a6ece84dCan I set the above configuration? (type 'yes' to accept): yes&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join...&gt;&gt;&gt; Performing Cluster Check (using node 172.38.0.11:6379)M: f70e64b741bad779e2b545b1429b47b2732a9a13 172.38.0.11:6379 slots:[0-5460] (5461 slots) master 1 additional replica(s)M: de19aadcb9d25a77ca0e5a23f1c45f31a6ece84d 172.38.0.12:6379 slots:[5461-10922] (5462 slots) master 1 additional replica(s)S: 13b65b47fcac7126bf6ca15a322037571c90464f 172.38.0.14:6379 slots: (0 slots) slave replicates 43b45750adfaea90314c724dd1b800621f6b40bfS: 03e58c163b80baf32990d6faf3a104d97e14f1e4 172.38.0.15:6379 slots: (0 slots) slave replicates f70e64b741bad779e2b545b1429b47b2732a9a13M: 43b45750adfaea90314c724dd1b800621f6b40bf 172.38.0.13:6379 slots:[10923-16383] (5461 slots) master 1 additional replica(s)S: b80d23b1516da408f9f9bad22007ce4389e529ca 172.38.0.16:6379 slots: (0 slots) slave replicates de19aadcb9d25a77ca0e5a23f1c45f31a6ece84d[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered./data # redis-cli -c127.0.0.1:6379&gt; cluster infocluster_state:okcluster_slots_assigned:16384cluster_slots_ok:16384cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:6cluster_size:3cluster_current_epoch:6cluster_my_epoch:1cluster_stats_messages_ping_sent:149cluster_stats_messages_pong_sent:140cluster_stats_messages_sent:289cluster_stats_messages_ping_received:135cluster_stats_messages_pong_received:149cluster_stats_messages_meet_received:5cluster_stats_messages_received:289127.0.0.1:6379&gt; cluster nodesde19aadcb9d25a77ca0e5a23f1c45f31a6ece84d 172.38.0.12:6379@16379 master - 0 1608624786213 2 conn61-1092213b65b47fcac7126bf6ca15a322037571c90464f 172.38.0.14:6379@16379 slave 43b45750adfaea90314c724ddf6b40bf 0 1608624785000 4 connected03e58c163b80baf32990d6faf3a104d97e14f1e4 172.38.0.15:6379@16379 slave f70e64b741bad779e2b545b1432a9a13 0 1608624785000 5 connected43b45750adfaea90314c724dd1b800621f6b40bf 172.38.0.13:6379@16379 master - 0 1608624785210 3 conn923-16383b80d23b1516da408f9f9bad22007ce4389e529ca 172.38.0.16:6379@16379 slave de19aadcb9d25a77ca0e5a23f6ece84d 0 1608624785712 6 connectedf70e64b741bad779e2b545b1429b47b2732a9a13 172.38.0.11:6379@16379 myself,master - 0 1608624785000cted 0-5460127.0.0.1:6379&gt; set a b-&gt; Redirected to slot [15495] located at 172.38.0.13:6379OK172.38.0.13:6379&gt; get a^C#即使把redis-3 stop了，仍然可以通过其他slaver找到key a所对应的值b，主从配置成功/data # redis-cli -c127.0.0.1:6379&gt; get a-&gt; Redirected to slot [15495] located at 172.38.0.14:6379\"b\"172.38.0.14:6379&gt; 搭建redis集群完成","categories":[{"name":"后台学习","slug":"后台学习","permalink":"http://renxingkai.github.io/categories/后台学习/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://renxingkai.github.io/tags/docker/"}],"author":"CinKate"},{"title":"《A Comparative Study of Word Embeddings for Reading Comprehension》论文阅读","slug":"WordEmbeddingsforReadingComprehension","date":"2019-05-11T19:43:34.000Z","updated":"2020-05-17T16:12:00.854Z","comments":true,"path":"2019/05/12/WordEmbeddingsforReadingComprehension/","link":"","permalink":"http://renxingkai.github.io/2019/05/12/WordEmbeddingsforReadingComprehension/","excerpt":"","text":"众所周知，预训练好的词向量有不同的维度，比如预训练好的GloVe词向量有从50-300维等的词向量表示，但这些不同维度的表示有什么区别，以及在什么时候该用什么维度的词向量（虽然各论文中大家大多用了300维的词向量），这些问题我也确实不太清除。这篇论文解答了我的这些困惑，写的还是很精彩的。 论文原链接 Let’s have a look: 这篇论文主要解决了两个问题点： - 在阅读理解任务中应该使用什么样的预训练词向量 - 测试阶段对于OOV词应怎样处理 所用的数据集和模型数据集： Who-Did-What(WDW) 完型填空类的数据集，从新闻故事中构建 Children’s Book Test(CBT) 从children’s book构建，此论文中仅用了CBT-NE，即数据集中答案为命名实体的数据 模型：Stanford AR、GA Reader 词向量对比： GloVe (50-300) word2vec (300) 实验和结果 词向量的对比 第一个结果：使用在合适的语料库上训练的词向量可以比随机初始化提高3-6％。然而，用于预训练的语料库和方法是重要的选择：例如，在CBT上训练的word2vec词向量比随机词向量执行效果更差。 另请注意，在每种情况下，GloVe词向量都优于在同一语料库中训练的word2vec嵌入。 但是由于词向量对训练参数很敏感，并不能说GloVe一定比word2vec好，但确实从各个论文中也可以看出，一般会优先选择GloVe。 第二个结果：对比GloVe不同维度对实验结果的影响(50-300)：随着词向量维度的增加，实验结果性能是下降的。但是即使使用了300维的GloVe词向量，实验结果仍然比word2vec词向量效果好。 第三个结果：在使用原始语料进行训练时，要先将停用词去掉，与停用词的共现提供关于特定单词的语义的很少有意义的信息，因此具有高百分比的语料库可能不会产生高质量的向量。训练词向量时，超参数调节很重要。 处理OOV词 第一个结果：一般对于OOV词的处理都是赋予一个固定大小不变的词向量（UNK）。这种方法忽略了这样一个事实，即分配为UNK的许多单词可能已经训练过VG中可用的词向量。实验中在测试时，任何新token将被分配其GloVe向量（如果存在）或UNK的向量。 第二个结果，不是为所有OOV词分配一个共同的UNK向量，而是为它们分配未经训练但唯一的随机向量可能更好。此方法在训练时访问测试集词表是没必要的。 总结作者已经证明，用于初始化单词向量的预训练词向量的选择对于阅读理解的神经网络模型的性能具有显著影响。在测试时处理OOV词的方法也是如此。 根据作者的实验，我们建议使用现成的GloVe词向量，并在测试时将预先训练的GloVe向量（如果可用）或随机但唯一的向量分配给OOV词。","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"词向量相关","slug":"词向量相关","permalink":"http://renxingkai.github.io/tags/词向量相关/"}],"author":"CinKate"},{"title":"FastQA学习","slug":"fastqa","date":"2019-04-21T10:52:36.000Z","updated":"2020-05-17T16:11:59.241Z","comments":true,"path":"2019/04/21/fastqa/","link":"","permalink":"http://renxingkai.github.io/2019/04/21/fastqa/","excerpt":"","text":"现在大多数的阅读理解系统都是 top-down 的形式构建的，也就是说一开始就提出了一个很复杂的结构（一般经典的就是 emedding-, encoding-, interaction-, answer-layer ），然后通过 ablation study，不断的减少一些模块配置来验证想法，大多数的创新点都在 interaction 层，比如BIDAF、R-Net等等，大量的工作都在问题和文章的交互query-aware表示上创新，类似人类做阅读理解问题的思路“重复多读文章”，“带着问题读文章”等等，普通的“阅读理解思路”也都被实现了，这篇论文作者发现了很多看似复杂的问题其实通过简单的 context/type matching heruistic 就可以解出来了，过程是选择满足条件的 answer spans: 与 question 对应的 answer type 匹配，比如说问 when 就回答 time； 与重要的 question words 位置上临近； 添加问题单词是否出现在文章中这一“重要”特征；并没有使用复杂的question与context的交互，就取得了在SQuAD榜上与SOTA接近的结果，这篇论文之后，后来的研究者们在做MRC时也会将基础特征加入到embedding中进行共同训练，开源链接。 以下是阅读源码的一些总结： 1.Highway Network的使用Highway Network主要解决的问题是，网络深度加深，梯度信息回流受阻造成网络训练困难的问题。 当网络加深，训练的误差反而上升了，而加入了Highway Network之后，这个问题得到了缓解。一般来说，深度网络训练困难是由于梯度回流受阻的问题，可能浅层网络没有办法得到调整。Highway Network 受LSTM启发，增加了一个门函数，让网络的输出由两部分组成，分别是网络的直接输入以及输入变形后的部分。 网络中把此层放在embedding层后面 import tensorflow as tffrom keras import backend as Kfrom keras.engine.topology import Layerfrom keras.layers import Lambda, Wrapperclass Highway(Layer): def __init__(self, hidden_size, **kwargs): self.hidden_size = hidden_size super().__init__(**kwargs) def build(self, input_shape): self.projection = self.add_weight(name=&apos;projection&apos;, shape=(1, input_shape[-1], self.hidden_size), initializer=&apos;glorot_uniform&apos;) self.W_h = self.add_weight(name=&apos;W_h&apos;, shape=(1, self.hidden_size, self.hidden_size), initializer=&apos;glorot_uniform&apos;) self.b_h = self.add_weight(name=&apos;b_h&apos;, shape=(self.hidden_size,), initializer=&apos;zeros&apos;) self.W_t = self.add_weight(name=&apos;W_t&apos;, shape=(1, self.hidden_size, self.hidden_size), initializer=&apos;glorot_uniform&apos;) self.b_t = self.add_weight(name=&apos;b_t&apos;, shape=(self.hidden_size,), initializer=&apos;zeros&apos;) def call(self, x): x = K.conv1d(x, self.projection) H = tf.nn.tanh(K.bias_add(K.conv1d(x, self.W_h), self.b_h)) T = tf.nn.sigmoid(K.bias_add(K.conv1d(x, self.W_t), self.b_t)) return T * x + (1 - T) * H def compute_output_shape(self, input_shape): batch, seq_len, d = input_shape return (batch, seq_len, self.hidden_size) 2.tf.sequence_mask的学习这个操作和one hot也很像，但是指定的不是index而是从前到后有多少个True，返回的是True和False。 sq_mask = tf.sequence_mask([1, 3, 2], 5)print(sess.run(sq_mask)) 输出： [[True, False, False, False, False],[True, True, True, False, False],[True, True, False, False, False]] 3.tf.expand_dims()学习TensorFlow中，想要维度增加一维，可以使用 tf.expand_dims(input, dim, name=None) 函数。当然，我们常用tf.reshape(input,shape=[])也可以达到相同效果，但是有些时候在构建图的过程中，placeholder没有被feed具体的值，这时就会包下面的错误：TypeError: Expected binary or unicode string, got 1 在这种情况下，我们就可以考虑使用expand_dims来将维度加1。比如我自己代码中遇到的情况，在对图像维度降到二维做特定操作后，要还原成四维[batch, height, width, channels]，前后各增加一维。如果用reshape，则因为上述原因报错 给出官方的例子： # &apos;t&apos; is a tensor of shape [2]shape(expand_dims(t, 0)) ==&gt; [1, 2]shape(expand_dims(t, 1)) ==&gt; [2, 1]shape(expand_dims(t, -1)) ==&gt; [2, 1]# &apos;t2&apos; is a tensor of shape [2, 3, 5]shape(expand_dims(t2, 0)) ==&gt; [1, 2, 3, 5]shape(expand_dims(t2, 2)) ==&gt; [2, 3, 1, 5]shape(expand_dims(t2, 3)) ==&gt; [2, 3, 5, 1] Args: input: A Tensor. dim: A Tensor. Must be one of the following types: int32, int64. 0-D (scalar). Specifies the dimension index at which to expand the shape of input. name: A name for the operation (optional).Returns: A Tensor. Has the same type as input. Contains the same data as input, but its shape has an additional dimension of size 1 added. 4.tf.tile()学习推荐博客tf.tile( input, #输入 multiples, #某一维度上复制的次数 name=None ) import tensorflow as tfa = tf.constant([[1, 2], [3, 4], [5, 6]], dtype=tf.float32)a1 = tf.tile(a, [2, 3])a2 = tf.tile(a, [1, 2])with tf.Session() as sess: print(sess.run(a)) print(sess.run(a1)) print(sess.run(a2)) 输出： [[1. 2.] [3. 4.] [5. 6.]] [[1. 2. 1. 2. 1. 2.] [3. 4. 3. 4. 3. 4.] [5. 6. 5. 6. 5. 6.] [1. 2. 1. 2. 1. 2.] [3. 4. 3. 4. 3. 4.] [5. 6. 5. 6. 5. 6.]][[1. 2. 1. 2.] [3. 4. 3. 4.] [5. 6. 5. 6.]] 5.tf.equal()学习 equal，相等的意思。顾名思义，就是判断，x, y 是不是相等，它的判断方法不是整体判断，而是逐个元素进行判断，如果相等就是 True，不相等，就是 False。 由于是逐个元素判断，所以 x，y 的维度要一致。 例子： import tensorflow as tfa = [[1,2,3],[4,5,6]]b = [[1,0,3],[1,5,1]]with tf.Session() as sess: print(sess.run(tf.equal(a,b))) 输出： [[ True False True] [False True False]] 6.tf.reduce_any()学习在boolean张量的维度上计算元素的 “逻辑或” x = tf.constant([[True, True], [False, False]])with tf.Session() as sess: print(tf.reduce_any(x)) # True print(tf.reduce_any(x, 0)) # [True, True] print(tf.reduce_any(x, 1)) # [True, False] 7.tf.squeeze()学习该函数返回一个张量，这个张量是将原始input中所有维度为1的那些维都删掉的结果axis可以用来指定要删掉的为1的维度，此处要注意指定的维度必须确保其是1，否则会报错squeeze( input, axis=None, name=None, squeeze_dims=None) 例子：# &apos;t&apos; 是一个维度是[1, 2, 1, 3, 1, 1]的张量tf.shape(tf.squeeze(t)) # [2, 3]， 默认删除所有为1的维度# &apos;t&apos; 是一个维度[1, 2, 1, 3, 1, 1]的张量tf.shape(tf.squeeze(t, [2, 4])) # [1, 2, 3, 1]，标号从零开始，只删掉了2和4维的1 8.RepeatVector层RepeatVector层将输入重复n次keras.layers.core.RepeatVector(n) 参数 n：整数，重复的次数 输入shape形如（nb_samples, features）的2D张量 输出shape形如（nb_samples, n, features）的3D张量 例子 model = Sequential()model.add(Dense(32, input_dim=32))# now: model.output_shape == (None, 32)# note: `None` is the batch dimensionmodel.add(RepeatVector(3))# now: model.output_shape == (None, 3, 32) 9.tf.gather()学习类似于数组的索引，可以把向量中某些索引值提取出来，得到新的向量，适用于要提取的索引为不连续的情况。这个函数似乎只适合在一维的情况下使用。 import tensorflow as tf a = tf.Variable([[1,2,3,4,5], [6,7,8,9,10], [11,12,13,14,15]])index_a = tf.Variable([0,2]) b = tf.Variable([1,2,3,4,5,6,7,8,9,10])index_b = tf.Variable([2,4,6,8]) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(sess.run(tf.gather(a, index_a))) print(sess.run(tf.gather(b, index_b))) # [[ 1 2 3 4 5]# [11 12 13 14 15]] # [3 5 7 9] tf.gather_nd同上，但允许在多维上进行索引，例子只展示了一种很简单的用法，","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"阅读理解","slug":"阅读理解","permalink":"http://renxingkai.github.io/tags/阅读理解/"}],"author":"CinKate"},{"title":"基于NLTK的TF-IDF关键词抽取","slug":"tfidfkeyextraction","date":"2019-04-10T14:46:21.000Z","updated":"2020-05-17T16:12:00.477Z","comments":true,"path":"2019/04/10/tfidfkeyextraction/","link":"","permalink":"http://renxingkai.github.io/2019/04/10/tfidfkeyextraction/","excerpt":"","text":"基于nltk总结了用TF-IDF提取关键词的方法，同时总结了文本标准化（预处理），SVD分解、基于TF-IDF、词频等的关键词抽取 SVD奇异值分解from scipy.sparse.linalg import svdsimport reimport nltkimport unicodedatadef low_rank_svd(matrix,singular_count=2): u,s,vt=svds(matrix,k=singular_count) return u,s,vt 删除换行,进行分句def parse_document(document): document=re.sub(&apos;\\n&apos;,&apos; &apos;,document) if isinstance(document,str): document=document elif isinstance(document,unicode): return unicodedata.normalize(&apos;NFKD&apos;,document).encode(&apos;ascii&apos;,&apos;ignore&apos;) else: raise ValueError(&apos;Document is not string or unicode!&apos;) document=document.strip() sentences=nltk.sent_tokenize(document) sentences=[sentence.strip() for sentence in sentences] return sentences 转移HTML标签from html.parser import HTMLParser html_parser=HTMLParser()def unescape_html(parser,text): return parser.unescape_html(text) 缩写词表CONTRACTION_MAP = &#123;&quot;ain&apos;t&quot;: &quot;is not&quot;,&quot;aren&apos;t&quot;: &quot;are not&quot;,&quot;can&apos;t&quot;: &quot;cannot&quot;,&quot;can&apos;t&apos;ve&quot;: &quot;cannot have&quot;,&quot;&apos;cause&quot;: &quot;because&quot;,&quot;could&apos;ve&quot;: &quot;could have&quot;,&quot;couldn&apos;t&quot;: &quot;could not&quot;,&quot;couldn&apos;t&apos;ve&quot;: &quot;could not have&quot;,&quot;didn&apos;t&quot;: &quot;did not&quot;,&quot;doesn&apos;t&quot;: &quot;does not&quot;,&quot;don&apos;t&quot;: &quot;do not&quot;,&quot;hadn&apos;t&quot;: &quot;had not&quot;,&quot;hadn&apos;t&apos;ve&quot;: &quot;had not have&quot;,&quot;hasn&apos;t&quot;: &quot;has not&quot;,&quot;haven&apos;t&quot;: &quot;have not&quot;,&quot;he&apos;d&quot;: &quot;he would&quot;,&quot;he&apos;d&apos;ve&quot;: &quot;he would have&quot;,&quot;he&apos;ll&quot;: &quot;he will&quot;,&quot;he&apos;ll&apos;ve&quot;: &quot;he he will have&quot;,&quot;he&apos;s&quot;: &quot;he is&quot;,&quot;how&apos;d&quot;: &quot;how did&quot;,&quot;how&apos;d&apos;y&quot;: &quot;how do you&quot;,&quot;how&apos;ll&quot;: &quot;how will&quot;,&quot;how&apos;s&quot;: &quot;how is&quot;,&quot;I&apos;d&quot;: &quot;I would&quot;,&quot;I&apos;d&apos;ve&quot;: &quot;I would have&quot;,&quot;I&apos;ll&quot;: &quot;I will&quot;,&quot;I&apos;ll&apos;ve&quot;: &quot;I will have&quot;,&quot;I&apos;m&quot;: &quot;I am&quot;,&quot;I&apos;ve&quot;: &quot;I have&quot;,&quot;i&apos;d&quot;: &quot;i would&quot;,&quot;i&apos;d&apos;ve&quot;: &quot;i would have&quot;,&quot;i&apos;ll&quot;: &quot;i will&quot;,&quot;i&apos;ll&apos;ve&quot;: &quot;i will have&quot;,&quot;i&apos;m&quot;: &quot;i am&quot;,&quot;i&apos;ve&quot;: &quot;i have&quot;,&quot;isn&apos;t&quot;: &quot;is not&quot;,&quot;it&apos;d&quot;: &quot;it would&quot;,&quot;it&apos;d&apos;ve&quot;: &quot;it would have&quot;,&quot;it&apos;ll&quot;: &quot;it will&quot;,&quot;it&apos;ll&apos;ve&quot;: &quot;it will have&quot;,&quot;it&apos;s&quot;: &quot;it is&quot;,&quot;let&apos;s&quot;: &quot;let us&quot;,&quot;ma&apos;am&quot;: &quot;madam&quot;,&quot;mayn&apos;t&quot;: &quot;may not&quot;,&quot;might&apos;ve&quot;: &quot;might have&quot;,&quot;mightn&apos;t&quot;: &quot;might not&quot;,&quot;mightn&apos;t&apos;ve&quot;: &quot;might not have&quot;,&quot;must&apos;ve&quot;: &quot;must have&quot;,&quot;mustn&apos;t&quot;: &quot;must not&quot;,&quot;mustn&apos;t&apos;ve&quot;: &quot;must not have&quot;,&quot;needn&apos;t&quot;: &quot;need not&quot;,&quot;needn&apos;t&apos;ve&quot;: &quot;need not have&quot;,&quot;o&apos;clock&quot;: &quot;of the clock&quot;,&quot;oughtn&apos;t&quot;: &quot;ought not&quot;,&quot;oughtn&apos;t&apos;ve&quot;: &quot;ought not have&quot;,&quot;shan&apos;t&quot;: &quot;shall not&quot;,&quot;sha&apos;n&apos;t&quot;: &quot;shall not&quot;,&quot;shan&apos;t&apos;ve&quot;: &quot;shall not have&quot;,&quot;she&apos;d&quot;: &quot;she would&quot;,&quot;she&apos;d&apos;ve&quot;: &quot;she would have&quot;,&quot;she&apos;ll&quot;: &quot;she will&quot;,&quot;she&apos;ll&apos;ve&quot;: &quot;she will have&quot;,&quot;she&apos;s&quot;: &quot;she is&quot;,&quot;should&apos;ve&quot;: &quot;should have&quot;,&quot;shouldn&apos;t&quot;: &quot;should not&quot;,&quot;shouldn&apos;t&apos;ve&quot;: &quot;should not have&quot;,&quot;so&apos;ve&quot;: &quot;so have&quot;,&quot;so&apos;s&quot;: &quot;so as&quot;,&quot;that&apos;d&quot;: &quot;that would&quot;,&quot;that&apos;d&apos;ve&quot;: &quot;that would have&quot;,&quot;that&apos;s&quot;: &quot;that is&quot;,&quot;there&apos;d&quot;: &quot;there would&quot;,&quot;there&apos;d&apos;ve&quot;: &quot;there would have&quot;,&quot;there&apos;s&quot;: &quot;there is&quot;,&quot;they&apos;d&quot;: &quot;they would&quot;,&quot;they&apos;d&apos;ve&quot;: &quot;they would have&quot;,&quot;they&apos;ll&quot;: &quot;they will&quot;,&quot;they&apos;ll&apos;ve&quot;: &quot;they will have&quot;,&quot;they&apos;re&quot;: &quot;they are&quot;,&quot;they&apos;ve&quot;: &quot;they have&quot;,&quot;to&apos;ve&quot;: &quot;to have&quot;,&quot;wasn&apos;t&quot;: &quot;was not&quot;,&quot;we&apos;d&quot;: &quot;we would&quot;,&quot;we&apos;d&apos;ve&quot;: &quot;we would have&quot;,&quot;we&apos;ll&quot;: &quot;we will&quot;,&quot;we&apos;ll&apos;ve&quot;: &quot;we will have&quot;,&quot;we&apos;re&quot;: &quot;we are&quot;,&quot;we&apos;ve&quot;: &quot;we have&quot;,&quot;weren&apos;t&quot;: &quot;were not&quot;,&quot;what&apos;ll&quot;: &quot;what will&quot;,&quot;what&apos;ll&apos;ve&quot;: &quot;what will have&quot;,&quot;what&apos;re&quot;: &quot;what are&quot;,&quot;what&apos;s&quot;: &quot;what is&quot;,&quot;what&apos;ve&quot;: &quot;what have&quot;,&quot;when&apos;s&quot;: &quot;when is&quot;,&quot;when&apos;ve&quot;: &quot;when have&quot;,&quot;where&apos;d&quot;: &quot;where did&quot;,&quot;where&apos;s&quot;: &quot;where is&quot;,&quot;where&apos;ve&quot;: &quot;where have&quot;,&quot;who&apos;ll&quot;: &quot;who will&quot;,&quot;who&apos;ll&apos;ve&quot;: &quot;who will have&quot;,&quot;who&apos;s&quot;: &quot;who is&quot;,&quot;who&apos;ve&quot;: &quot;who have&quot;,&quot;why&apos;s&quot;: &quot;why is&quot;,&quot;why&apos;ve&quot;: &quot;why have&quot;,&quot;will&apos;ve&quot;: &quot;will have&quot;,&quot;won&apos;t&quot;: &quot;will not&quot;,&quot;won&apos;t&apos;ve&quot;: &quot;will not have&quot;,&quot;would&apos;ve&quot;: &quot;would have&quot;,&quot;wouldn&apos;t&quot;: &quot;would not&quot;,&quot;wouldn&apos;t&apos;ve&quot;: &quot;would not have&quot;,&quot;y&apos;all&quot;: &quot;you all&quot;,&quot;y&apos;all&apos;d&quot;: &quot;you all would&quot;,&quot;y&apos;all&apos;d&apos;ve&quot;: &quot;you all would have&quot;,&quot;y&apos;all&apos;re&quot;: &quot;you all are&quot;,&quot;y&apos;all&apos;ve&quot;: &quot;you all have&quot;,&quot;you&apos;d&quot;: &quot;you would&quot;,&quot;you&apos;d&apos;ve&quot;: &quot;you would have&quot;,&quot;you&apos;ll&quot;: &quot;you will&quot;,&quot;you&apos;ll&apos;ve&quot;: &quot;you will have&quot;,&quot;you&apos;re&quot;: &quot;you are&quot;,&quot;you&apos;ve&quot;: &quot;you have&quot;&#125; 文本标准化import stringfrom nltk.stem import WordNetLemmatizerstopword_list = nltk.corpus.stopwords.words(&apos;english&apos;)wnl = WordNetLemmatizer()html_parser = HTMLParser() 文本分词def tokenize_text(text): tokens = nltk.word_tokenize(text) tokens = [token.strip() for token in tokens] return tokens 扩展缩写def expand_contractions(text, contraction_mapping): contractions_pattern = re.compile(&apos;(&#123;&#125;)&apos;.format(&apos;|&apos;.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL) def expand_match(contraction): match = contraction.group(0) first_char = match[0] expanded_contraction = contraction_mapping.get(match if contraction_mapping.get(match) else contraction_mapping.get(match.lower())) expanded_contraction = first_char+expanded_contraction[1:] return expanded_contraction expanded_text = contractions_pattern.sub(expand_match, text) expanded_text = re.sub(&quot;&apos;&quot;, &quot;&quot;, expanded_text) return expanded_text 标记文本词性from pattern.en import tagfrom nltk.corpus import wordnet as wn# 标记文本词性def pos_tag_text(text): def penn_to_wn_tags(pos_tag): if pos_tag.startswith(&apos;J&apos;): return wn.ADJ elif pos_tag.startswith(&apos;V&apos;): return wn.VERB elif pos_tag.startswith(&apos;N&apos;): return wn.NOUN elif pos_tag.startswith(&apos;R&apos;): return wn.ADV else: return None tagged_text = tag(text) tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag)) for word, pos_tag in tagged_text] return tagged_lower_text 基于词性标签提取主干词def lemmatize_text(text): pos_tagged_text = pos_tag_text(text) lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag else word for word, pos_tag in pos_tagged_text] lemmatized_text = &apos; &apos;.join(lemmatized_tokens) return lemmatized_text 删除特殊字符def remove_special_characters(text): tokens = tokenize_text(text) pattern = re.compile(&apos;[&#123;&#125;]&apos;.format(re.escape(string.punctuation))) filtered_tokens = filter(None, [pattern.sub(&apos; &apos;, token) for token in tokens]) filtered_text = &apos; &apos;.join(filtered_tokens) return filtered_text 删除停用词def remove_stopwords(text): tokens = tokenize_text(text) filtered_tokens = [token for token in tokens if token not in stopword_list] filtered_text = &apos; &apos;.join(filtered_tokens) return filtered_text 转移HTML标签def unescape_html(parser, text): return parser.unescape(text) 标准化文本(合并执行上面流程)def normalize_corpus(corpus, lemmatize=True, tokenize=False): normalized_corpus = [] for text in corpus: text = html_parser.unescape(text) text = expand_contractions(text, CONTRACTION_MAP) if lemmatize: text = lemmatize_text(text) else: text = text.lower() text = remove_special_characters(text) text = remove_stopwords(text) if tokenize: text = tokenize_text(text) normalized_corpus.append(text) else: normalized_corpus.append(text) return normalized_corpus 文本特征提取 基于词项次数的二值特征 基于词袋模型的频率特征 TF-IDF权重特征 构建特征矩阵binary、frequency、tfidffrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizerdef build_feature_matrix(documents,feature_type=&apos;frequency&apos;): feature_type=feature_type.lower().strip() if feature_type==&apos;binary&apos;: vectorizer=CountVectorizer(binary=True,min_df=1,ngram_range=(1,1)) elif feature_type==&apos;frequency&apos;: vectorizer=CountVectorizer(binary=False,min_df=1,ngram_range=(1,1)) elif feature_type==&apos;tfidf&apos;: vectorizer=TfidfVectorizer(min_df=1,ngram_range=(1,1)) else: raise Exception(&quot;Wrong feature type entered. Possible values: &apos;binary&apos;, &apos;frequency&apos;, &apos;tfidf&apos;&quot;) feature_matrix=vectorizer.fit_transform(documents).astype(float) return vectorizer,feature_matrix 关键短语提取词项搭配from nltk.corpus import gutenbergimport nltkfrom operator import itemgetteralice = gutenberg.sents(fileids=&apos;carroll-alice.txt&apos;)alice = [&apos; &apos;.join(ts) for ts in alice]norm_alice = normalize_corpus(alice, lemmatize=False) 将语料压缩成1个大的文本串def flatten_corpus(corpus): return &apos; &apos;.join([document.strip() for document in corpus]) 计算n元分词（比较巧妙）def compute_ngrams(sequence,n):# print([sequence[index:] for index in range(n)])# print(list(zip(*[sequence[index:] for index in range(n)]))) #解压时仅按最小元素数组数量进行解压 return zip(*[sequence[index:] for index in range(n)]) 获取n元分词def get_top_ngram(corpus,ngram_val=1,limit=5): corpus=flatten_corpus(corpus) tokens=nltk.word_tokenize(corpus) ngrams=compute_ngrams(tokens,ngram_val) #获取单词频率 ngrams_freq_dist=nltk.FreqDist(ngrams) #排序频率 sorted_ngrams_fd=sorted(ngrams_freq_dist.items(),key=itemgetter(1),reverse=True) sorted_ngrams=sorted_ngrams_fd[0:limit] sorted_ngrams=[(&apos; &apos;.join(text),freq) for text,freq in sorted_ngrams] return sorted_ngrams 输出频率前10的二元分词get_top_ngram(corpus=norm_alice,ngram_val=2,limit=10) 输出结果[(&apos;said alice&apos;, 123), (&apos;mock turtle&apos;, 56), (&apos;march hare&apos;, 31), (&apos;said king&apos;, 29), (&apos;thought alice&apos;, 26), (&apos;white rabbit&apos;, 22), (&apos;said hatter&apos;, 22), (&apos;said mock&apos;, 20), (&apos;said caterpillar&apos;, 18), (&apos;said gryphon&apos;, 18)] 频率前10的三元分词get_top_ngram(corpus=norm_alice,ngram_val=3,limit=10) 输出结果 [(&apos;said mock turtle&apos;, 20), (&apos;said march hare&apos;, 9), (&apos;poor little thing&apos;, 6), (&apos;little golden key&apos;, 5), (&apos;certainly said alice&apos;, 5), (&apos;white kid gloves&apos;, 5), (&apos;march hare said&apos;, 5), (&apos;mock turtle said&apos;, 5), (&apos;know said alice&apos;, 4), (&apos;might well say&apos;, 4)] 频率前10的一元分词get_top_ngram(corpus=norm_alice,ngram_val=1,limit=10) 输出结果[(&apos;said&apos;, 462), (&apos;alice&apos;, 398), (&apos;little&apos;, 128), (&apos;one&apos;, 104), (&apos;know&apos;, 88), (&apos;like&apos;, 85), (&apos;would&apos;, 83), (&apos;went&apos;, 83), (&apos;could&apos;, 77), (&apos;queen&apos;, 75)] 使用nltk的搭配查找器二元词项from nltk.collocations import BigramCollocationFinderfrom nltk.collocations import BigramAssocMeasuresfinder=BigramCollocationFinder.from_documents([item.split() for item in norm_alice])bigram_measures=BigramAssocMeasures()#使用原始频率进行查找finder.nbest(bigram_measures.raw_freq,10) 输出结果 [(&apos;said&apos;, &apos;alice&apos;), (&apos;mock&apos;, &apos;turtle&apos;), (&apos;march&apos;, &apos;hare&apos;), (&apos;said&apos;, &apos;king&apos;), (&apos;thought&apos;, &apos;alice&apos;), (&apos;said&apos;, &apos;hatter&apos;), (&apos;white&apos;, &apos;rabbit&apos;), (&apos;said&apos;, &apos;mock&apos;), (&apos;said&apos;, &apos;caterpillar&apos;), (&apos;said&apos;, &apos;gryphon&apos;)] 二元使用点互信息PMI进行查找搭配finder.nbest(bigram_measures.pmi,10) 三元词组from nltk.collocations import TrigramAssocMeasuresfrom nltk.collocations import TrigramCollocationFinderfinder=TrigramCollocationFinder.from_documents([item.split() for item in norm_alice])trigram_measures=TrigramAssocMeasures()#三元组频率finder.nbest(trigram_measures.raw_freq,10) 输出结果[(&apos;said&apos;, &apos;mock&apos;, &apos;turtle&apos;), (&apos;said&apos;, &apos;march&apos;, &apos;hare&apos;), (&apos;poor&apos;, &apos;little&apos;, &apos;thing&apos;), (&apos;little&apos;, &apos;golden&apos;, &apos;key&apos;), (&apos;march&apos;, &apos;hare&apos;, &apos;said&apos;), (&apos;mock&apos;, &apos;turtle&apos;, &apos;said&apos;), (&apos;white&apos;, &apos;kid&apos;, &apos;gloves&apos;), (&apos;beau&apos;, &apos;ootiful&apos;, &apos;soo&apos;), (&apos;certainly&apos;, &apos;said&apos;, &apos;alice&apos;), (&apos;might&apos;, &apos;well&apos;, &apos;say&apos;)] 三元使用点互信息PMI进行查找搭配finder.nbest(trigram_measures.pmi,10) 输出结果 [(&apos;accustomed&apos;, &apos;usurpation&apos;, &apos;conquest&apos;), (&apos;adjourn&apos;, &apos;immediate&apos;, &apos;adoption&apos;), (&apos;adoption&apos;, &apos;energetic&apos;, &apos;remedies&apos;), (&apos;ancient&apos;, &apos;modern&apos;, &apos;seaography&apos;), (&apos;apple&apos;, &apos;roast&apos;, &apos;turkey&apos;), (&apos;arithmetic&apos;, &apos;ambition&apos;, &apos;distraction&apos;), (&apos;brother&apos;, &apos;latin&apos;, &apos;grammar&apos;), (&apos;canvas&apos;, &apos;bag&apos;, &apos;tied&apos;), (&apos;cherry&apos;, &apos;tart&apos;, &apos;custard&apos;), (&apos;circle&apos;, &apos;exact&apos;, &apos;shape&apos;)] 基于权重标签的短语提取 使用浅层分析提取所有名词短语词块 计算每个词块的TF-IDF权重并返回最大加权短语 toy_text = &quot;&quot;&quot;Elephants are large mammals of the family Elephantidae and the order Proboscidea. Two species are traditionally recognised, the African elephant and the Asian elephant. Elephants are scattered throughout sub-Saharan Africa, South Asia, and Southeast Asia. Male African elephants are the largest extant terrestrial animals. All elephants have a long trunk used for many purposes, particularly breathing, lifting water and grasping objects. Their incisors grow into tusks, which can serve as weapons and as tools for moving objects and digging. Elephants&apos; large ear flaps help to control their body temperature. Their pillar-like legs can carry their great weight. African elephants have larger ears and concave backs while Asian elephants have smaller ears and convex or level backs. &quot;&quot;&quot; import numpy as npimport itertoolsfrom gensim import corpora, models 基本上，我们有一个已定义的语法模式来分块或提取名词短语。我们在同一模式中定义一个分块器，对于文档中的每个句子，首先用它的POS标签来标注它(因此，不应该对文本进行规范化)，然后构建一个具有名词短语的浅层分析树作为词块和其他全部基于POS标签的单词作为缝隙，缝隙是不属于任何词块的部分。完成此操作后，我们使用tree2conl1tags函数来生成(w, t，c)三元组，它们是的单词、POS标签和IOB格式的词块标签。删除所有词块带有’O ‘标签的标签，因为它们基本上是不属于任何词块的单词或词项。最后，从这些有效的词块中，组合分块的词项，并从每个词块分组中生成短语。 提取文档中的名词短语 v adj adv#提取文档中的名词短语 v adj advdef get_chunks(sentences, grammar = r&apos;NP: &#123;&lt;DT&gt;? &lt;JJ&gt;* &lt;NN.*&gt;+&#125;&apos;): all_chunks = [] chunker = nltk.chunk.regexp.RegexpParser(grammar) for sentence in sentences: tagged_sents = nltk.pos_tag_sents( [nltk.word_tokenize(sentence)]) chunks = [chunker.parse(tagged_sent) for tagged_sent in tagged_sents] wtc_sents = [nltk.chunk.tree2conlltags(chunk) for chunk in chunks] flattened_chunks = list( itertools.chain.from_iterable( wtc_sent for wtc_sent in wtc_sents) )# print(flattened_chunks)# print(flattened_chunks) valid_chunks_tagged = [(status, [wtc for wtc in chunk]) for status, chunk in itertools.groupby(flattened_chunks,lambda chunk: chunk != &apos;O&apos;)]# print(&apos;---&apos;*20)# print(valid_chunks_tagged) valid_chunks = [&apos; &apos;.join(word.lower() for word, tag, chunk in wtc_group if word.lower() not in stopword_list) for status, wtc_group in valid_chunks_tagged if status] all_chunks.append(valid_chunks) return all_chunks sentences = parse_document(toy_text) valid_chunks = get_chunks(sentences) 获取TF-IDF关键短语权重def get_tfidf_weighted_keyphrases(sentences, grammar=r&apos;NP: &#123;&lt;DT&gt;? &lt;JJ&gt;* &lt;NN.*&gt;+&#125;&apos;, top_n=10): valid_chunks = get_chunks(sentences, grammar=grammar) dictionary = corpora.Dictionary(valid_chunks) corpus = [dictionary.doc2bow(chunk) for chunk in valid_chunks] tfidf = models.TfidfModel(corpus) corpus_tfidf = tfidf[corpus] weighted_phrases = &#123;dictionary.get(id): round(value,3) for doc in corpus_tfidf for id, value in doc&#125; weighted_phrases = sorted(weighted_phrases.items(), key=itemgetter(1), reverse=True) return weighted_phrases[:top_n] 前两个关键短语get_tfidf_weighted_keyphrases(sentences, top_n=2) 输出结果 [(&apos;elephants large mammals family elephantidae order proboscidea .&apos;, 1.0), (&apos;two species traditionally recognised , african elephant asian elephant .&apos;, 1.0)] 其他语料实验get_tfidf_weighted_keyphrases(alice, top_n=10) 输出结果 [(&quot;[ alice &apos; adventures wonderland lewis carroll 1865 ]&quot;, 1.0), (&apos;chapter .&apos;, 1.0), (&apos;rabbit - hole&apos;, 1.0), (&quot;alice beginning get tired sitting sister bank , nothing : twice peeped book sister reading , pictures conversations , &apos; use book , &apos; thought alice &apos; without pictures conversation ? &apos;&quot;, 1.0), (&apos;considering mind ( well could , hot day made feel sleepy stupid ) , whether pleasure making daisy - chain would worth trouble getting picking daisies , suddenly white rabbit pink eyes ran close .&apos;, 1.0), (&quot;nothing remarkable ; alice think much way hear rabbit say , &apos; oh dear !&quot;, 1.0), (&apos;oh dear !&apos;, 1.0), (&quot;shall late ! &apos;&quot;, 1.0), (&apos;( thought afterwards , occurred ought wondered , time seemed quite natural ) ; rabbit actually took watch waistcoat - pocket , looked , hurried , alice started feet , flashed across mind never seen rabbit either waistcoat - pocket , watch take , burning curiosity , ran across field , fortunately time see pop large rabbit - hole hedge .&apos;, 1.0), (&apos;another moment went alice , never considering world get .&apos;, 1.0)]","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"关键词抽取","slug":"关键词抽取","permalink":"http://renxingkai.github.io/tags/关键词抽取/"}],"author":"CinKate"},{"title":"Word2Vec相关(用TFIDF加权词向量)","slug":"word-tfidf","date":"2019-04-05T10:34:06.000Z","updated":"2020-05-17T16:12:00.563Z","comments":true,"path":"2019/04/05/word-tfidf/","link":"","permalink":"http://renxingkai.github.io/2019/04/05/word-tfidf/","excerpt":"","text":"今天是快乐的清明节，而博主还在实验室敲代码，23333这次记录下Word2Vec相关的姿势~ Word2Vec模型直接用开源的gensism库进行词向量训练： import gensimimport nltkimport numpy as np#自制语料CORPUS = [&apos;the sky is blue&apos;,&apos;sky is blue and sky is beautiful&apos;,&apos;the beautiful sky is so blue&apos;,&apos;i love blue cheese&apos;]new_doc = [&apos;loving this blue sky today&apos;] 对语料进行分词 #tokenize corpusTOKENIZED_CORPUS=[nltk.word_tokenize(sentence) for sentence in CORPUS]tokenized_new_doc=[nltk.word_tokenize(sentence) for sentence in new_doc]print(TOKENIZED_CORPUS)print(tokenized_new_doc) 输出 [[&apos;the&apos;, &apos;sky&apos;, &apos;is&apos;, &apos;blue&apos;], [&apos;sky&apos;, &apos;is&apos;, &apos;blue&apos;, &apos;and&apos;, &apos;sky&apos;, &apos;is&apos;, &apos;beautiful&apos;], [&apos;the&apos;, &apos;beautiful&apos;, &apos;sky&apos;, &apos;is&apos;, &apos;so&apos;, &apos;blue&apos;], [&apos;i&apos;, &apos;love&apos;, &apos;blue&apos;, &apos;cheese&apos;]] [[&apos;loving&apos;, &apos;this&apos;, &apos;blue&apos;, &apos;sky&apos;, &apos;today&apos;]] 构建词向量 model=gensim.models.Word2Vec(TOKENIZED_CORPUS,size=10,window=10,min_count=2,sample=1e-3) 平均词向量来表示文档 #num_features表示的文本单词大小def average_word_vectors(words,model,vocabulary,num_features): feature_vector=np.zeros((num_features,),dtype=&apos;float64&apos;) nwords=0 for word in words: if word in vocabulary: nwords=nwords+1 feature_vector=np.add(feature_vector,model[word]) if nwords: feature_vector=np.divide(feature_vector,nwords) return feature_vectordef averaged_word_vectorizer(corpus,model,num_features): #get the all vocabulary vocabulary=set(model.wv.index2word) features=[average_word_vectors(tokenized_sentence,model,vocabulary,num_features) for tokenized_sentence in corpus] return np.array(features) avg_word_vec_features=averaged_word_vectorizer(TOKENIZED_CORPUS,model=model,num_features=10)print(avg_word_vec_features) 输出array([[-0.00710545, -0.01549264, 0.02188712, -0.00322829, 0.00586532, -0.00687592, 0.00339291, -0.01177494, 0.00265543, -0.00539964], [-0.0157312 , -0.01630003, 0.00551589, 0.00166568, 0.02385859, 0.0085727 , 0.02538068, -0.02266891, 0.02231819, -0.02521743], [-0.0070758 , -0.00578274, 0.01280785, -0.00960104, 0.00821758, -0.00023592, 0.01009926, -0.00624976, 0.00913788, -0.01323305], [ 0.01571231, -0.02214988, 0.02293927, -0.03584988, -0.02027377, 0.00031135, 0.00284845, 0.01365358, 0.00845861, -0.0247597 ]]) nd_avg_word_vec_features=averaged_word_vectorizer(corpus=tokenized_new_doc,model=model,num_features=10)print(nd_avg_word_vec_features) 输出array([[-0.00968785, -0.02889012, 0.02670473, -0.01596956, 0.00815679, -0.00325876, 0.02226594, -0.01347479, 0.01384218, -0.01042995]]) # TF-IDF加权平均词向量如果直接求平均效果不好的话，或者过于简单的话，可以对词求TFIDF，然后乘以相应的权重 def tfidf_wtd_avg_word_vectors(words,tfidf_vector,tfidf_vocabulary,model,num_features): word_tfidfs=[tfidf_vector[0,tfidf_vocabulary.get(word)] if tfidf_vocabulary.get(word) else 0 for word in words] word_tfidf_map=&#123;word:tfidf_val for word,tfidf_val in zip(words,word_tfidfs)&#125; feature_vector=np.zeros((num_features,),dtype=&apos;float64&apos;) vocabulary=set(model.wv.index2word) wts=0 for word in words: if word in vocabulary: word_vector=model[word] weighted_word_vector=word_tfidf_map[word]*word_vector wts=wts+word_tfidf_map[word] feature_vector=np.add(feature_vector,weighted_word_vector) if wts: feature_vector=np.divide(feature_vector,wts) return feature_vectordef tfidf_weighted_averaged_word_vectorizer(corpus,tfidf_vectors,tfidf_vocabulary,model,num_features): docs_tfidfs=[(doc,doc_tfidf) for doc,doc_tfidf in zip(corpus,tfidf_vectors)] features=[tfidf_wtd_avg_word_vectors(tokenized_sentence,tfidf,tfidf_vocabulary,model,num_features) for tokenized_sentence,tfidf in docs_tfidfs] return np.array(features) TFIDF预处理from sklearn.feature_extraction.text import TfidfTransformerfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.feature_extraction.text import CountVectorizerimport pandas as pddef tfidf_transformer(bow_matrix): transformer = TfidfTransformer(norm=&apos;l2&apos;, smooth_idf=True, use_idf=True) tfidf_matrix = transformer.fit_transform(bow_matrix) return transformer, tfidf_matrixdef tfidf_extractor(corpus, ngram_range=(1,1)): vectorizer = TfidfVectorizer(min_df=1, norm=&apos;l2&apos;, smooth_idf=True, use_idf=True, ngram_range=ngram_range) features = vectorizer.fit_transform(corpus) return vectorizer, featuresdef bow_extractor(corpus, ngram_range=(1,1)): #min_df为1说明文档中词频最小为1也会被考虑 #ngram_range可以设置(1,3)将建立包括所有unigram、bigram、trigram的向量空间 vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range) features = vectorizer.fit_transform(corpus) return vectorizer, featuresdef display_features(features, feature_names): df = pd.DataFrame(data=features, columns=feature_names) print(df) bow_vectorizer, bow_features = bow_extractor(CORPUS)feature_names = bow_vectorizer.get_feature_names()tfidf_trans, tfidf_features = tfidf_transformer(bow_features)tfidf_vectorizer, tdidf_features = tfidf_extractor(CORPUS)display_features(np.round(tdidf_features.todense(), 2), feature_names)nd_tfidf = tfidf_vectorizer.transform(new_doc)display_features(np.round(nd_tfidf.todense(), 2), feature_names) TFIDF加权词向量corpus_tfidf=tfidf_featuresvocab=tfidf_vectorizer.vocabulary_ wt_tfidf_word_vec_features=tfidf_weighted_averaged_word_vectorizer(corpus=TOKENIZED_CORPUS,tfidf_vectors=corpus_tfidf,tfidf_vocabulary=vocab,model=model,num_features=10)print(wt_tfidf_word_vec_features) 输出array([[-0.00728862, -0.01345045, 0.02334223, -0.00258989, 0.00500905, -0.00913428, 0.00057808, -0.01095917, -0.00025702, -0.00165257], [-0.02009719, -0.01936696, 0.0056747 , 0.00887485, 0.02952368, 0.00819392, 0.02715274, -0.0298718 , 0.02297843, -0.0237992 ], [-0.00721121, -0.00258696, 0.01239834, -0.01018197, 0.00795635, -0.00085167, 0.00906817, -0.00469667, 0.00799437, -0.01167674], [ 0.01571231, -0.02214988, 0.02293927, -0.03584988, -0.02027377, 0.00031135, 0.00284845, 0.01365358, 0.0084586 , -0.0247597 ]]) nd_wt_tfidf_word_vec_features=tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_new_doc,tfidf_vectors=nd_tfidf,tfidf_vocabulary=vocab,model=model,num_features=10)print(nd_wt_tfidf_word_vec_features) 输出 array([[-0.01223734, -0.02956665, 0.02708268, -0.01397412, 0.01101045, -0.00361711, 0.02421493, -0.01619775, 0.01438254, -0.00899163]])","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"词向量相关","slug":"词向量相关","permalink":"http://renxingkai.github.io/tags/词向量相关/"}],"author":"CinKate"},{"title":"《基于BiDAF多文档重排序的阅读理解模型》论文阅读","slug":"rbidaf","date":"2019-04-02T18:58:21.000Z","updated":"2020-05-17T16:12:00.222Z","comments":true,"path":"2019/04/03/rbidaf/","link":"","permalink":"http://renxingkai.github.io/2019/04/03/rbidaf/","excerpt":"","text":"0 引言目前的机器学习方法主要有两类：抽取式和生成式，抽取式通过给定问题以及相关的文章进行训练,让机器具备阅读的能力，并对提出的新问题,在相关文章中抽取出相应的答案。另一种是生成式,从理论_上来说不受知识的局限,对于问题自动生成答案,但是生成式有时产生的答案答非所问，句式不通,不能很好地体现出人类的思维逻辑以及自然表述的特点。 在本文中，提出了一种基于BiDAF模型的RBiDAF机器阅读理解模型。该模型是一-种抽取式的机器阅读理解模型,在BiDAF模型四层网络框架的基础，上添加了ParaRanking层，针对ParaRank-ing,本文提出了多特征融合的ParaRanking 算法，此外本文还在答案预测层,提出了基于先验知识的多答案交叉验证算法,进而对答案进行综合预测。 1 机器阅读理解和相关工作自斯坦福机器阅读理解数据集SQuAD问世以来,经过谷歌、微软、百度、科大讯飞、腾讯、斯坦福大学等在内的众多研究机构的不懈努力,形成了“词向量化-语义编码-语义交互-答案预测”这样一-套四层机器阅读理解模型体系。该体系的主要思想是:首先将自然文本表示为可计算的向量,其次融合问题向量与支撑文档向量来学习到语义交互信息,最后根据交互信息预测答案的位置或逐一输出最大概率的字词来生成答案。 词向量化层(Word-Embedder) 的作用是使用词向量技术将分词后的自然文本转化为稠密、连续且可计算的低维向量。 文本编码层(Encoder)的作用是进行语义加工，向量化层输出的结果是一串独立的词向量序列,而编码层根据这些词向量捕捉词与词的前后语义关系,并把这种语义关系融入词向量中,生成一串互相关联的文本编码序列。 语义交互层（Interaction-Layer)是整个模型体系中最重要的一环。在进入这一层之前，问题与给定支撑文档在大多数情况下是分别独立进行向量转化与语义编码的。当然，在有些模型中，问题词向量序列也会被提前融合到文档向量中。当前大部分研究工作集中在语义交互层的设计上，在这一层，将最终得到混合两者语义的交互向量。此外，交互向量也时常与未交互前的问题编码向量直接拼接，以强调问题语义的重要性。 答案预测层（Answer-Layer)负责根据语义交互向量产出 最终的答案。目前，答案预测模型主要是生成模型与边界模型（边界模型常用的有Pointer Network指出答案所在的开始、结束位置）。答案预测层将答案均视作一串词序列，生成模型逐个预测该序列中每个词应使用给定文档中哪个词进行填充，每次预测均基于之前预测完成的结果。边界模型则相当于一个简化的生成模型，其预先假定问题都可以使用给定文档中的一个连续短语或句子进行回答，因此只需预测答案的起点与终点词的位置即可。目前，边界模型的预测效率与结果均好于生成模型。 以下作者对比了近年来在SQuAD榜上的部分阅读理解模型： Match-LSTM(2016提出， 发表于ICLR 17)Match-LSTM是首个应用于SQuAD数据的端到端机器阅读理解模型，并成功超越原有使用人工特征进行答案抽取的基线模型。该模型的特点是：（1）在文本编码层使用单向LSTM进行语义建模；（2）在语义交互层对支撑文档中的每个词计算该词在问题编码向量上的注意力分配向量，将这一注意力分配向量与问题编码向量点乘获得文档词–问题交互向量，并再拼接上文档词编码向量，最后用一个新的单向LSTM网络对拼接后的向量进行二次语义编码；（3)用反向LSTM重复（1)、（2)操作，并将正反向二次语义编码向量拼接。 BIDAF(2016提出， 发表于ICLR 17)BIDAF可以视作对Match-LSTM匹配模型的改进。其主要变化在于：（1)词向量化层增加了对字的向量化；（2)在语义交互 充了问题中每个词在文档编码向量上的注意力分配向量， 以提升文档词-问题交互向量的语义交互程度；（3）改用双向LSTM网络二次语义编码。与单向LSTM相比，双向LSTM可以同时提取到每个词相关的上下文信息。 R-Net(发表于ACL 17)R-Net是对Match-LSTM匹配模型的改进。这一模型最大的特点是采用了双语义交互层设计。在一级语义交互层，R-Net仿照Match-LSTM实现将问题信息融入到每个文档词中去；而在二级语义交互层，R-Net则使用相同办法将已经获得的文档词–问题语义编码向量再度与问题编码向量二次融合，进一步加强语义匹配。 QANet(发表于ICLR 18)QANet则是一种在BIDAF模型基础上为追求效率而设计的模型。该模型非常创新地在文本编码层使用CNN与Multi-Head Self-Attention机制实现语义编码，由于CNN可以捕捉局部特征、Self-Attention能够捕捉全局特征，因此完全可以用它们替代传统的LSTM网络。此外，由于CNN的建模效率显著高于LSTM网络，该模型以在更大规模的数据集上进行深度学习——泛化能力得到了进一步提升。这一模型可以在SQuAD数据集上达到训练速度提高3〜13倍！推理速度提高4~9倍，且获得与先前基于LSTM网络媲美的精度。 V-net(百度公司发表于ACL 18)V-net是一种新的多文档校验深度神经网络建模方法，该模型通过注意力使不同候选文档抽取的答案能够互相印证，从而预测出更好的答案。 2 数据探索和数据处理百度数据集与其他数据集很大的区别在于，每篇文章中包含了很多个段落，而SQuAD数据集的支撑文档直接是一个最相关段落，微软数据集MS MARCO则是若干篇只有一个段落的文章。因此，在百度机器阅读理解任务中，需要在主流四层体系的基础上，增加一个段落定位层。 在DuReader原文中提到，使用recall指标增加的段落定位层，并使用recall指标进行段落选择，可以使模型的效果至少10% 3 RBiDAF模型设计与实现本文提出的基于BiDAF模型的RBiDAF模型，主要是在BiDAF模型的基础上添加了ParaRanking，在该层提出了ParaRanking算法，从而对候选段落进行排序（ParaRanking)操作，进而筛选出含答案概率更高的候选段落。 此外在答案预测层，提出了基于先验知识的多答案交叉验证（MACVerify)算法，从而对答案进行综合预测。 3.1 ParaRanking算法DuReader数据集中，每一个问题对应多个段落,尤其是在Search数据集中，问题和段落的比接近1:57,所以应该尽量检索出含有答案的段落,从而减小候选段落集的数据规模。在这里本文提出了多特征融合的ParaRanking算法,图8是ParaRanking算法的大体架构,主要包括段落过滤、段落重组、语义匹配、最大覆盖度、特征加权以及多文档投票。 3.1.1 段落过滤 本文利用特征工程根据问题类型对不相关段落进行过滤,例如,实体类型的问题中,问题中的关键词是“联系方式”、“热线”，那么本文利用正则表达式将不含电话号码的段落进行过滤，最终本文设计了23条规则对段落进行初步过滤。 3.1.2段落重组 DuReader数据集中的段落长度极度不平衡,有些段落的长度很短,这种情况会造成段落的上下文信息缺失，不利于模型的Match操作。而且本文通过观察训练集中答案的分布，发现有些答案是跨段落的，尤其是描述类的问题，所以如果仅仅以某-一个原始段落作为预测的输人，那么将无法解决答案跨段落的问题,因此本文将原始的段落进行重组，重组后长度控制在长度splice_ L之内。 3.1.3语义匹配 问题(question)与段落(paragraph)间的匹配不仅要考虑问题和段落之间的显式关系,还要考虑两者之间的隐式关系，即两者之间的语义关系。例如，question:北京2017年的商业住房的均价是多少?paragraph:据我所知是四万元一平。上例question和paragraph之间的最大覆盖度虽然为0,但是两者之间具有极大的语义相关性,并且“四万元一平”极有可能是答案。所以为了克服字词匹配上的弊端，本文选择利用深度神经网络计算question和para-graph之间的语义相关性。 由于ARC-II保留了词序信息,更具一般性，所以本文采用ARC-II文本匹配模型对question以及paragraph之间的语义相关度进行计算,在第一层中,首先把卷积窗口设定为k1,然后对句子Squestion和句子Sprangraph中所有组合的二维矩阵进行卷积,每一个二维矩阵输出一个值(文中把这个称作一维卷积,因为实际上是把组合中所有词语的vector排成一行进行的卷积计算),构成Layer-2,然后进行2X2的MaxPooling。后续的卷积层均是传统的二维卷积操作，与第一层卷积层后的简单MaxPooling方式不同,后续的卷积层的Pooling是一种动态Pooling方法。输出固定维度的向量,接着输人MLP层,最终得到文本相似度分数ps。 3.1.4 最大覆盖度 本文沿用了基线模型的最大覆盖度算法,DuReader的基线模型采用问题和段落的最大词级别的覆盖度算法对段落进行排序,然后对每一个篇章挑选top-1作为模型的输入,本文将问题与段落的最大覆盖度作为ParaRanking的一个重要特征值定义为pc,其中不同于基线模型中最大覆盖度算法的是,这里分别选择了词和字两个粒度进行最大覆盖度计算,两者相加作为最终pc的值。 3.1.5 特征加权 首先通过分析DuReader的训练集可知,在描述类问题的答案中存在大量列表类型的答案，所以本文针对描述类问题识别出段落中的列表信息,并根据这一特征对段落的ParaRanking值进行加权，定义权值为B。经过语义匹配、最大覆盖度计算以及特征加权可以得到问题和段落i的最终匹配得分，如式下式所示。 3.1.6 多文档投票 本文两次用到多文档投票，一次在ParaRanking操作中，一次在答案预测中，前后两次所用到的方法有些不同。使用多文档投票是基于某一问题的正确答案在多个段落中会多次出现这一假设。首先 定义候选段落集合为Dp，对于段落i属于Dp，那么每一个段落的投票得分如下式所示。 所以最终得分段落，的最终得分为: 其中，f函数是指数平滑函数，最终经过ParaRanking 算法 ，每一个段落,i(属于Dp)会生成一个分score，随后根据score选择输人模型的段落集合Df，并且Df数量远远小于Dp。 RBiDAF模型架构 5 总结与展望本文提出了一种基于BiDAF模型的RBiDAF机器阅读理解模型。首先对DuReader数据集进行分析并对数据进行清洗,从而提取出有利于模型训练的特征;然后本文对RBiDAF机器阅读理解模型进行相关设计和实现,该模型的创新点在于在BiDAF模型四层网络框架的基础上添加了ParaRanking层,在该层,本文提出了基于多特征融合的ParaRanking算法。此外本文还在答案预测层,提出了基于先验知识的MACVerify算法,利用该算法对答案进行综合预测。最后经过实验和分析,RBiDAF模型能够产生有效的答案。在未来的工作中，首先将尝试实验多种词嵌入方法,很多学者证实选择合适的词嵌人方法对该任务会产生很大的影响;其次尝试采用机器翻译模型与对抗式生成模型(GAN)增强训练语料;最后在文本交互层融合双向注意力(Bi-Attention)与多轮匹配机制(Multi-Matching),从而可以在多文档场景下取得更好的效果。 论文地址","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/categories/深度学习/"}],"tags":[{"name":"阅读理解","slug":"阅读理解","permalink":"http://renxingkai.github.io/tags/阅读理解/"},{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/tags/深度学习/"}],"author":"CinKate"},{"title":"英文文本预处理代码","slug":"textpreprocess","date":"2019-03-29T21:50:08.000Z","updated":"2020-05-17T16:12:00.814Z","comments":true,"path":"2019/03/30/textpreprocess/","link":"","permalink":"http://renxingkai.github.io/2019/03/30/textpreprocess/","excerpt":"","text":"贴一段在做Kaggle QIQC时别人开源的kernel英语文本预处理代码，在做英文nlp任务时还是很有用的~ import osimport reimport gcimport stringimport unicodedataimport operatorimport numpy as npimport pandas as pdfrom tqdm import tqdmtqdm.pandas()&quot;&quot;&quot;utils&quot;&quot;&quot;def load_data(datapath): print(&quot;loading data ......&quot;) df_train = pd.read_csv(os.path.join(datapath, &quot;train.csv&quot;)) df_test = pd.read_csv(os.path.join(datapath, &quot;test.csv&quot;)) print(&quot;train data with shape : &quot;, df_train.shape) print(&quot;test data with shape : &quot;, df_test.shape) return df_train, df_test&quot;&quot;&quot;nlp&quot;&quot;&quot;def clean_misspell(text): &quot;&quot;&quot; misspell list (quora vs. glove) &quot;&quot;&quot; misspell_to_sub = &#123; &apos;Terroristan&apos;: &apos;terrorist Pakistan&apos;, &apos;terroristan&apos;: &apos;terrorist Pakistan&apos;, &apos;BIMARU&apos;: &apos;Bihar, Madhya Pradesh, Rajasthan, Uttar Pradesh&apos;, &apos;Hinduphobic&apos;: &apos;Hindu phobic&apos;, &apos;hinduphobic&apos;: &apos;Hindu phobic&apos;, &apos;Hinduphobia&apos;: &apos;Hindu phobic&apos;, &apos;hinduphobia&apos;: &apos;Hindu phobic&apos;, &apos;Babchenko&apos;: &apos;Arkady Arkadyevich Babchenko faked death&apos;, &apos;Boshniaks&apos;: &apos;Bosniaks&apos;, &apos;Dravidanadu&apos;: &apos;Dravida Nadu&apos;, &apos;mysoginists&apos;: &apos;misogynists&apos;, &apos;MGTOWS&apos;: &apos;Men Going Their Own Way&apos;, &apos;mongloid&apos;: &apos;Mongoloid&apos;, &apos;unsincere&apos;: &apos;insincere&apos;, &apos;meninism&apos;: &apos;male feminism&apos;, &apos;jewplicate&apos;: &apos;jewish replicate&apos;, &apos;unoin&apos;: &apos;Union&apos;, &apos;daesh&apos;: &apos;Islamic State of Iraq and the Levant&apos;, &apos;Kalergi&apos;: &apos;Coudenhove-Kalergi&apos;, &apos;Bhakts&apos;: &apos;Bhakt&apos;, &apos;bhakts&apos;: &apos;Bhakt&apos;, &apos;Tambrahms&apos;: &apos;Tamil Brahmin&apos;, &apos;Pahul&apos;: &apos;Amrit Sanskar&apos;, &apos;SJW&apos;: &apos;social justice warrior&apos;, &apos;SJWs&apos;: &apos;social justice warrior&apos;, &apos; incel&apos;: &apos; involuntary celibates&apos;, &apos; incels&apos;: &apos; involuntary celibates&apos;, &apos;emiratis&apos;: &apos;Emiratis&apos;, &apos;weatern&apos;: &apos;western&apos;, &apos;westernise&apos;: &apos;westernize&apos;, &apos;Pizzagate&apos;: &apos;Pizzagate conspiracy theory&apos;, &apos;naïve&apos;: &apos;naive&apos;, &apos;Skripal&apos;: &apos;Sergei Skripal&apos;, &apos;Remainers&apos;: &apos;British remainer&apos;, &apos;remainers&apos;: &apos;British remainer&apos;, &apos;bremainer&apos;: &apos;British remainer&apos;, &apos;antibrahmin&apos;: &apos;anti Brahminism&apos;, &apos;HYPSM&apos;: &apos; Harvard, Yale, Princeton, Stanford, MIT&apos;, &apos;HYPS&apos;: &apos; Harvard, Yale, Princeton, Stanford&apos;, &apos;kompromat&apos;: &apos;compromising material&apos;, &apos;Tharki&apos;: &apos;pervert&apos;, &apos;tharki&apos;: &apos;pervert&apos;, &apos;mastuburate&apos;: &apos;masturbate&apos;, &apos;Zoë&apos;: &apos;Zoe&apos;, &apos;indans&apos;: &apos;Indian&apos;, &apos; xender&apos;: &apos; gender&apos;, &apos;Naxali &apos;: &apos;Naxalite &apos;, &apos;Naxalities&apos;: &apos;Naxalites&apos;, &apos;Bathla&apos;: &apos;Namit Bathla&apos;, &apos;Mewani&apos;: &apos;Indian politician Jignesh Mevani&apos;, &apos;clichéd&apos;: &apos;cliche&apos;, &apos;cliché&apos;: &apos;cliche&apos;, &apos;clichés&apos;: &apos;cliche&apos;, &apos;Wjy&apos;: &apos;Why&apos;, &apos;Fadnavis&apos;: &apos;Indian politician Devendra Fadnavis&apos;, &apos;Awadesh&apos;: &apos;Indian engineer Awdhesh Singh&apos;, &apos;Awdhesh&apos;: &apos;Indian engineer Awdhesh Singh&apos;, &apos;Khalistanis&apos;: &apos;Sikh separatist movement&apos;, &apos;madheshi&apos;: &apos;Madheshi&apos;, &apos;BNBR&apos;: &apos;Be Nice, Be Respectful&apos;, &apos;Bolsonaro&apos;: &apos;Jair Bolsonaro&apos;, &apos;XXXTentacion&apos;: &apos;Tentacion&apos;, &apos;Padmavat&apos;: &apos;Indian Movie Padmaavat&apos;, &apos;Žižek&apos;: &apos;Slovenian philosopher Slavoj Žižek&apos;, &apos;Adityanath&apos;: &apos;Indian monk Yogi Adityanath&apos;, &apos;Brexit&apos;: &apos;British Exit&apos;, &apos;Brexiter&apos;: &apos;British Exit supporter&apos;, &apos;Brexiters&apos;: &apos;British Exit supporters&apos;, &apos;Brexiteer&apos;: &apos;British Exit supporter&apos;, &apos;Brexiteers&apos;: &apos;British Exit supporters&apos;, &apos;Brexiting&apos;: &apos;British Exit&apos;, &apos;Brexitosis&apos;: &apos;British Exit disorder&apos;, &apos;brexit&apos;: &apos;British Exit&apos;, &apos;brexiters&apos;: &apos;British Exit supporters&apos;, &apos;jallikattu&apos;: &apos;Jallikattu&apos;, &apos;fortnite&apos;: &apos;Fortnite &apos;, &apos;Swachh&apos;: &apos;Swachh Bharat mission campaign &apos;, &apos;Quorans&apos;: &apos;Quoran&apos;, &apos;Qoura &apos;: &apos;Quora &apos;, &apos;quoras&apos;: &apos;Quora&apos;, &apos;Quroa&apos;: &apos;Quora&apos;, &apos;QUORA&apos;: &apos;Quora&apos;, &apos;narcissit&apos;: &apos;narcissist&apos;, # extra in sample &apos;Doklam&apos;: &apos;Tibet&apos;, &apos;Drumpf &apos;: &apos;Donald Trump fool &apos;, &apos;Drumpfs&apos;: &apos;Donald Trump fools&apos;, &apos;Strzok&apos;: &apos;Hillary Clinton scandal&apos;, &apos;rohingya&apos;: &apos;Rohingya &apos;, &apos;wumao &apos;: &apos;cheap Chinese stuff&apos;, &apos;wumaos&apos;: &apos;cheap Chinese stuff&apos;, &apos;Sanghis&apos;: &apos;Sanghi&apos;, &apos;Tamilans&apos;: &apos;Tamils&apos;, &apos;biharis&apos;: &apos;Biharis&apos;, &apos;Rejuvalex&apos;: &apos;hair growth formula&apos;, &apos;Feku&apos;: &apos;The Man of India &apos;, &apos;deplorables&apos;: &apos;deplorable&apos;, &apos;muhajirs&apos;: &apos;Muslim immigrant&apos;, &apos;Gujratis&apos;: &apos;Gujarati&apos;, &apos;Chutiya&apos;: &apos;Tibet people &apos;, &apos;Chutiyas&apos;: &apos;Tibet people &apos;, &apos;thighing&apos;: &apos;masturbate&apos;, &apos;卐&apos;: &apos;Nazi Germany&apos;, &apos;Pribumi&apos;: &apos;Native Indonesian&apos;, &apos;Gurmehar&apos;: &apos;Gurmehar Kaur Indian student activist&apos;, &apos;Novichok&apos;: &apos;Soviet Union agents&apos;, &apos;Khazari&apos;: &apos;Khazars&apos;, &apos;Demonetization&apos;: &apos;demonetization&apos;, &apos;demonetisation&apos;: &apos;demonetization&apos;, &apos;demonitisation&apos;: &apos;demonetization&apos;, &apos;demonitization&apos;: &apos;demonetization&apos;, &apos;demonetisation&apos;: &apos;demonetization&apos;, &apos;cryptocurrencies&apos;: &apos;cryptocurrency&apos;, &apos;Hindians&apos;: &apos;North Indian who hate British&apos;, &apos;vaxxer&apos;: &apos;vocal nationalist &apos;, &apos;remoaner&apos;: &apos;remainer &apos;, &apos;bremoaner&apos;: &apos;British remainer &apos;, &apos;Jewism&apos;: &apos;Judaism&apos;, &apos;Eroupian&apos;: &apos;European&apos;, &apos;WMAF&apos;: &apos;White male married Asian female&apos;, &apos;moeslim&apos;: &apos;Muslim&apos;, &apos;cishet&apos;: &apos;cisgender and heterosexual person&apos;, &apos;Eurocentric&apos;: &apos;Eurocentrism &apos;, &apos;Jewdar&apos;: &apos;Jew dar&apos;, &apos;Asifa&apos;: &apos;abduction, rape, murder case &apos;, &apos;marathis&apos;: &apos;Marathi&apos;, &apos;Trumpanzees&apos;: &apos;Trump chimpanzee fool&apos;, &apos;Crimean&apos;: &apos;Crimea people &apos;, &apos;atrracted&apos;: &apos;attract&apos;, &apos;LGBT&apos;: &apos;lesbian, gay, bisexual, transgender&apos;, &apos;Boshniak&apos;: &apos;Bosniaks &apos;, &apos;Myeshia&apos;: &apos;widow of Green Beret killed in Niger&apos;, &apos;demcoratic&apos;: &apos;Democratic&apos;, &apos;raaping&apos;: &apos;rape&apos;, &apos;Dönmeh&apos;: &apos;Islam&apos;, &apos;feminazism&apos;: &apos;feminism nazi&apos;, &apos;langague&apos;: &apos;language&apos;, &apos;Hongkongese&apos;: &apos;HongKong people&apos;, &apos;hongkongese&apos;: &apos;HongKong people&apos;, &apos;Kashmirians&apos;: &apos;Kashmirian&apos;, &apos;Chodu&apos;: &apos;fucker&apos;, &apos;penish&apos;: &apos;penis&apos;, &apos;micropenis&apos;: &apos;tiny penis&apos;, &apos;Madridiots&apos;: &apos;Real Madrid idiot supporters&apos;, &apos;Ambedkarite&apos;: &apos;Dalit Buddhist movement &apos;, &apos;ReleaseTheMemo&apos;: &apos;cry for the right and Trump supporters&apos;, &apos;harrase&apos;: &apos;harass&apos;, &apos;Barracoon&apos;: &apos;Black slave&apos;, &apos;Castrater&apos;: &apos;castration&apos;, &apos;castrater&apos;: &apos;castration&apos;, &apos;Rapistan&apos;: &apos;Pakistan rapist&apos;, &apos;rapistan&apos;: &apos;Pakistan rapist&apos;, &apos;Turkified&apos;: &apos;Turkification&apos;, &apos;turkified&apos;: &apos;Turkification&apos;, &apos;Dumbassistan&apos;: &apos;dumb ass Pakistan&apos;, &apos;facetards&apos;: &apos;Facebook retards&apos;, &apos;rapefugees&apos;: &apos;rapist refugee&apos;, &apos;superficious&apos;: &apos;superficial&apos;, # extra from kagglers &apos;colour&apos;: &apos;color&apos;, &apos;centre&apos;: &apos;center&apos;, &apos;favourite&apos;: &apos;favorite&apos;, &apos;travelling&apos;: &apos;traveling&apos;, &apos;counselling&apos;: &apos;counseling&apos;, &apos;theatre&apos;: &apos;theater&apos;, &apos;cancelled&apos;: &apos;canceled&apos;, &apos;labour&apos;: &apos;labor&apos;, &apos;organisation&apos;: &apos;organization&apos;, &apos;wwii&apos;: &apos;world war 2&apos;, &apos;citicise&apos;: &apos;criticize&apos;, &apos;youtu &apos;: &apos;youtube &apos;, &apos;sallary&apos;: &apos;salary&apos;, &apos;Whta&apos;: &apos;What&apos;, &apos;narcisist&apos;: &apos;narcissist&apos;, &apos;narcissit&apos;: &apos;narcissist&apos;, &apos;howdo&apos;: &apos;how do&apos;, &apos;whatare&apos;: &apos;what are&apos;, &apos;howcan&apos;: &apos;how can&apos;, &apos;howmuch&apos;: &apos;how much&apos;, &apos;howmany&apos;: &apos;how many&apos;, &apos;whydo&apos;: &apos;why do&apos;, &apos;doI&apos;: &apos;do I&apos;, &apos;theBest&apos;: &apos;the best&apos;, &apos;howdoes&apos;: &apos;how does&apos;, &apos;mastrubation&apos;: &apos;masturbation&apos;, &apos;mastrubate&apos;: &apos;masturbate&apos;, &apos;mastrubating&apos;: &apos;masturbating&apos;, &apos;pennis&apos;: &apos;penis&apos;, &apos;Etherium&apos;: &apos;Ethereum&apos;, &apos;bigdata&apos;: &apos;big data&apos;, &apos;2k17&apos;: &apos;2017&apos;, &apos;2k18&apos;: &apos;2018&apos;, &apos;qouta&apos;: &apos;quota&apos;, &apos;exboyfriend&apos;: &apos;ex boyfriend&apos;, &apos;airhostess&apos;: &apos;air hostess&apos;, &apos;whst&apos;: &apos;what&apos;, &apos;watsapp&apos;: &apos;whatsapp&apos;, # extra &apos;bodyshame&apos;: &apos;body shaming&apos;, &apos;bodyshoppers&apos;: &apos;body shopping&apos;, &apos;bodycams&apos;: &apos;body cams&apos;, &apos;Cananybody&apos;: &apos;Can any body&apos;, &apos;deadbody&apos;: &apos;dead body&apos;, &apos;deaddict&apos;: &apos;de addict&apos;, &apos;Northindian&apos;: &apos;North Indian &apos;, &apos;northindian&apos;: &apos;north Indian &apos;, &apos;northkorea&apos;: &apos;North Korea&apos;, &apos;Whykorean&apos;: &apos;Why Korean&apos;, &apos;koreaboo&apos;: &apos;Korea boo &apos;, &apos;Brexshit&apos;: &apos;British Exit bullshit&apos;, &apos;shithole&apos;: &apos; shithole &apos;, &apos;shitpost&apos;: &apos;shit post&apos;, &apos;shitslam&apos;: &apos;shit Islam&apos;, &apos;shitlords&apos;: &apos;shit lords&apos;, &apos;Fck&apos;: &apos;Fuck&apos;, &apos;fck&apos;: &apos;fuck&apos;, &apos;Clickbait&apos;: &apos;click bait &apos;, &apos;clickbait&apos;: &apos;click bait &apos;, &apos;mailbait&apos;: &apos;mail bait&apos;, &apos;healhtcare&apos;: &apos;healthcare&apos;, &apos;trollbots&apos;: &apos;troll bots&apos;, &apos;trollled&apos;: &apos;trolled&apos;, &apos;trollimg&apos;: &apos;trolling&apos;, &apos;cybertrolling&apos;: &apos;cyber trolling&apos;, &apos;sickular&apos;: &apos;India sick secular &apos;, &apos;suckimg&apos;: &apos;sucking&apos;, &apos;Idiotism&apos;: &apos;idiotism&apos;, &apos;Niggerism&apos;: &apos;Nigger&apos;, &apos;Niggeriah&apos;: &apos;Nigger&apos; &#125; misspell_re = re.compile(&apos;(%s)&apos; % &apos;|&apos;.join(misspell_to_sub.keys())) def _replace(match): &quot;&quot;&quot; reference: https://www.kaggle.com/hengzheng/attention-capsule-why-not-both-lb-0-694 # noqa &quot;&quot;&quot; try: word = misspell_to_sub.get(match.group(0)) except KeyError: word = match.group(0) print(&apos;!!Error: Could Not Find Key: &#123;&#125;&apos;.format(word)) return word return misspell_re.sub(_replace, text)def spacing_misspell(text): &quot;&quot;&quot; &apos;deadbody&apos; -&gt; &apos;dead body&apos; &quot;&quot;&quot; misspell_list = [ &apos;(F|f)uck&apos;, &apos;Trump&apos;, &apos;\\W(A|a)nti&apos;, &apos;(W|w)hy&apos;, &apos;(W|w)hat&apos;, &apos;How&apos;, &apos;care\\W&apos;, &apos;\\Wover&apos;, &apos;gender&apos;, &apos;people&apos;, ] misspell_re = re.compile(&apos;(%s)&apos; % &apos;|&apos;.join(misspell_list)) return misspell_re.sub(r&quot; \\1 &quot;, text)def clean_latex(text): &quot;&quot;&quot; convert r&quot;[math]\\vec&#123;x&#125; + \\vec&#123;y&#125;&quot; to English &quot;&quot;&quot; # edge case text = re.sub(r&apos;\\[math\\]&apos;, &apos; LaTex math &apos;, text) text = re.sub(r&apos;\\[\\/math\\]&apos;, &apos; LaTex math &apos;, text) text = re.sub(r&apos;\\\\&apos;, &apos; LaTex &apos;, text) pattern_to_sub = &#123; r&apos;\\\\mathrm&apos;: &apos; LaTex math mode &apos;, r&apos;\\\\mathbb&apos;: &apos; LaTex math mode &apos;, r&apos;\\\\boxed&apos;: &apos; LaTex equation &apos;, r&apos;\\\\begin&apos;: &apos; LaTex equation &apos;, r&apos;\\\\end&apos;: &apos; LaTex equation &apos;, r&apos;\\\\left&apos;: &apos; LaTex equation &apos;, r&apos;\\\\right&apos;: &apos; LaTex equation &apos;, r&apos;\\\\(over|under)brace&apos;: &apos; LaTex equation &apos;, r&apos;\\\\text&apos;: &apos; LaTex equation &apos;, r&apos;\\\\vec&apos;: &apos; vector &apos;, r&apos;\\\\var&apos;: &apos; variable &apos;, r&apos;\\\\theta&apos;: &apos; theta &apos;, r&apos;\\\\mu&apos;: &apos; average &apos;, r&apos;\\\\min&apos;: &apos; minimum &apos;, r&apos;\\\\max&apos;: &apos; maximum &apos;, r&apos;\\\\sum&apos;: &apos; + &apos;, r&apos;\\\\times&apos;: &apos; * &apos;, r&apos;\\\\cdot&apos;: &apos; * &apos;, r&apos;\\\\hat&apos;: &apos; ^ &apos;, r&apos;\\\\frac&apos;: &apos; / &apos;, r&apos;\\\\div&apos;: &apos; / &apos;, r&apos;\\\\sin&apos;: &apos; Sine &apos;, r&apos;\\\\cos&apos;: &apos; Cosine &apos;, r&apos;\\\\tan&apos;: &apos; Tangent &apos;, r&apos;\\\\infty&apos;: &apos; infinity &apos;, r&apos;\\\\int&apos;: &apos; integer &apos;, r&apos;\\\\in&apos;: &apos; in &apos;, &#125; # post process for look up pattern_dict = &#123;k.strip(&apos;\\\\&apos;): v for k, v in pattern_to_sub.items()&#125; # init re patterns = pattern_to_sub.keys() pattern_re = re.compile(&apos;(%s)&apos; % &apos;|&apos;.join(patterns)) def _replace(match): &quot;&quot;&quot; reference: https://www.kaggle.com/hengzheng/attention-capsule-why-not-both-lb-0-694 # noqa &quot;&quot;&quot; try: word = pattern_dict.get(match.group(0).strip(&apos;\\\\&apos;)) except KeyError: word = match.group(0) print(&apos;!!Error: Could Not Find Key: &#123;&#125;&apos;.format(word)) return word return pattern_re.sub(_replace, text)def normalize_unicode(text): &quot;&quot;&quot; unicode string normalization &quot;&quot;&quot; return unicodedata.normalize(&apos;NFKD&apos;, text)def remove_newline(text): &quot;&quot;&quot; remove \\n and \\t &quot;&quot;&quot; text = re.sub(&apos;\\n&apos;, &apos; &apos;, text) text = re.sub(&apos;\\t&apos;, &apos; &apos;, text) text = re.sub(&apos;\\b&apos;, &apos; &apos;, text) text = re.sub(&apos;\\r&apos;, &apos; &apos;, text) return textdef decontracted(text): &quot;&quot;&quot; de-contract the contraction &quot;&quot;&quot; # specific text = re.sub(r&quot;(W|w)on(\\&apos;|\\’)t&quot;, &quot;will not&quot;, text) text = re.sub(r&quot;(C|c)an(\\&apos;|\\’)t&quot;, &quot;can not&quot;, text) text = re.sub(r&quot;(Y|y)(\\&apos;|\\’)all&quot;, &quot;you all&quot;, text) text = re.sub(r&quot;(Y|y)a(\\&apos;|\\’)ll&quot;, &quot;you all&quot;, text) # general text = re.sub(r&quot;(I|i)(\\&apos;|\\’)m&quot;, &quot;i am&quot;, text) text = re.sub(r&quot;(A|a)in(\\&apos;|\\’)t&quot;, &quot;is not&quot;, text) text = re.sub(r&quot;n(\\&apos;|\\’)t&quot;, &quot; not&quot;, text) text = re.sub(r&quot;(\\&apos;|\\’)re&quot;, &quot; are&quot;, text) text = re.sub(r&quot;(\\&apos;|\\’)s&quot;, &quot; is&quot;, text) text = re.sub(r&quot;(\\&apos;|\\’)d&quot;, &quot; would&quot;, text) text = re.sub(r&quot;(\\&apos;|\\’)ll&quot;, &quot; will&quot;, text) text = re.sub(r&quot;(\\&apos;|\\’)t&quot;, &quot; not&quot;, text) text = re.sub(r&quot;(\\&apos;|\\’)ve&quot;, &quot; have&quot;, text) return textdef spacing_punctuation(text): &quot;&quot;&quot; add space before and after punctuation and symbols &quot;&quot;&quot; regular_punct = list(string.punctuation) extra_punct = [ &apos;,&apos;, &apos;.&apos;, &apos;&quot;&apos;, &apos;:&apos;, &apos;)&apos;, &apos;(&apos;, &apos;-&apos;, &apos;!&apos;, &apos;?&apos;, &apos;|&apos;, &apos;;&apos;, &quot;&apos;&quot;, &apos;$&apos;, &apos;&amp;&apos;, &apos;/&apos;, &apos;[&apos;, &apos;]&apos;, &apos;&gt;&apos;, &apos;%&apos;, &apos;=&apos;, &apos;#&apos;, &apos;*&apos;, &apos;+&apos;, &apos;\\\\&apos;, &apos;•&apos;, &apos;~&apos;, &apos;@&apos;, &apos;£&apos;, &apos;·&apos;, &apos;_&apos;, &apos;&#123;&apos;, &apos;&#125;&apos;, &apos;©&apos;, &apos;^&apos;, &apos;®&apos;, &apos;`&apos;, &apos;&lt;&apos;, &apos;→&apos;, &apos;°&apos;, &apos;€&apos;, &apos;™&apos;, &apos;›&apos;, &apos;♥&apos;, &apos;←&apos;, &apos;×&apos;, &apos;§&apos;, &apos;″&apos;, &apos;′&apos;, &apos;Â&apos;, &apos;█&apos;, &apos;½&apos;, &apos;à&apos;, &apos;…&apos;, &apos;“&apos;, &apos;★&apos;, &apos;”&apos;, &apos;–&apos;, &apos;●&apos;, &apos;â&apos;, &apos;►&apos;, &apos;−&apos;, &apos;¢&apos;, &apos;²&apos;, &apos;¬&apos;, &apos;░&apos;, &apos;¶&apos;, &apos;↑&apos;, &apos;±&apos;, &apos;¿&apos;, &apos;▾&apos;, &apos;═&apos;, &apos;¦&apos;, &apos;║&apos;, &apos;―&apos;, &apos;¥&apos;, &apos;▓&apos;, &apos;—&apos;, &apos;‹&apos;, &apos;─&apos;, &apos;▒&apos;, &apos;：&apos;, &apos;¼&apos;, &apos;⊕&apos;, &apos;▼&apos;, &apos;▪&apos;, &apos;†&apos;, &apos;■&apos;, &apos;’&apos;, &apos;▀&apos;, &apos;¨&apos;, &apos;▄&apos;, &apos;♫&apos;, &apos;☆&apos;, &apos;é&apos;, &apos;¯&apos;, &apos;♦&apos;, &apos;¤&apos;, &apos;▲&apos;, &apos;è&apos;, &apos;¸&apos;, &apos;¾&apos;, &apos;Ã&apos;, &apos;⋅&apos;, &apos;‘&apos;, &apos;∞&apos;, &apos;∙&apos;, &apos;）&apos;, &apos;↓&apos;, &apos;、&apos;, &apos;│&apos;, &apos;（&apos;, &apos;»&apos;, &apos;，&apos;, &apos;♪&apos;, &apos;╩&apos;, &apos;╚&apos;, &apos;³&apos;, &apos;・&apos;, &apos;╦&apos;, &apos;╣&apos;, &apos;╔&apos;, &apos;╗&apos;, &apos;▬&apos;, &apos;❤&apos;, &apos;ï&apos;, &apos;Ø&apos;, &apos;¹&apos;, &apos;≤&apos;, &apos;‡&apos;, &apos;√&apos;, &apos;«&apos;, &apos;»&apos;, &apos;´&apos;, &apos;º&apos;, &apos;¾&apos;, &apos;¡&apos;, &apos;§&apos;, &apos;£&apos;, &apos;₤&apos;] all_punct = &apos;&apos;.join(sorted(list(set(regular_punct + extra_punct)))) re_tok = re.compile(f&apos;([&#123;all_punct&#125;])&apos;) return re_tok.sub(r&apos; \\1 &apos;, text)def spacing_digit(text): &quot;&quot;&quot; add space before and after digits &quot;&quot;&quot; re_tok = re.compile(&apos;([0-9])&apos;) return re_tok.sub(r&apos; \\1 &apos;, text)def spacing_number(text): &quot;&quot;&quot; add space before and after numbers &quot;&quot;&quot; re_tok = re.compile(&apos;([0-9]&#123;1,&#125;)&apos;) return re_tok.sub(r&apos; \\1 &apos;, text)def remove_number(text): &quot;&quot;&quot; numbers are not toxic &quot;&quot;&quot; return re.sub(&apos;\\d+&apos;, &apos; &apos;, text)def remove_space(text): &quot;&quot;&quot; remove extra spaces and ending space if any &quot;&quot;&quot; text = re.sub(&apos;\\s+&apos;, &apos; &apos;, text) text = re.sub(&apos;\\s+$&apos;, &apos;&apos;, text) return text&quot;&quot;&quot;tokenizer&quot;&quot;&quot;def preprocess(text, remove_num=True): &quot;&quot;&quot; preprocess text into clean text for tokenization NOTE: 1. glove supports uppper case words 2. glove supports digit 3. glove supports punctuation 5. glove supports domains e.g. www.apple.com 6. glove supports misspelled words e.g. FUCKKK &quot;&quot;&quot; # # 1. normalize # text = normalize_unicode(text) # # 2. remove new line # text = remove_newline(text) # 3. de-contract text = decontracted(text) # 4. clean misspell text = clean_misspell(text) # 5. space misspell text = spacing_misspell(text) # 6. clean_latex text = clean_latex(text) # 7. space text = spacing_punctuation(text) # 8. handle number if remove_num: text = remove_number(text) else: text = spacing_digit(text) # 9. remove space text = remove_space(text) return text 调用preprocess(text) 就好，返回处理完后的文本","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"文本预处理","slug":"文本预处理","permalink":"http://renxingkai.github.io/tags/文本预处理/"}],"author":"CinKate"},{"title":"nltk---词性标注","slug":"nltk-postag","date":"2019-03-29T09:06:25.000Z","updated":"2020-05-17T16:11:59.994Z","comments":true,"path":"2019/03/29/nltk-postag/","link":"","permalink":"http://renxingkai.github.io/2019/03/29/nltk-postag/","excerpt":"","text":"1.POS标签器推荐使用nltk推荐的pos_tag()函数，基于Penn Treebank，以下代码展示了使用nltk获取句子POS标签的方法： sentence = &apos;The brown fox is quick and he is jumping over the lazy dog&apos;# recommended tagger based on PTBimport nltktokens = nltk.word_tokenize(sentence)tagged_sent = nltk.pos_tag(tokens, tagset=&apos;universal&apos;)print (tagged_sent) 输出： [(&apos;The&apos;, &apos;DET&apos;), (&apos;brown&apos;, &apos;ADJ&apos;), (&apos;fox&apos;, &apos;NOUN&apos;), (&apos;is&apos;, &apos;VERB&apos;), (&apos;quick&apos;, &apos;ADJ&apos;), (&apos;and&apos;, &apos;CONJ&apos;), (&apos;he&apos;, &apos;PRON&apos;), (&apos;is&apos;, &apos;VERB&apos;), (&apos;jumping&apos;, &apos;VERB&apos;), (&apos;over&apos;, &apos;ADP&apos;), (&apos;the&apos;, &apos;DET&apos;), (&apos;lazy&apos;, &apos;ADJ&apos;), (&apos;dog&apos;, &apos;NOUN&apos;)] 2.建立自己的POS标签器准备数据：# preparing the datafrom nltk.corpus import treebankdata = treebank.tagged_sents()train_data = data[:3500]test_data = data[3500:]print (train_data[0]) 输出：[(&apos;Pierre&apos;, &apos;NNP&apos;), (&apos;Vinken&apos;, &apos;NNP&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;61&apos;, &apos;CD&apos;), (&apos;years&apos;, &apos;NNS&apos;), (&apos;old&apos;, &apos;JJ&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;will&apos;, &apos;MD&apos;), (&apos;join&apos;, &apos;VB&apos;), (&apos;the&apos;, &apos;DT&apos;), (&apos;board&apos;, &apos;NN&apos;), (&apos;as&apos;, &apos;IN&apos;), (&apos;a&apos;, &apos;DT&apos;), (&apos;nonexecutive&apos;, &apos;JJ&apos;), (&apos;director&apos;, &apos;NN&apos;), (&apos;Nov.&apos;, &apos;NNP&apos;), (&apos;29&apos;, &apos;CD&apos;), (&apos;.&apos;, &apos;.&apos;)] 2.1DefaultTagger默认标签器首先我们试下从SequentialBackoffTagger基类继承的DefaultTagger，并为每个单词分配相同的用户输入POS标签。 # default taggerfrom nltk.tag import DefaultTaggerdt = DefaultTagger(&apos;NN&apos;)print(dt.evaluate(test_data))print(dt.tag(tokens)) 输出： 0.1454158195372253[(&apos;The&apos;, &apos;NN&apos;), (&apos;brown&apos;, &apos;NN&apos;), (&apos;fox&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;NN&apos;), (&apos;quick&apos;, &apos;NN&apos;), (&apos;and&apos;, &apos;NN&apos;), (&apos;he&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;NN&apos;), (&apos;jumping&apos;, &apos;NN&apos;), (&apos;over&apos;, &apos;NN&apos;), (&apos;the&apos;, &apos;NN&apos;), (&apos;lazy&apos;, &apos;NN&apos;), (&apos;dog&apos;, &apos;NN&apos;)] 默认获得了14%的准确率，由于给标签器输入的都是相同的标签（‘NN’），因此输出标签获得的都是名词。 2.2RegexpTagger正则表达式标签器# regex taggerfrom nltk.tag import RegexpTagger# define regex tag patternspatterns = [ (r&apos;.*ing$&apos;, &apos;VBG&apos;), # gerunds (r&apos;.*ed$&apos;, &apos;VBD&apos;), # simple past (r&apos;.*es$&apos;, &apos;VBZ&apos;), # 3rd singular present (r&apos;.*ould$&apos;, &apos;MD&apos;), # modals (r&apos;.*\\&apos;s$&apos;, &apos;NN$&apos;), # possessive nouns (r&apos;.*s$&apos;, &apos;NNS&apos;), # plural nouns (r&apos;^-?[0-9]+(.[0-9]+)?$&apos;, &apos;CD&apos;), # cardinal numbers (r&apos;.*&apos;, &apos;NN&apos;) # nouns (default) ... ]rt = RegexpTagger(patterns)print(rt.evaluate(test_data))print(rt.tag(tokens)) 输出： 0.24039113176493368[(&apos;The&apos;, &apos;NN&apos;), (&apos;brown&apos;, &apos;NN&apos;), (&apos;fox&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;NNS&apos;), (&apos;quick&apos;, &apos;NN&apos;), (&apos;and&apos;, &apos;NN&apos;), (&apos;he&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;NNS&apos;), (&apos;jumping&apos;, &apos;VBG&apos;), (&apos;over&apos;, &apos;NN&apos;), (&apos;the&apos;, &apos;NN&apos;), (&apos;lazy&apos;, &apos;NN&apos;), (&apos;dog&apos;, &apos;NN&apos;)] 准确率提高到了24%，还是有效果的~ 2.3一、二、三元标签器## N gram taggersfrom nltk.tag import UnigramTaggerfrom nltk.tag import BigramTaggerfrom nltk.tag import TrigramTaggerut = UnigramTagger(train_data)bt = BigramTagger(train_data)tt = TrigramTagger(train_data)print(ut.evaluate(test_data))print(ut.tag(tokens))print(bt.evaluate(test_data))print(bt.tag(tokens))print (tt.evaluate(test_data))print(tt.tag(tokens)) 输出： 0.8607803272340013[(&apos;The&apos;, &apos;DT&apos;), (&apos;brown&apos;, None), (&apos;fox&apos;, None), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;quick&apos;, &apos;JJ&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;he&apos;, &apos;PRP&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;jumping&apos;, &apos;VBG&apos;), (&apos;over&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;DT&apos;), (&apos;lazy&apos;, None), (&apos;dog&apos;, None)]0.13466937748087907[(&apos;The&apos;, &apos;DT&apos;), (&apos;brown&apos;, None), (&apos;fox&apos;, None), (&apos;is&apos;, None), (&apos;quick&apos;, None), (&apos;and&apos;, None), (&apos;he&apos;, None), (&apos;is&apos;, None), (&apos;jumping&apos;, None), (&apos;over&apos;, None), (&apos;the&apos;, None), (&apos;lazy&apos;, None), (&apos;dog&apos;, None)]0.08064672281924679[(&apos;The&apos;, &apos;DT&apos;), (&apos;brown&apos;, None), (&apos;fox&apos;, None), (&apos;is&apos;, None), (&apos;quick&apos;, None), (&apos;and&apos;, None), (&apos;he&apos;, None), (&apos;is&apos;, None), (&apos;jumping&apos;, None), (&apos;over&apos;, None), (&apos;the&apos;, None), (&apos;lazy&apos;, None), (&apos;dog&apos;, None)] 发现一元的准确率最高，达到了86%,二、三元准确率低的原因可能是在训练数据中观察到的二元词组和三元词组不一定会在测试数据中以相同的方式出现。 2.4包含标签列表的组合标签器及使用backoff标签器本质上，我们将创建一个标签器链，对于每一个标签器，吐过他不能标记输入的标识，则标签器的下一步将会回退到backoff标签器： def combined_tagger(train_data, taggers, backoff=None): for tagger in taggers: backoff = tagger(train_data, backoff=backoff) return backoff#backoff to regtaggerct = combined_tagger(train_data=train_data, taggers=[UnigramTagger, BigramTagger, TrigramTagger], backoff=rt)print(ct.evaluate(test_data)) print(ct.tag(tokens)) 输出： 0.9094781682641108[(&apos;The&apos;, &apos;DT&apos;), (&apos;brown&apos;, &apos;NN&apos;), (&apos;fox&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;quick&apos;, &apos;JJ&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;he&apos;, &apos;PRP&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;jumping&apos;, &apos;VBG&apos;), (&apos;over&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;DT&apos;), (&apos;lazy&apos;, &apos;NN&apos;), (&apos;dog&apos;, &apos;NN&apos;)] 准确率到了90% 2.5ClassifierBasedPOSTagger标签器(有监督分类算法)使用ClassifierBasedPOSTagger类中的classifier_builder参数中的有监督机器学习算法来训练标签器。 from nltk.classify import NaiveBayesClassifier, MaxentClassifierfrom nltk.tag.sequential import ClassifierBasedPOSTaggernbt = ClassifierBasedPOSTagger(train=train_data, classifier_builder=NaiveBayesClassifier.train)print(nbt.evaluate(test_data))print(nbt.tag(tokens)) 输出： 0.9306806079969019[(&apos;The&apos;, &apos;DT&apos;), (&apos;brown&apos;, &apos;JJ&apos;), (&apos;fox&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;quick&apos;, &apos;JJ&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;he&apos;, &apos;PRP&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;jumping&apos;, &apos;VBG&apos;), (&apos;over&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;DT&apos;), (&apos;lazy&apos;, &apos;JJ&apos;), (&apos;dog&apos;, &apos;VBG&apos;)] 有监督准确率达到了0.93 2.6Just For Fun!(MaxentClassifier)# try this out for fun!met = ClassifierBasedPOSTagger(train=train_data, classifier_builder=MaxentClassifier.train)print(met.evaluate(test_data)) print(met.tag(tokens)) 输出： ==&gt; Training (100 iterations) Iteration Log Likelihood Accuracy --------------------------------------- 1 -3.82864 0.007 2 -0.76176 0.957 Final nan 0.9840.9269048310581857[(&apos;The&apos;, &apos;DT&apos;), (&apos;brown&apos;, &apos;NN&apos;), (&apos;fox&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;quick&apos;, &apos;JJ&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;he&apos;, &apos;PRP&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;jumping&apos;, &apos;VBG&apos;), (&apos;over&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;DT&apos;), (&apos;lazy&apos;, &apos;NN&apos;), (&apos;dog&apos;, &apos;NN&apos;)] MaxentClassifier准确率达到了0.92","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"文本预处理","slug":"文本预处理","permalink":"http://renxingkai.github.io/tags/文本预处理/"},{"name":"词性标注","slug":"词性标注","permalink":"http://renxingkai.github.io/tags/词性标注/"}],"author":"CinKate"},{"title":"nltk---分词与文本预处理","slug":"nltk-tokenize","date":"2019-03-28T15:08:16.000Z","updated":"2020-05-17T16:12:00.073Z","comments":true,"path":"2019/03/28/nltk-tokenize/","link":"","permalink":"http://renxingkai.github.io/2019/03/28/nltk-tokenize/","excerpt":"","text":"参考《text-analytics-with-python》中的第三章中的处理和理解文本对nltk等常用nlp包进行总结，以供之后复习与使用~ 1.tokenize(切分词(句子))首先，标识(token)是具有一定的句法语义且独立的最小文本成分， 1.1句子切分句子切分基本技术包括在句子之间寻找特定的分割符，例如句号(‘.’)，换行符(‘\\n’)或者分号(‘;’)等。在nltk中，主要关注以下句子切分器: nltk.sent_tokenize(默认句子切分器) nltk.tokenize.PunktSentenceTokenizer() nltk.tokenize.RegexpTokenizer()以下直接上代码： import nltkfrom nltk.corpus import gutenbergfrom pprint import pprint#载入语料alice=gutenberg.raw(fileids=&apos;carroll-alice.txt&apos;)sample_text = &apos;We will discuss briefly about the basic syntax,\\ structure and design philosophies. \\ There is a defined hierarchical syntax for Python code which you should remember \\ when writing code! Python is a really powerful programming language!&apos;# Total characters in Alice in Wonderlandprint(len(alice))# First 100 characters in the corpusprint(alice[0:100]) 输出： 144395[Alice&apos;s Adventures in Wonderland by Lewis Carroll 1865]CHAPTER I. Down the Rabbit-HoleAlice was 1.1.1默认分词器–nltk.sent_tokenize#默认分词器default_st = nltk.sent_tokenizealice_sentences = default_st(text=alice)sample_sentences = default_st(text=sample_text)print(&apos;Total sentences in sample_text:&apos;, len(sample_sentences))print(&apos;Sample text sentences :-&apos;)pprint(sample_sentences)print(&apos;\\nTotal sentences in alice:&apos;, len(alice_sentences))print(&apos;First 5 sentences in alice:-&apos;)pprint(alice_sentences[0:5]) 输出： Total sentences in sample_text: 3Sample text sentences :-[&apos;We will discuss briefly about the basic syntax, structure and design &apos; &apos;philosophies.&apos;, &apos;There is a defined hierarchical syntax for Python code which you should &apos; &apos;remember when writing code!&apos;, &apos;Python is a really powerful programming language!&apos;]Total sentences in alice: 1625First 5 sentences in alice:-[&quot;[Alice&apos;s Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.&quot;, &apos;Down the Rabbit-Hole\\n&apos; &apos;\\n&apos; &apos;Alice was beginning to get very tired of sitting by her sister on the\\n&apos; &apos;bank, and of having nothing to do: once or twice she had peeped into the\\n&apos; &apos;book her sister was reading, but it had no pictures or conversations in\\n&apos; &quot;it, &apos;and what is the use of a book,&apos; thought Alice &apos;without pictures or\\n&quot; &quot;conversation?&apos;&quot;, &apos;So she was considering in her own mind (as well as she could, for the\\n&apos; &apos;hot day made her feel very sleepy and stupid), whether the pleasure\\n&apos; &apos;of making a daisy-chain would be worth the trouble of getting up and\\n&apos; &apos;picking the daisies, when suddenly a White Rabbit with pink eyes ran\\n&apos; &apos;close by her.&apos;, &apos;There was nothing so VERY remarkable in that; nor did Alice think it so\\n&apos; &quot;VERY much out of the way to hear the Rabbit say to itself, &apos;Oh dear!&quot;, &apos;Oh dear!&apos;] 德语切分 ## 其他语言句子切分器from nltk.corpus import europarl_raw#德语german_text = europarl_raw.german.raw(fileids=&apos;ep-00-01-17.de&apos;)# 语料中的词数print(len(german_text))# 前100字符print(german_text[0:100]) 输出： 157171 Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit 使用默认分词器切分德语# 默认句子切分器german_sentences_def = default_st(text=german_text, language=&apos;german&apos;)# loading german text tokenizer into a PunktSentenceTokenizer instance german_tokenizer = nltk.data.load(resource_url=&apos;tokenizers/punkt/german.pickle&apos;)german_sentences = german_tokenizer.tokenize(german_text)# verify the type of german_tokenizer# should be PunktSentenceTokenizerprint(type(german_tokenizer))# check if results of both tokenizers match# should be Trueprint(german_sentences_def == german_sentences)# print first 5 sentences of the corpusfor sent in german_sentences[0:5]: print(sent) 输出: &lt;class &apos;nltk.tokenize.punkt.PunktSentenceTokenizer&apos;&gt;True Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .Wie Sie feststellen konnten , ist der gefürchtete &quot; Millenium-Bug &quot; nicht eingetreten .Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken . 1.1.2使用PunktSentenceTokenizer## using PunktSentenceTokenizer for sentence tokenizationpunkt_st = nltk.tokenize.PunktSentenceTokenizer()sample_sentences = punkt_st.tokenize(sample_text)pprint(sample_sentences) 输出: [&apos;We will discuss briefly about the basic syntax, structure and design &apos; &apos;philosophies.&apos;, &apos;There is a defined hierarchical syntax for Python code which you should &apos; &apos;remember when writing code!&apos;, &apos;Python is a really powerful programming language!&apos;] 1.1.3使用RegexpTokenizer#使用正则表达式做句子切分## using RegexpTokenizer for sentence tokenizationSENTENCE_TOKENS_PATTERN = r&apos;(?&lt;!\\w\\.\\w.)(?&lt;![A-Z][a-z]\\.)(?&lt;![A-Z]\\.)(?&lt;=\\.|\\?|\\!)\\s&apos;regex_st = nltk.tokenize.RegexpTokenizer( pattern=SENTENCE_TOKENS_PATTERN, gaps=True)sample_sentences = regex_st.tokenize(sample_text)pprint(sample_sentences) 输出： [&apos;We will discuss briefly about the basic syntax, structure and design &apos; &apos;philosophies.&apos;, &apos; There is a defined hierarchical syntax for Python code which you should &apos; &apos;remember when writing code!&apos;, &apos;Python is a really powerful programming language!&apos;] 1.2词语切分1.2.1默认分词器nltk.word_tokenize## 分词sentence = &quot;The brown fox wasn&apos;t that quick and he couldn&apos;t win the race&quot;# default word tokenizerdefault_wt = nltk.word_tokenizewords = default_wt(sentence)print(words) 输出： [&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;was&apos;, &quot;n&apos;t&quot;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &apos;could&apos;, &quot;n&apos;t&quot;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 1.2.2Treebank分词器# treebank word tokenizertreebank_wt = nltk.TreebankWordTokenizer()words = treebank_wt.tokenize(sentence)print(words) 输出： [&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;was&apos;, &quot;n&apos;t&quot;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &apos;could&apos;, &quot;n&apos;t&quot;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 1.2.3正则分词器RegexpTokenizer# 正则切分TOKEN_PATTERN = r&apos;\\w+&apos; regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN, gaps=False)words = regex_wt.tokenize(sentence)print(words) 输出： [&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;wasn&apos;, &apos;t&apos;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &apos;couldn&apos;, &apos;t&apos;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 设置正则模式: GAP_PATTERN = r&apos;\\s+&apos; regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN, gaps=True)words = regex_wt.tokenize(sentence)print(words) 输出： [&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &quot;wasn&apos;t&quot;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &quot;couldn&apos;t&quot;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 分词索引word_indices = list(regex_wt.span_tokenize(sentence))print(word_indices)print([sentence[start:end] for start, end in word_indices]) 输出：[(0, 3), (4, 9), (10, 13), (14, 20), (21, 25), (26, 31), (32, 35), (36, 38), (39, 47), (48, 51), (52, 55), (56, 60)][&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &quot;wasn&apos;t&quot;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &quot;couldn&apos;t&quot;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 1.2.4WordPunctTokenizer分词器# derived regex tokenizers(派生类执行分词)wordpunkt_wt = nltk.WordPunctTokenizer()words = wordpunkt_wt.tokenize(sentence)print(words) 输出： [&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;wasn&apos;, &quot;&apos;&quot;, &apos;t&apos;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &apos;couldn&apos;, &quot;&apos;&quot;, &apos;t&apos;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 1.2.5WhitespaceTokenizer分词器#WhitespaceTokenizer基于诸如缩进符、换行符及空格的空白字符将句子分割成单词whitespace_wt = nltk.WhitespaceTokenizer()words = whitespace_wt.tokenize(sentence)print(words) 输出： [&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &quot;wasn&apos;t&quot;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &quot;couldn&apos;t&quot;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 2.文本规范化文本规范化定义为这样的一一个过程，它包含一系列步骤， 依次是转换、清洗以及将文本数据标准化成可供NLP、分析系统和应用程序使用的格式。通常，文本切分本身也是文本规范化的一部分。除了文本切分以外，还有各种其他技术，包括文本清洗、大小写转换、词语校正、停用词删除、词干提取和词形还原。文本规范化也常常称为文本清洗或转换。 本节将讨论在文本规范化过程中使用的各种技术。在探索各种技术之前，请使用以下代码段来加载基本的依存关系以及将使用的语料库: import nltkimport reimport stringfrom pprint import pprintcorpus = [&quot;The brown fox wasn&apos;t that quick and he couldn&apos;t win the race&quot;, &quot;Hey that&apos;s a great deal! I just bought a phone for $199&quot;, &quot;@@You&apos;ll (learn) a **lot** in the book. Python is an amazing language!@@&quot;] 2.2文本清洗可以使用nltk中的clean_html()函数，或者BeautifulSoup库来解析HTML数据，还可以使用自定义的逻辑，包括正则表达式、xpath和lxml库来解析XML数据。 2.3文本切分def tokenize_text(text): sentences = nltk.sent_tokenize(text) word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences] return word_tokens token_list = [tokenize_text(text) for text in corpus]print(token_list) 输出： [[[&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;was&apos;, &quot;n&apos;t&quot;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &apos;could&apos;, &quot;n&apos;t&quot;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;]], [[&apos;Hey&apos;, &apos;that&apos;, &quot;&apos;s&quot;, &apos;a&apos;, &apos;great&apos;, &apos;deal&apos;, &apos;!&apos;], [&apos;I&apos;, &apos;just&apos;, &apos;bought&apos;, &apos;a&apos;, &apos;phone&apos;, &apos;for&apos;, &apos;$&apos;, &apos;199&apos;]], [[&apos;@&apos;, &apos;@&apos;, &apos;You&apos;, &quot;&apos;ll&quot;, &apos;(&apos;, &apos;learn&apos;, &apos;)&apos;, &apos;a&apos;, &apos;**lot**&apos;, &apos;in&apos;, &apos;the&apos;, &apos;book&apos;, &apos;.&apos;], [&apos;Python&apos;, &apos;is&apos;, &apos;an&apos;, &apos;amazing&apos;, &apos;language&apos;, &apos;!&apos;], [&apos;@&apos;, &apos;@&apos;]]] 2.4删除特殊字符在分词后删除特殊字符def remove_characters_after_tokenization(tokens): pattern = re.compile(&apos;[&#123;&#125;]&apos;.format(re.escape(string.punctuation))) filtered_tokens = [pattern.sub(&apos;&apos;, token) for token in tokens] return filtered_tokens filtered_list_1 = [[remove_characters_after_tokenization(tokens) for tokens in sentence_tokens] for sentence_tokens in token_list]pprint(filtered_list_1) 输出： [[[&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;was&apos;, &apos;nt&apos;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &apos;could&apos;, &apos;nt&apos;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;]], [[&apos;Hey&apos;, &apos;that&apos;, &apos;s&apos;, &apos;a&apos;, &apos;great&apos;, &apos;deal&apos;, &apos;&apos;], [&apos;I&apos;, &apos;just&apos;, &apos;bought&apos;, &apos;a&apos;, &apos;phone&apos;, &apos;for&apos;, &apos;&apos;, &apos;199&apos;]], [[&apos;&apos;, &apos;&apos;, &apos;You&apos;, &apos;ll&apos;, &apos;&apos;, &apos;learn&apos;, &apos;&apos;, &apos;a&apos;, &apos;lot&apos;, &apos;in&apos;, &apos;the&apos;, &apos;book&apos;, &apos;&apos;], [&apos;Python&apos;, &apos;is&apos;, &apos;an&apos;, &apos;amazing&apos;, &apos;language&apos;, &apos;&apos;], [&apos;&apos;, &apos;&apos;]]] 在分词前删除特殊字符def remove_characters_before_tokenization(sentence, keep_apostrophes=False): sentence = sentence.strip() if keep_apostrophes: PATTERN = r&apos;[?|$|&amp;|*|%|@|(|)|~]&apos; filtered_sentence = re.sub(PATTERN, r&apos;&apos;, sentence) else: PATTERN = r&apos;[^a-zA-Z0-9 ]&apos; filtered_sentence = re.sub(PATTERN, r&apos;&apos;, sentence) return filtered_sentence filtered_list_2 = [remove_characters_before_tokenization(sentence) for sentence in corpus] print(filtered_list_2)cleaned_corpus = [remove_characters_before_tokenization(sentence, keep_apostrophes=True) for sentence in corpus]print(cleaned_corpus) 输出： [&apos;The brown fox wasnt that quick and he couldnt win the race&apos;, &apos;Hey thats a great deal I just bought a phone for 199&apos;, &apos;Youll learn a lot in the book Python is an amazing language&apos;][&quot;The brown fox wasn&apos;t that quick and he couldn&apos;t win the race&quot;, &quot;Hey that&apos;s a great deal! I just bought a phone for 199&quot;, &quot;You&apos;ll learn a lot in the book. Python is an amazing language!&quot;] 2.5扩展缩写词将is’nt 还原为is not等等… from contractions import contractions_dictdef expand_contractions(sentence, contraction_mapping): contractions_pattern = re.compile(&apos;(&#123;&#125;)&apos;.format(&apos;|&apos;.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL) def expand_match(contraction): match = contraction.group(0) first_char = match[0] expanded_contraction = contraction_mapping.get(match)\\ if contraction_mapping.get(match)\\ else contraction_mapping.get(match.lower()) expanded_contraction = first_char+expanded_contraction[1:] return expanded_contraction expanded_sentence = contractions_pattern.sub(expand_match, sentence) return expanded_sentence expanded_corpus = [expand_contractions(sentence, contractions_dict) for sentence in cleaned_corpus] print(expanded_corpus) 输出： [&apos;The brown fox was not that quick and he could not win the race&apos;, &apos;Hey that is a great deal! I just bought a phone for 199&apos;, &apos;You will learn a lot in the book. Python is an amazing language!&apos;] 2.6大小写转换# case conversion print(corpus[0].lower())print(corpus[0].upper()) 输出：the brown fox wasn&apos;t that quick and he couldn&apos;t win the raceTHE BROWN FOX WASN&apos;T THAT QUICK AND HE COULDN&apos;T WIN THE RACE 2.7删除停用词# removing stopwordsdef remove_stopwords(tokens): stopword_list = nltk.corpus.stopwords.words(&apos;english&apos;) filtered_tokens = [token for token in tokens if token not in stopword_list] return filtered_tokens expanded_corpus_tokens = [tokenize_text(text) for text in expanded_corpus] filtered_list_3 = [[remove_stopwords(tokens) for tokens in sentence_tokens] for sentence_tokens in expanded_corpus_tokens]print(filtered_list_3) 输出: [[[&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;quick&apos;, &apos;could&apos;, &apos;win&apos;, &apos;race&apos;]], [[&apos;Hey&apos;, &apos;great&apos;, &apos;deal&apos;, &apos;!&apos;], [&apos;I&apos;, &apos;bought&apos;, &apos;phone&apos;, &apos;199&apos;]], [[&apos;You&apos;, &apos;learn&apos;, &apos;lot&apos;, &apos;book&apos;, &apos;.&apos;], [&apos;Python&apos;, &apos;amazing&apos;, &apos;language&apos;, &apos;!&apos;]]] 2.8词语校正删除重复的字符# removing repeated characterssample_sentence = &apos;My schooool is realllllyyy amaaazingggg&apos;sample_sentence_tokens = tokenize_text(sample_sentence)[0]from nltk.corpus import wordnetdef remove_repeated_characters(tokens): repeat_pattern = re.compile(r&apos;(\\w*)(\\w)\\2(\\w*)&apos;) match_substitution = r&apos;\\1\\2\\3&apos; def replace(old_word): if wordnet.synsets(old_word): return old_word new_word = repeat_pattern.sub(match_substitution, old_word) return replace(new_word) if new_word != old_word else new_word correct_tokens = [replace(word) for word in tokens] return correct_tokensprint(remove_repeated_characters(sample_sentence_tokens)) 输出:[&apos;My&apos;, &apos;school&apos;, &apos;is&apos;, &apos;really&apos;, &apos;amazing&apos;] 2.9词干提取2.9.1Port词干提取器# porter stemmerfrom nltk.stem import PorterStemmerps = PorterStemmer()print(ps.stem(&apos;jumping&apos;), ps.stem(&apos;jumps&apos;), ps.stem(&apos;jumped&apos;))print(ps.stem(&apos;lying&apos;))print(ps.stem(&apos;strange&apos;)) 输出： jump jump jumpliestrang 2.9.2LancasterStemmer词干提取器# lancaster stemmerfrom nltk.stem import LancasterStemmerls = LancasterStemmer()print(ls.stem(&apos;jumping&apos;), ls.stem(&apos;jumps&apos;), ls.stem(&apos;jumped&apos;))print (ls.stem(&apos;lying&apos;))print (ls.stem(&apos;strange&apos;)) 输出： jump jump jumplyingstrange 2.9.3RegexpStemmer正则词干提取器# regex stemmerfrom nltk.stem import RegexpStemmerrs = RegexpStemmer(&apos;ing$|s$|ed$&apos;, min=4)print( rs.stem(&apos;jumping&apos;), rs.stem(&apos;jumps&apos;), rs.stem(&apos;jumped&apos;))print (rs.stem(&apos;lying&apos;))print (rs.stem(&apos;strange&apos;)) 输出： jump jump jumplystrange 2.9.4SnowballStemmer词干提取器# snowball stemmerfrom nltk.stem import SnowballStemmerss = SnowballStemmer(&quot;german&quot;)print (&apos;Supported Languages:&apos;, SnowballStemmer.languages)# autobahnen -&gt; cars# autobahn -&gt; carss.stem(&apos;autobahnen&apos;)# springen -&gt; jumping# spring -&gt; jumpss.stem(&apos;springen&apos;) 输出： Supported Languages: (&apos;arabic&apos;, &apos;danish&apos;, &apos;dutch&apos;, &apos;english&apos;, &apos;finnish&apos;, &apos;french&apos;, &apos;german&apos;, &apos;hungarian&apos;, &apos;italian&apos;, &apos;norwegian&apos;, &apos;porter&apos;, &apos;portuguese&apos;, &apos;romanian&apos;, &apos;russian&apos;, &apos;spanish&apos;, &apos;swedish&apos;)Out[14]:&apos;spring&apos; 2.10词形还原# lemmatizationfrom nltk.stem import WordNetLemmatizerwnl = WordNetLemmatizer()# lemmatize nounsprint( wnl.lemmatize(&apos;cars&apos;, &apos;n&apos;))print (wnl.lemmatize(&apos;men&apos;, &apos;n&apos;))# lemmatize verbsprint (wnl.lemmatize(&apos;running&apos;, &apos;v&apos;))print (wnl.lemmatize(&apos;ate&apos;, &apos;v&apos;))# lemmatize adjectivesprint (wnl.lemmatize(&apos;saddest&apos;, &apos;a&apos;))print (wnl.lemmatize(&apos;fancier&apos;, &apos;a&apos;))# ineffective lemmatizationprint (wnl.lemmatize(&apos;ate&apos;, &apos;n&apos;))print (wnl.lemmatize(&apos;fancier&apos;, &apos;v&apos;)) 输出： carmenruneatsadfancyatefancier","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"文本预处理","slug":"文本预处理","permalink":"http://renxingkai.github.io/tags/文本预处理/"},{"name":"分词","slug":"分词","permalink":"http://renxingkai.github.io/tags/分词/"}],"author":"CinKate"},{"title":"《活着》读后感","slug":"huozheduhougan","date":"2019-03-25T15:01:09.000Z","updated":"2020-05-17T16:11:59.457Z","comments":true,"path":"2019/03/25/huozheduhougan/","link":"","permalink":"http://renxingkai.github.io/2019/03/25/huozheduhougan/","excerpt":"","text":"之前在本科的时候只看过《活着》这部电影（葛优、巩俐主演），电影里已经很惨了，当时虽然看了把两遍电影，但是最大的感受是：虽然这部电影名为“活着”，可却不停地有人离开，也对福贵的悲痛人生感到痛惜，看着自己的亲人，一个个，一个个离开自己，世上也孤零零仅剩自己一个人，那种悲伤之情真的难以承受。 最近，在微信读书上看原著，可能由于年龄的增加，经历的事多了，以及作者的绝妙文笔，感觉读起来的画面感，不亚于看一部精彩的电影，甚至了超过了表演形式。 原著中，福贵的生平更惨，电影中他的孙子还能陪他一起在世上，原著中却真的只有他一个人走到了最后，亲手埋下了自己的儿子、女儿、妻子、女婿、孙子……从最初的悲恸不已，到后来的“心情平淡”，或许生活的残酷已经将这个男人摧残的遍体鳞伤，但他仍然坚强地活着，攒了两年钱却买了一头已然年暮的老牛，因为他知道活着的不易，他珍惜活着，他珍惜每一天。 可惜生活能留给他的，也仅剩了活着。“少年去游荡，中年想掘藏，老年做和尚。”经历了最惨痛的事情，才能看淡生活吧。 真的希望，活着的人能好好活着，每天清晨迎接初日，傍晚送走晚霞，日复一日，如此安好~ 二零一九年三月二十五日 于岳麓山下","categories":[{"name":"读书有感","slug":"读书有感","permalink":"http://renxingkai.github.io/categories/读书有感/"}],"tags":[{"name":"活着","slug":"活着","permalink":"http://renxingkai.github.io/tags/活着/"}],"author":"CinKate"},{"title":"Keras踩坑总结","slug":"kerestricks","date":"2019-03-23T17:04:49.000Z","updated":"2020-05-17T16:11:59.830Z","comments":true,"path":"2019/03/24/kerestricks/","link":"","permalink":"http://renxingkai.github.io/2019/03/24/kerestricks/","excerpt":"","text":"转载自：链接 Keras 是一个用 Python 编写的高级神经网络 API，它能够以 TensorFlow, CNTK, 或者 Theano 作为后端运行。Keras 的开发重点是支持快速的实验。能够以最小的时间把你的想法转换为实验结果，是做好研究的关键。本人是keras的忠实粉丝，可能是因为它实在是太简单易用了，不用多少代码就可以将自己的想法完全实现，但是在使用的过程中还是遇到了不少坑，本文做了一个归纳，供大家参考。 Keras 兼容的 Python 版本: Python 2.7-3.6。 详细教程请参阅Keras官方中文文档 1、Keras输出的loss，val这些值如何保存到文本中去：Keras中的fit函数会返回一个History对象，它的History.history属性会把之前的那些值全保存在里面，如果有验证集的话，也包含了验证集的这些指标变化情况，具体写法： hist=model.fit(train_set_x,train_set_y,batch_size=256,shuffle=True,nb_epoch=nb_epoch,validation_split=0.1)with open(&apos;log_sgd_big_32.txt&apos;,&apos;w&apos;) as f: f.write(str(hist.history)) 我觉得保存之前的loss，val这些值还是比较重要的，在之后的调参过程中有时候还是需要之前loss的结果作为参考的，特别是你自己添加了一些自己的loss的情况下，但是这样的写法会使整个文本的取名比较乱，所以其实可以考虑使用Aetros的插件，Aetros网址，这是一个基于Keras的一个管理工具，可以可视化你的网络结构，中间卷积结果的可视化，以及保存你以往跑的所有结果，还是很方便的，就是有些不稳定，有时候会崩。。。 history对象包含两个重要属性：epoch：训练的轮数history：它是一个字典，包含val_loss,val_acc,loss,acc四个key。 2、关于训练集，验证集和测试集：其实一开始我也没搞清楚这个问题，拿着测试集当验证集用，其实验证集是从训练集中抽取出来用于调参的，而测试集是和训练集无交集的，用于测试所选参数用于该模型的效果的，这个还是不要弄错了。。。在Keras中，验证集的划分只要在fit函数里设置validation_split的值就好了，这个对应了取训练集中百分之几的数据出来当做验证集。但由于shuffle是在validation _split之后执行的，所以如果一开始训练集没有shuffle的话，有可能使验证集全是负样本。测试集的使用只要在evaluate函数里设置就好了。 print model.evaluate（test_set_x，test_set_y ,batch_size=256） 这里注意evaluate和fit函数的默认batch_size都是32，自己记得修改。 总结： 验证集是在fit的时候通过validation_split参数自己从训练集中划分出来的； 测试集需要专门的使用evaluate去进行评价。 3、关于优化方法使用的问题之学习率调整开始总会纠结哪个优化方法好用，但是最好的办法就是试，无数次尝试后不难发现，Sgd的这种学习率非自适应的优化方法，调整学习率和初始化的方法会使它的结果有很大不同，但是由于收敛确实不快，总感觉不是很方便，我觉得之前一直使用Sgd的原因一方面是因为优化方法不多，其次是用Sgd都能有这么好的结果，说明你网络该有多好啊。其他的Adam，Adade，RMSprop结果都差不多，Nadam因为是adam的动量添加的版本，在收敛效果上会更出色。所以如果对结果不满意的话，就把这些方法换着来一遍吧。 （1）方法一：通过LearningRateScheduler实现学习率调整有很多初学者人会好奇怎么使sgd的学习率动态的变化，其实Keras里有个反馈函数叫LearningRateScheduler，具体使用如下： #使学习率指数下降def step_decay(epoch): initial_lrate = 0.01 drop = 0.5 epochs_drop = 10.0 lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop)) return lratelrate = LearningRateScheduler(step_decay)sgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False)model.fit(train_set_x, train_set_y, validation_split=0.1, nb_epoch=200, batch_size=256, callbacks=[lrate]) （2）方式二：最直接的调整学习率方式当然也可以直接在sgd声明函数中修改参数来直接修改学习率，学习率变化如下图： sgd = SGD(lr=learning_rate, decay=learning_rate/nb_epoch, momentum=0.9, nesterov=True) 具体可以参考这篇文章Using Learning Rate Schedules for Deep Learning Models in Python with Keras 除此之外，还有一种学利率调整方式，即 （3）方法三：通过ReduceLROnPlateau调整学习率keras.callbacks.ReduceLROnPlateau(monitor=&apos;val_loss&apos;, factor=0.1, patience=10, verbose=0, mode=&apos;auto&apos;, epsilon=0.0001, cooldown=0, min_lr=0) 当评价指标不在提升时，减少学习率。当学习停滞时，减少2倍或10倍的学习率常常能获得较好的效果。该回调函数检测指标的情况，如果在patience个epoch中看不到模型性能提升，则减少学习率 参数 monitor：被监测的量factor：每次减少学习率的因子，学习率将以lr = lr*factor的形式被减少patience：当patience个epoch过去而模型性能不提升时，学习率减少的动作会被触发mode：‘auto’，‘min’，‘max’之一，在min模式下，如果检测值触发学习率减少。在max模式下，当检测值不再上升则触发学习率减少。epsilon：阈值，用来确定是否进入检测值的“平原区”cooldown：学习率减少后，会经过cooldown个epoch才重新进行正常操作min_lr：学习率的下限 代码示例如下： from keras.callbacks import ReduceLROnPlateaureduce_lr = ReduceLROnPlateau(monitor=&apos;val_loss&apos;, patience=10, mode=&apos;auto&apos;)model.fit(train_x, train_y, batch_size=32, epochs=5, validation_split=0.1, callbacks=[reduce_lr]) 4、如何用 Keras 处理超过内存的数据集？你可以使用 model.train_on_batch(x，y) 和 model.test_on_batch(x，y) 进行批量训练与测试。请参阅 模型文档。 或者，你可以编写一个生成批处理训练数据的生成器，然后使用 model.fit_generator(data_generator，steps_per_epoch，epochs) 方法。 5、Batchnormalization层的放置问题：BN层是真的吊，简直神器，除了会使网络搭建的时间和每个epoch的时间延长一点之外，但是关于这个问题我看到了无数的说法，对于卷积和池化层的放法，又说放中间的，也有说池化层后面的，对于dropout层，有说放在它后面的，也有说放在它前面的，对于这个问题我的说法还是试！虽然麻烦。。。但是DL本来不就是一个偏工程性的学科吗。。。还有一点是需要注意的，就是BN层的参数问题，我一开始也没有注意到，仔细看BN层的参数： keras.layers.normalization.BatchNormalization(epsilon=1e-06, mode=0, axis=-1, momentum=0.9, weights=None, beta_init=&apos;zero&apos;, gamma_init=&apos;one&apos;) 参数mode：整数，指定规范化的模式，取0或10：按特征规范化，输入的各个特征图将独立被规范化。规范化的轴由参数axis指定。注意，如果输入是形如（samples，channels，rows，cols）的4D图像张量，则应设置规范化的轴为1，即沿着通道轴规范化。输入格式是‘tf’同理。1：按样本规范化，该模式默认输入为2D 我们大都使用的都是mode=0也就是按特征规范化，对于放置在卷积和池化之间或之后的4D张量，需要设置axis=1，而Dense层之后的BN层则直接使用默认值就好了。 6、在验证集的误差不再下降时，如何中断训练？你可以使用 EarlyStopping 回调： from keras.callbacks import EarlyStoppingearly_stopping = EarlyStopping(monitor=&apos;val_loss&apos;, patience=2)model.fit(x, y, validation_split=0.2, callbacks=[early_stopping]) 总结：关于callbacks参数的妙用 （1）查询每隔epoch之后的loss和acc （2）通过LearningRateScheduler实现衰减学习率或自定义衰减学习率 （3）通过EarlyStopping实现中断训练 （4）我们还可以自己定义回调函数，所为回调函数其实就是在训练完每一个epoch之后我们希望实现的操作。 7.如何「冻结」网络层？「冻结」一个层意味着将其排除在训练之外，即其权重将永远不会更新。这在微调模型或使用固定的词向量进行文本输入中很有用。有两种方式实现： 方式一：在构造层的时候传递一个bool类型trainable参数，如下： frozen_layer = Dense(32, trainable=False) 您可以将 trainable 参数（布尔值）传递给一个层的构造器，以将该层设置为不可训练的： 方式二：通过层对象的trainable属性去设置，如下： x = Input(shape=(32,))layer = Dense(32) #构造一个层layer.trainable = False #设置层的trainable属性y = layer(x) 注意：可以在实例化之后将网络层的 trainable 属性设置为 True 或 False。为了使之生效，在修改 trainable 属性之后，需要在模型上调用 compile()。及重新编译模型。 8.如何从 Sequential 模型中移除一个层？你可以通过调用模型的 .pop() 来删除 Sequential 模型中最后添加的层： model = Sequential()model.add(Dense(32, activation=&apos;relu&apos;, input_dim=784))model.add(Dense(32, activation=&apos;relu&apos;))print(len(model.layers)) # &quot;2&quot;model.pop()print(len(model.layers)) # &quot;1&quot;","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/tags/深度学习/"},{"name":"keras","slug":"keras","permalink":"http://renxingkai.github.io/tags/keras/"}],"author":"CinKate"},{"title":"BIDAF代码阅读","slug":"bidaf","date":"2019-03-21T11:31:10.000Z","updated":"2020-05-17T16:11:59.217Z","comments":true,"path":"2019/03/21/bidaf/","link":"","permalink":"http://renxingkai.github.io/2019/03/21/bidaf/","excerpt":"","text":"1.assert的用法 ： 主要用于检查条件，不符合就终止程序a=-1#报错assert a&gt;0,&quot;a超出范围&quot;#正常运行assert a&lt;0 2. 打开文件codecs.open()解决不同文件的编码问题，会将文件内容转为unicodeimport codecs, sys# 用codecs提供的open方法来指定打开的文件的语言编码，它会在读 取的时候自动转换为内部unicode bfile = codecs.open( &quot; dddd.txt &quot; , &apos; r &apos; , &quot; big5 &quot; )# bfile = open(&quot;dddd.txt&quot;, &apos;r&apos;) ss = bfile.read()bfile.close()# 输出，这个时候看到的就是转换后的结果。如果使用语言内建的open函数 来打开文件，这里看到的必定是乱码 以下是prepro.py文件的代码阅读与分析：import spacyimport jsonfrom tqdm import tqdmfrom collections import Counterimport randomimport codecsimport numpy as npimport osimport tensorflow as tf#加载模型nlp=spacy.blank(&apos;en&apos;)#对句子进行分词def word_tokenize(sent): doc=nlp(sent) return [token.text for token in doc]#常用的word2idx#此处输出spans形式：[(0, 1), (2, 6), (7, 8), (8, 9), (9, 10), (11, 15), (16, 18)]#意为取出该词当前所在的位置，并且结束长度+当前长度#两者之差即为该单词长度def convert_idx(text,tokens): current=0 spans=[] for token in tokens: current=text.find(token,current) if current&lt;0: print(&apos;Token &#123;&#125; cannot be found!&apos;.format(token)) raise Exception() #[(0, 1), (2, 6), (7, 8), (8, 9), (9, 10), (11, 15), (16, 18)] #取出该词当前所在的位置，并且结束长度+当前长度 #两者之差即为该单词长度 spans.append((current,current+len(token))) current+=len(token) return spans#预处理文件def process_file(filename,data_type=None,word_counter=None,char_counter=None): print(&quot;Generating &#123;&#125; examples...&quot;.format(data_type)) examples = [] eval_examples = &#123;&#125; total=0 with open(filename,&apos;r&apos;) as fh: source=json.load(fh) print(len(source[&apos;data&apos;])) #遍历每篇文章dev有48篇文章 for article in tqdm(source[&quot;data&quot;]): #遍历每篇文章的段落 for para in article[&apos;paragraphs&apos;]: #替换段落中的&apos;&apos;和`` context = para[&quot;context&quot;].replace(&quot;&apos;&apos;&quot;, &apos;&quot; &apos;).replace(&quot;``&quot;, &apos;&quot; &apos;) #并对段落进行分词,分词中还是带了标点和特殊符号，需要后面进行处理 context_tokens=word_tokenize(context) #[&apos;The&apos;, &apos;connection&apos;, &apos;between&apos;, &apos;macroscopic&apos;, &apos;nonconservative&apos;, &apos;forces&apos;, &apos;and&apos;, &apos;microscopic&apos;, &apos;conservative&apos;, &apos;forces&apos;, &apos;is&apos;, &apos;described&apos;, &apos;by&apos;, &apos;detailed&apos;, &apos;treatment&apos;, &apos;with&apos;, &apos;statistical&apos;, &apos;mechanics&apos;, &apos;.&apos;, &apos;In&apos;, &apos;macroscopic&apos;, &apos;closed&apos;, &apos;systems&apos;, &apos;,&apos;, &apos;nonconservative&apos;, &apos;forces&apos;, &apos;act&apos;, &apos;to&apos;, &apos;change&apos;, &apos;the&apos;, &apos;internal&apos;, &apos;energies&apos;, &apos;of&apos;, &apos;the&apos;, &apos;system&apos;, &apos;,&apos;, &apos;and&apos;, &apos;are&apos;, &apos;often&apos;, &apos;associated&apos;, &apos;with&apos;, &apos;the&apos;, &apos;transfer&apos;, &apos;of&apos;, &apos;heat&apos;, &apos;.&apos;, &apos;According&apos;, &apos;to&apos;, &apos;the&apos;, &apos;Second&apos;, &apos;law&apos;, &apos;of&apos;, &apos;thermodynamics&apos;, &apos;,&apos;, &apos;nonconservative&apos;, &apos;forces&apos;, &apos;necessarily&apos;, &apos;result&apos;, &apos;in&apos;, &apos;energy&apos;, &apos;transformations&apos;, &apos;within&apos;, &apos;closed&apos;, &apos;systems&apos;, &apos;from&apos;, &apos;ordered&apos;, &apos;to&apos;, &apos;more&apos;, &apos;random&apos;, &apos;conditions&apos;, &apos;as&apos;, &apos;entropy&apos;, &apos;increases&apos;, &apos;.&apos;] #获取每个单词的字符表示 context_chars = [list(token) for token in context_tokens] #word2idx 每个词开始的位置和结束的位置 spans = convert_idx(context, context_tokens) for token in context_tokens: #这儿加的是每个qas的长度？？ word_counter[token] += len(para[&quot;qas&quot;]) for char in token: #每个单词的字符这儿也加的是每个qas的长度 #Counter(&#123;&apos;e&apos;: 28293, &apos;a&apos;: 19610, &apos;n&apos;: 17317, &apos;t&apos;: 17071, &apos;r&apos;: 15443, &apos;o&apos;: 15358, &apos;i&apos;: # 14669, &apos;s&apos;: 14081, &apos;h&apos;: 11839, &apos;l&apos;: 9031, &apos;d&apos;: 8982, &apos;c&apos;: 6540, &apos;u&apos;: 5885, # &apos;w&apos;: 5806, &apos;f&apos;: 4516, &apos;g&apos;: 4463, &apos;p&apos;: 4372, &apos;m&apos;: 4165, &apos;,&apos;: 3116, &apos;y&apos;: 2842, &apos;b&apos;: 2321, &apos;v&apos;: 2152, # &apos;.&apos;: 2057, &apos;B&apos;: 2052, &apos;S&apos;: 1832, &apos;1&apos;: 1776, &apos;k&apos;: 1553, &apos;0&apos;: 1168, &apos;C&apos;: 1107, &apos;F&apos;: 963, &apos;T&apos;: 876, # &apos;2&apos;: 856, &apos;P&apos;: 836, &apos;I&apos;: 819, &apos;5&apos;: 798, &apos;N&apos;: 766, &apos;L&apos;: 741, &apos;X&apos;: 714, &apos;M&apos;: 672, &apos;4&apos;: 662, &apos;3&apos;: 636, # &apos;A&apos;: 619, &apos;9&apos;: 584, &quot;&apos;&quot;: 552, &apos;-&apos;: 523, &apos;7&apos;: 488, &apos;D&apos;: 470, &apos;–&apos;: 415, &apos;(&apos;: 412, &apos;)&apos;: 412, &apos;8&apos;: 380, # &apos;6&apos;: 371, &apos;V&apos;: 352, &apos;O&apos;: 272, &apos;J&apos;: 268, &apos;j&apos;: 249, &apos;q&apos;: 235, &apos;&quot;&apos;: 222, &apos;G&apos;: 221, &apos;x&apos;: 220, &apos;E&apos;: 177, # &apos;R&apos;: 173, &apos;W&apos;: 168, &apos;K&apos;: 159, &apos;H&apos;: 117, &apos;U&apos;: 108, &apos;z&apos;: 107, &apos;½&apos;: 81, &apos;:&apos;: 81, &apos;;&apos;: 63, &apos;$&apos;: 49, &apos;#&apos;: 30, # &apos;é&apos;: 26, &apos;/&apos;: 21, &apos;Q&apos;: 15&#125;) char_counter[char] += len(para[&quot;qas&quot;]) #遍历qas for qa in para[&quot;qas&quot;]: total += 1 #替换问题&apos;&apos; `` ques = qa[&quot;question&quot;].replace( &quot;&apos;&apos;&quot;, &apos;&quot; &apos;).replace(&quot;``&quot;, &apos;&quot; &apos;) #对问题进行分词 ques_tokens = word_tokenize(ques) #取出问题中的字符 ques_chars = [list(token) for token in ques_tokens] #遍历问题每个词 for token in ques_tokens: #此处真的正确 word_counter[token] += 1 for char in token: char_counter[char] += 1 y1s, y2s = [], [] answer_texts = [] #遍历答案文本 for answer in qa[&quot;answers&quot;]: #答案文本 answer_text = answer[&quot;text&quot;] #开始位置 answer_start = answer[&apos;answer_start&apos;] answer_end = answer_start + len(answer_text) answer_texts.append(answer_text) answer_span = [] #加入答案span answer_span for idx, span in enumerate(spans): if not (answer_end &lt;= span[0] or answer_start &gt;= span[1]): answer_span.append(idx) y1, y2 = answer_span[0], answer_span[-1] y1s.append(y1) y2s.append(y2) example = &#123;&quot;context_tokens&quot;: context_tokens, &quot;context_chars&quot;: context_chars, &quot;ques_tokens&quot;: ques_tokens, &quot;ques_chars&quot;: ques_chars, &quot;y1s&quot;: y1s, &quot;y2s&quot;: y2s, &quot;id&quot;: total&#125; examples.append(example) #未分词结果 eval_examples[str(total)] = &#123; &quot;context&quot;: context, &quot;spans&quot;: spans, &quot;answers&quot;: answer_texts, &quot;uuid&quot;: qa[&quot;id&quot;]&#125; random.shuffle(examples) print(&quot;&#123;&#125; questions in total&quot;.format(len(examples))) return examples, eval_examples#获取词向量def get_embedding(counter, data_type, limit=-1, emb_file=None, size=None, vec_size=None, token2idx_dict=None): print(&quot;Generating &#123;&#125; embedding...&quot;.format(data_type)) embedding_dict=&#123;&#125; #过滤掉低频词，仅取出频率较高的词 filtered_elements=[k for k,v in counter.items() if v&gt;limit] #判断词向量文件是否为空 if emb_file is not None: assert size is not None#如果size为空直接退出程序 assert vec_size is not None#如果vec_size为空直接退出程序 #读取词向量 with codecs.open(emb_file, &quot;r&quot;, encoding=&quot;utf-8&quot;) as fh: # 依次遍历词向量每一行 for line in tqdm(fh, total=size): #分开词和向量 array = line.split() #取出开头的单词 word=&quot;&quot;.join(array[0:-vec_size]) #取出单词对应的词向量 vector=list(map(float,array[-vec_size:])) #词向量的单词在counter单词中，并且 在文本中的单词数目&gt;limit if word in counter and counter[word]&gt;limit: embedding_dict[word]=vector print(&quot;&#123;&#125; / &#123;&#125; tokens have corresponding &#123;&#125; embedding vector&quot;.format( len(embedding_dict), len(filtered_elements), data_type)) #如果词向量文件为空 else: assert vec_size is not None #遍历所有过滤的词 for token in filtered_elements: #对每个单词进行随机初始化向量 embedding_dict[token]=[np.random.normal(scale=0.01) for _ in range(vec_size)] print(&quot;&#123;&#125; tokens have corresponding embedding vector&quot;.format( len(filtered_elements))) #处理OOV词 NULL = &quot;--NULL--&quot; OOV = &quot;--OOV--&quot; #从下标2索引开始，过滤掉NULL和OOV 创建token2_idx_dict token2idx_dict=&#123;token:idx for idx,token in enumerate(embedding_dict.keys(),2)&#125; if token2idx_dict is None else token2idx_dict #NULL OOV 设置token2idx token2idx_dict[NULL] = 0 token2idx_dict[OOV] = 1 #NULL OOV设置embedding_dict embedding_dict[NULL] = [0. for _ in range(vec_size)] embedding_dict[OOV] = [0. for _ in range(vec_size)] #id2embedding 单词id对应的词向量 id2emb_dict=&#123;idx:embedding_dict[token] for token,idx in token2idx_dict.items() &#125; #获取词向量矩阵 emb_mat=[id2emb_dict[idx] for idx in range(id2emb_dict)] #仅返回 词向量矩阵，token2idx_dict return emb_mat, token2idx_dict#构建文本特征question paragraph answer and so ondef build_features(config, examples, data_type, out_file, word2idx_dict, char2idx_dict, is_test=False): #文章长度 para_limit=config.test_para_limit if is_test else config.para_limit #问题长度 ques_limit = config.test_ques_limit if is_test else config.ques_limit #字符限制长度 char_limit = config.char_limit #过滤文章和问题长度函数 def filter_func(example, is_test=False): return len(example[&quot;context_tokens&quot;]) &gt; para_limit or len(example[&quot;ques_tokens&quot;]) &gt; ques_limit print(&quot;Processing &#123;&#125; examples...&quot;.format(data_type)) writer = tf.python_io.TFRecordWriter(out_file) total = 0 total_ = 0 meta = &#123;&#125; #处理文章 for example in tqdm(examples): total_+=1 #过滤长度大于限制值的文章 if filter_func(example, is_test): continue total += 1 #段落ids context_idxs = np.zeros([para_limit], dtype=np.int32) #段落id char对应的矩阵 context_char_idxs = np.zeros([para_limit, char_limit], dtype=np.int32) ##问题ids ques_idxs = np.zeros([ques_limit], dtype=np.int32) ##问题id char对应的矩阵 ques_char_idxs = np.zeros([ques_limit, char_limit], dtype=np.int32) #段落长度 y1 = np.zeros([para_limit], dtype=np.float32) y2 = np.zeros([para_limit], dtype=np.float32) #获取单词 def _get_word(word): for each in (word, word.lower(), word.capitalize(), word.upper()): if each in word2idx_dict: #返回每个单词对应的id return word2idx_dict[each] return 1 #获取字符 def _get_char(char): if char in char2idx_dict: # 返回每个字符对应的id return char2idx_dict[char] return 1 #为每个文章内容获取对应的ids context_tokens为已经分好词的文章 for i, token in enumerate(example[&quot;context_tokens&quot;]): context_idxs[i] = _get_word(token) # 为每个问题内容获取对应的ids ques_tokens为已经分好词的问题 for i, token in enumerate(example[&quot;ques_tokens&quot;]): ques_idxs[i] = _get_word(token) # 为每个文章内容获取对应的chars for i, token in enumerate(example[&quot;context_chars&quot;]): for j, char in enumerate(token): #不能超出char的限制 if j == char_limit: break #赋值char 不够的用0填充 context_char_idxs[i, j] = _get_char(char) # 为每个问题内容获取对应的chars for i, token in enumerate(example[&quot;ques_chars&quot;]): for j, char in enumerate(token): #不能超出char的限制 if j == char_limit: break # 赋值char 不够的用0填充 ques_char_idxs[i, j] = _get_char(char) #开始，结束位置 start, end = example[&quot;y1s&quot;][-1], example[&quot;y2s&quot;][-1] y1[start], y2[end] = 1.0, 1.0 #构建tensorflow 记录 record = tf.train.Example(features=tf.train.Features(feature=&#123; &quot;context_idxs&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_idxs.tostring()])), &quot;ques_idxs&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_idxs.tostring()])), &quot;context_char_idxs&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_char_idxs.tostring()])), &quot;ques_char_idxs&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_char_idxs.tostring()])), &quot;y1&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[y1.tostring()])), &quot;y2&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[y2.tostring()])), &quot;id&quot;: tf.train.Feature(int64_list=tf.train.Int64List(value=[example[&quot;id&quot;]])) &#125;)) writer.write(record.SerializeToString()) print(&quot;Build &#123;&#125; / &#123;&#125; instances of features in total&quot;.format(total, total_)) meta[&quot;total&quot;] = total writer.close() return meta#保存文件def save(filename, obj, message=None): if message is not None: print(&quot;Saving &#123;&#125;...&quot;.format(message)) with open(filename, &quot;w&quot;) as fh: json.dump(obj, fh)# 预处理文件def prepro(config): #单词，字符计数器 word_counter, char_counter = Counter(), Counter() #处理训练集 train_examples, train_eval = process_file( config.train_file, &quot;train&quot;, word_counter, char_counter) #处理验证集 dev_examples, dev_eval = process_file( config.dev_file, &quot;dev&quot;, word_counter, char_counter) #处理测试集 test_examples, test_eval = process_file( config.test_file, &quot;test&quot;, word_counter, char_counter) #词向量文件 word_emb_file = config.fasttext_file if config.fasttext else config.glove_word_file #字符向量文件 char_emb_file = config.glove_char_file if config.pretrained_char else None #字符向量大小 char_emb_size = config.glove_char_size if config.pretrained_char else None #字符向量维度 char_emb_dim = config.glove_dim if config.pretrained_char else config.char_dim #word2idx字典 word2idx_dict = None #如果存在word2idx字典 则直接导入 if os.path.isfile(config.word2idx_file): with open(config.word2idx_file, &quot;r&quot;) as fh: word2idx_dict = json.load(fh) #构建词向量矩阵 word_emb_mat, word2idx_dict = get_embedding(word_counter, &quot;word&quot;, emb_file=word_emb_file, size=config.glove_word_size, vec_size=config.glove_dim, token2idx_dict=word2idx_dict) #构建字符向量矩阵 char2idx_dict = None # 如果存在char2idx字典 则直接导入 if os.path.isfile(config.char2idx_file): with open(config.char2idx_file, &quot;r&quot;) as fh: char2idx_dict = json.load(fh) # 构建字符向量矩阵 char_emb_mat, char2idx_dict = get_embedding( char_counter, &quot;char&quot;, emb_file=char_emb_file, size=char_emb_size, vec_size=char_emb_dim, token2idx_dict=char2idx_dict) #对训练集、验证集、测试集构建特征 build_features(config, train_examples, &quot;train&quot;, config.train_record_file, word2idx_dict, char2idx_dict) dev_meta = build_features(config, dev_examples, &quot;dev&quot;, config.dev_record_file, word2idx_dict, char2idx_dict) test_meta = build_features(config, test_examples, &quot;test&quot;, config.test_record_file, word2idx_dict, char2idx_dict, is_test=True) #对预处理的文件进行保存 save(config.word_emb_file, word_emb_mat, message=&quot;word embedding&quot;) save(config.char_emb_file, char_emb_mat, message=&quot;char embedding&quot;) save(config.train_eval_file, train_eval, message=&quot;train eval&quot;) save(config.dev_eval_file, dev_eval, message=&quot;dev eval&quot;) save(config.test_eval_file, test_eval, message=&quot;test eval&quot;) save(config.dev_meta, dev_meta, message=&quot;dev meta&quot;) save(config.word2idx_file, word2idx_dict, message=&quot;word2idx&quot;) save(config.char2idx_file, char2idx_dict, message=&quot;char2idx&quot;) save(config.test_meta, test_meta, message=&quot;test meta&quot;)","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/categories/深度学习/"}],"tags":[{"name":"阅读理解","slug":"阅读理解","permalink":"http://renxingkai.github.io/tags/阅读理解/"},{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/tags/深度学习/"}],"author":"CinKate"},{"title":"神经网络调参的一些tips","slug":"nntuningparameter","date":"2019-03-19T20:10:50.000Z","updated":"2020-05-17T16:12:00.003Z","comments":true,"path":"2019/03/20/nntuningparameter/","link":"","permalink":"http://renxingkai.github.io/2019/03/20/nntuningparameter/","excerpt":"","text":"参考建议调整超参数思想为控制变量法，并且按照学习率、批处理大小和隐藏层设计的顺序进行。以下是一些建议： 使用一个规模比较小的数据集一般都会把数据集分为训练集、测试集、验证集三份，训练集和验证集也被称为开发数据集，有的数据集不设验证集，是因为数据量小，通常可以用训练集调整超参数。 如果有验证集，验证集的数据不宜过多，因为数据越多，越需要多次迭代才能看到超参数的效果，所需要的时间就越长，在寻找一组最佳参数阶段，需要比较不同参数下损失变化的曲线和精度的值。 调整学习率控制其他超参数不变，改变学习率，比如从0.0001开始，顺序选择0.001、0.01、0.005、0.1和0.5，然后比较在不同的学习率下损失函数的曲线增长或减小的幅度，我们可以找到一个区间，也就是在这个区间内，损失函数的波形是稳定下降的，不会发生振荡，则取这个区间最小的值就可以。 调整batch_size控制其他超参数不变，改变批处理大小，可以依次选择20、50、100、200，然后比较在不同的批处理大小下能使准确率变化最陡的值，准确率变化越陡，证明参数学习收敛越快。 调整隐藏层设计控制其他超参数不变，改变隐藏层的层数或每层神经元的多少，选择能获得最高准确率的值。 超参数的调整主要还是需要自己做大量的实验，得出较好的“经验”，这样调起来会更得心应手一些~","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/categories/深度学习/"}],"tags":[{"name":"神经网络","slug":"神经网络","permalink":"http://renxingkai.github.io/tags/神经网络/"},{"name":"调参","slug":"调参","permalink":"http://renxingkai.github.io/tags/调参/"}],"author":"CinKate"},{"title":"第一篇博文","slug":"firstpage","date":"2019-03-19T14:30:40.000Z","updated":"2020-05-17T16:11:59.814Z","comments":true,"path":"2019/03/19/firstpage/","link":"","permalink":"http://renxingkai.github.io/2019/03/19/firstpage/","excerpt":"今天，是我对github.io的一次尝试，也是我第一次建立自己的博客，希望记录下学习、生活、成长的事迹，多学、多产出，其实更好~","text":"今天，是我对github.io的一次尝试，也是我第一次建立自己的博客，希望记录下学习、生活、成长的事迹，多学、多产出，其实更好~ 分享一首自己很喜欢的词： 扬州慢·淮左名都宋代：姜夔淳熙丙申至日，予过维扬。夜雪初霁，荠麦弥望。入其城，则四顾萧条，寒水自碧，暮色渐起，戍角悲吟。予怀怆然，感慨今昔，因自度此曲。千岩老人以为有“黍离”之悲也。 淮左名都，竹西佳处，解鞍少驻初程。过春风十里。尽荠麦青青。自胡马窥江去后，废池乔木，犹厌言兵。渐黄昏，清角吹寒。都在空城。杜郎俊赏，算而今、重到须惊。纵豆蔻词工，青楼梦好，难赋深情。二十四桥仍在，波心荡、冷月无声。念桥边红药，年年知为谁生。 接下来的日子，锻炼自己的耐力，Always learn from the dalao~坐看天边云卷云舒~","categories":[{"name":"杂谈","slug":"杂谈","permalink":"http://renxingkai.github.io/categories/杂谈/"}],"tags":[{"name":"第一次建博客","slug":"第一次建博客","permalink":"http://renxingkai.github.io/tags/第一次建博客/"},{"name":"That's a great day","slug":"That-s-a-great-day","permalink":"http://renxingkai.github.io/tags/That-s-a-great-day/"}],"author":"CinKate"}],"categories":[{"name":"搜索&推荐","slug":"搜索-推荐","permalink":"http://renxingkai.github.io/categories/搜索-推荐/"},{"name":"机器阅读理解","slug":"机器阅读理解","permalink":"http://renxingkai.github.io/categories/机器阅读理解/"},{"name":"文本生成","slug":"文本生成","permalink":"http://renxingkai.github.io/categories/文本生成/"},{"name":"图神经网络","slug":"图神经网络","permalink":"http://renxingkai.github.io/categories/图神经网络/"},{"name":"机器学习","slug":"机器学习","permalink":"http://renxingkai.github.io/categories/机器学习/"},{"name":"搜索","slug":"搜索","permalink":"http://renxingkai.github.io/categories/搜索/"},{"name":"后台学习","slug":"后台学习","permalink":"http://renxingkai.github.io/categories/后台学习/"},{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"},{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/categories/深度学习/"},{"name":"读书有感","slug":"读书有感","permalink":"http://renxingkai.github.io/categories/读书有感/"},{"name":"杂谈","slug":"杂谈","permalink":"http://renxingkai.github.io/categories/杂谈/"}],"tags":[{"name":"search&recommend","slug":"search-recommend","permalink":"http://renxingkai.github.io/tags/search-recommend/"},{"name":"MRC NQ","slug":"MRC-NQ","permalink":"http://renxingkai.github.io/tags/MRC-NQ/"},{"name":"MTL MRC","slug":"MTL-MRC","permalink":"http://renxingkai.github.io/tags/MTL-MRC/"},{"name":"文本风格迁移","slug":"文本风格迁移","permalink":"http://renxingkai.github.io/tags/文本风格迁移/"},{"name":"GraphEmbedding","slug":"GraphEmbedding","permalink":"http://renxingkai.github.io/tags/GraphEmbedding/"},{"name":"评估指标","slug":"评估指标","permalink":"http://renxingkai.github.io/tags/评估指标/"},{"name":"search","slug":"search","permalink":"http://renxingkai.github.io/tags/search/"},{"name":"git","slug":"git","permalink":"http://renxingkai.github.io/tags/git/"},{"name":"docker","slug":"docker","permalink":"http://renxingkai.github.io/tags/docker/"},{"name":"词向量相关","slug":"词向量相关","permalink":"http://renxingkai.github.io/tags/词向量相关/"},{"name":"阅读理解","slug":"阅读理解","permalink":"http://renxingkai.github.io/tags/阅读理解/"},{"name":"关键词抽取","slug":"关键词抽取","permalink":"http://renxingkai.github.io/tags/关键词抽取/"},{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/tags/深度学习/"},{"name":"文本预处理","slug":"文本预处理","permalink":"http://renxingkai.github.io/tags/文本预处理/"},{"name":"词性标注","slug":"词性标注","permalink":"http://renxingkai.github.io/tags/词性标注/"},{"name":"分词","slug":"分词","permalink":"http://renxingkai.github.io/tags/分词/"},{"name":"活着","slug":"活着","permalink":"http://renxingkai.github.io/tags/活着/"},{"name":"keras","slug":"keras","permalink":"http://renxingkai.github.io/tags/keras/"},{"name":"神经网络","slug":"神经网络","permalink":"http://renxingkai.github.io/tags/神经网络/"},{"name":"调参","slug":"调参","permalink":"http://renxingkai.github.io/tags/调参/"},{"name":"第一次建博客","slug":"第一次建博客","permalink":"http://renxingkai.github.io/tags/第一次建博客/"},{"name":"That's a great day","slug":"That-s-a-great-day","permalink":"http://renxingkai.github.io/tags/That-s-a-great-day/"}]}