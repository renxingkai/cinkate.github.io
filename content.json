{"meta":{"title":"CinKate's Blogs","subtitle":"长笛一声人倚楼~","description":"长笛一声人倚楼~","author":"CinKate","url":"http://renxingkai.github.io","root":"/"},"pages":[{"title":"archives","date":"2019-03-19T17:20:15.000Z","updated":"2020-05-17T16:11:58.945Z","comments":true,"path":"archives/index.html","permalink":"http://renxingkai.github.io/archives/index.html","excerpt":"","text":""},{"title":"","date":"2020-12-03T03:47:53.253Z","updated":"2020-12-03T03:47:53.253Z","comments":true,"path":"about/index.html","permalink":"http://renxingkai.github.io/about/index.html","excerpt":"","text":"个人信息姓名： 任星凯 性别： 男 出生年月： 1996/02 邮箱： renxingkai0101@163.com QQ： 179049243 就业意愿城市： 北京、杭州、西安、郑州 教育经历 2018.09–至今 中南大学，计算机学院，硕士 2019.07–至今 国防科技大学，电子科学学院，联合培养 2014.09–2018.06 中北大学，物联网工程，学士 实习经历 2020.06-2020.07 贝壳找房 NLP算法工程师 2020.07-2020.09 联想研究院 AI Lab NLP算法工程师 比赛获奖 时间 比赛 结果 2020.09 2020年法研杯法律阅读理解竞赛 1st 2020.09 2020 年 CCL 小牛杯幽默情绪识别竞赛 2nd 2020.09 2020 年 CCKS 新冠知识图谱构建与问答评测 2nd 2020.09 2020 年百度人工智能开源大赛 10th/826 2020.08 2020 ICDM Knowledge Graph Contest : Specification 10th 2020.04 中国人工智能大赛·语言与知识技术竞赛（个人赛) 7th/738 2020.02 Kaggle Google QUEST Q&amp;A Labeling 5th/1571 金牌 2020.01 Kaggle TensorFlow 2.0 Question Answering 53rd/1233 银牌 2019.11 汽车论坛消费者用车体验内容的判别与标注 竞赛 5th/837 2019.10 莱斯杯军事阅读理解竞赛 16th/625 2019.10 CCF技术需求匹配竞赛 23rd/862 2019.05 2019年法研杯法律阅读理解竞赛 4th/148 2017.05 蓝桥杯程序设计竞赛(Java) 国家二等奖 2017.04 蓝桥杯程序设计竞赛(Java) 山西省一等奖 2016.11 华北五省计算机应用大赛 国家二等奖 2016.04 蓝桥杯程序设计竞赛(Java) 山西省一等奖 2015.09 全国大学生英语竞赛 三等奖 奖学金 时间 奖项 2020.09 中南大学研究生学业一等奖学金 2019.09 中南大学研究生学业二等奖学金 2018.09 中南大学研究生学业一等奖学金 2017.09 本科生国家奖学金 2017.01 中北大学综合素质一等奖学金 论文 Xingkai Ren, Ronghua Shi, Fangfang Li. Distill BERT to Traditional Models in Chinese Machine Reading Comprehension. AAAI Workshop, 2020"},{"title":"categories","date":"2019-03-19T18:33:09.000Z","updated":"2020-05-17T16:11:59.906Z","comments":true,"path":"categories/index.html","permalink":"http://renxingkai.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-03-19T17:12:23.000Z","updated":"2020-05-17T16:11:59.002Z","comments":true,"path":"tags/index.html","permalink":"http://renxingkai.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Towards Fine-grained Text Sentiment Transfer 笔记","slug":"FGST","date":"2021-01-12T23:53:55.000Z","updated":"2021-01-14T15:44:18.087Z","comments":true,"path":"2021/01/13/FGST/","link":"","permalink":"http://renxingkai.github.io/2021/01/13/FGST/","excerpt":"","text":"论文链接Abstract本文专注于细粒度的情感迁移(FGST)，该任务的目标是在保留原始语义内容的同时，修改输入序列以满足给定的情感强度。不同于传统的情绪传递任务，它只反转文本的情绪极性（正/负），FTST任务需要更细致和细粒度的情绪控制。为了解决这个问题，我们提出了一个新的Seq2SentiSeq模型。具体地，通过高斯核层将数字情感强度值并入解码器以精细地控制输出的情感强度。此外，针对并行数据不足的问题，提出了一种循环强化学习算法来指导模型训练。在这个框架中，精心设计的奖励可以平衡情感转换和内容保存，同时不需要任何ground truth的输出。实验结果表明，该方法在自动评价和人工评价方面均优于现有方法。 1 Introduction为了对文本生成进行更细致、更精确的情感控制，我们转向细粒度文本情感转移（FTST），它在保持语义内容不变的情况下，修改一个序列以满足给定的情感强度。例子如下： FTST任务有两个主要挑战。首先，在生成句子时很难实现情感强度的细粒度控制。以前关于粗粒度文本情感迁移的工作通常对每个情感标签使用单独的解码器（Xu et al.，2018；Zhang et al.，2018b）或将每个情感标签嵌入单独的向量（Fu et al.，2018；Li et al.，2018）。然而，这些方法对于细粒度文本情感迁移是不可行的，因为目标情感强度值是一个实际值，而不是离散的标签。第二，平行数据在实践中是不可用的。换句话说，我们只能访问标有细粒度情绪评级或强度值的语料库。因此，在FTST任务中，我们不能通过ground truth输出来训练生成模型。 针对上述两大挑战，我们提出了两个相应的解决方案。首先，为了控制生成句子的情感强度，我们提出了一种新的情感强度控制序列对序列（Seq2Seq）模型Seq2SentiSeq。它通过高斯核层将情感强度值引入到传统的Seq2Seq模型中。通过这种方式，该模型可以鼓励在解码过程中生成情感强度接近给定强度值的词。其次，由于缺乏并行数据，我们不能直接用极大似然估计（MLE）对模型进行训练。因此，我们提出了一种循环强化学习算法来指导模型训练，而不需要任何并行数据。设计的奖励可以平衡情感迁移和内容保存，同时不需要任何地面真相输出。 本文主要贡献如下： 提出了一种情感强度控制的生成模型Seq2SentiSeq，通过高斯核层引入情感强度值，实现对生成句子的细粒度情感控制。 为了适应非并行数据，我们设计了一种循环强化学习算法CycleRL来指导模型的无监督训练。 实验表明，该方法在自动评价和人工评价两方面均优于现有系统。 5 Related Work近年来，关于无监督情绪传递的文献越来越多。这项任务的目的是翻转一个句子的情感极性，但保持其内容不变，没有平行数据。然而，对于情感的细粒度控制的研究却很少。Liao et al.（2018）通过启发式规则利用伪并行数据，从而将此任务转化为监督任务。然后提出了一种基于变分自动编码器（VAE）的模型，首先将内容因子和源情感因子分离，然后将内容因子和目标情感因子结合起来。然而，伪并行数据的质量并不理想，这严重影响了VAE模型的性能。与之不同的是，我们在训练过程中通过反译（Lample等人，2018b）动态更新伪并行数据（等式12）。 NLP的其他一些任务也对控制文本生成的细粒度属性感兴趣。例如，Zhang et al.（2018a）和Ke et al.（2018）提出控制对话生成的特异性和多样性。我们从这些文章中借鉴了一些想法，但我们的工作动机和提出的模式却与之相去甚远。主要区别在于：（1）由于情感依赖于局部语境，而特异性独立于局部语境，因此我们的模型中有一系列的设计来考虑局部语境（或先前生成的词）st（例如，公式1，公式3）。（2） 由于缺乏并行数据，我们提出了一种循环强化学习算法来训练所提出的模型（第2.3节）。 6 Conclusion我们提出了一个Seq2SentiSeq模型来控制生成句子的细粒度情感强度。为了在没有任何并行数据的情况下训练所提出的模型，我们设计了一种循环强化学习算法。我们将所提出的方法应用于Yelp评论数据集，获得了自动评估和人工评估的最新结果。","categories":[{"name":"文本生成","slug":"文本生成","permalink":"http://renxingkai.github.io/categories/文本生成/"}],"tags":[{"name":"文本风格迁移","slug":"文本风格迁移","permalink":"http://renxingkai.github.io/tags/文本风格迁移/"}],"author":"CinKate"},{"title":"图神经网络相关学习","slug":"graph-embedding","date":"2021-01-09T10:00:56.000Z","updated":"2021-01-11T03:09:40.943Z","comments":true,"path":"2021/01/09/graph-embedding/","link":"","permalink":"http://renxingkai.github.io/2021/01/09/graph-embedding/","excerpt":"","text":"只用于记录学习，图侵删。 图学习算法分类结构图 Deep Walk &amp; Node2vec GraphSAGE","categories":[{"name":"图神经网络","slug":"图神经网络","permalink":"http://renxingkai.github.io/categories/图神经网络/"}],"tags":[{"name":"GraphEmbedding","slug":"GraphEmbedding","permalink":"http://renxingkai.github.io/tags/GraphEmbedding/"}],"author":"CinKate"},{"title":"Style Transformer：Unpaired Text Style Transfer without Disentangled Latent Representation 笔记","slug":"style-transfomer-paper","date":"2021-01-06T21:31:33.000Z","updated":"2021-01-11T03:19:15.909Z","comments":true,"path":"2021/01/07/style-transfomer-paper/","link":"","permalink":"http://renxingkai.github.io/2021/01/07/style-transfomer-paper/","excerpt":"","text":"论文链接Abstract在未配对的文本风格转换中，对潜空间中的文本内容和风格进行消解是一种普遍现象。然而有两个主要的问题存在现在的大多数神经网络中：1)从一个句子的语义中完全剔除风格信息是很困难的；2)基于递归神经网络（RNN）的编码器和解码器，在潜在表示的中介下，不能很好地处理长期依赖的问题，导致非文体语义内容保存不好。在本文中，我们提出了Style Transfomer，它不需要假设源句的潜在表示，而是在Transformer中配置注意机制，以实现更好的风格转换和内容保留。 1.Introduction语篇风格转换的任务是改变语篇的风格属性（如情感），同时在语境中保留与风格无关的内容。由于文本风格定义比较模糊，构建相同内容但文本风格不一样的句子对是比较困难的。因此文本风格迁移研究主要关注在未成对的文本句子。 神经网络成为了主流的文本风格迁移工具，主要使用Seq2Seq结构，encoder负责编码句子到一个vector，decoder生成不同风格的文本但是保留了原来的内容。这些方法都在专注于如何分清潜在语义空间中的content和style。由于没有成对的句子，对抗loss被使用到潜在空间去促进潜在空间中的风格信息。 现在面临的一些困难： 在潜在语义空间中，区分content和style是困难的， 区分content和style是没有必要的，一个好的风格迁移器，其实不用区分content和style，最好进行直接改写并输出。 受限的vector表示，会使得较长句子编码损失信息 为了理清潜在空间中的content和style，所有的现存方法都假设输入句子被编码到一个固定大小的潜在空间。这些方法因此不能使用attention机制去加强输入句子中的信息。 大多数模型使用RNN去作为encoder和decoder,对于较长句子编码效果并不好。 本文使用Transfomer作为基础模块。贡献如下： 我们提出一个创新的训练算法，不假设需要理清输入句子的content和style，因此，模型可以使用attention机制去提升性能。 首次使用Transfomer在风格迁移任务中。 在两个数据集中我们模型效果都很好，特别地，在content保留中，Style Transfomer取得了较大性能的提升。 2.Related Work3.Style Transformer3.1 Problem Formalization假设有K种不同的数据集Di,可以定义K种不同的风格，文本风格迁移目标是给一个随机的自然语言句子x和一个想要的风格s，需要让机器重写句子x成为新的句子y,句子y包含新的风格s,同时又保留了x的文本信息(语义)。 3.2 Model Overview本文目标是学习一个映射函数f(x,s) ，x是一个自然语言句子，s是一个风格控制变量。函数f的输出是迁移之后的句子y。 3.3 Style Transformer NetworkTransformer是一个标准的encoder-decoder结构，对于一个输入句子x=(x_1,x_2,...,x_n) 编码器encoder将输入映射到连续表示z=(z_1,z_2,...,z_n) 解码器根据条件概率和自回归的方式生成输出句子：p_\\theta(y|x)=\\prod_&#123;t=1&#125;^m p_\\theta(y_t|z,y_1,...,y_&#123;t-1&#125;) 在每一时间步t，下一个token通过softmax分类器来计算：p_\\theta(y_t|z,y_1,...,y_&#123;t-1&#125;)=softmax(o_t) ot是Transfomer decoder的输出。 为了能够控制风格，我们在Transformer Encoder中加入了一个额外的风格向量:Encoder(x,s,\\theta_E) 因此，网络的输出变为:p_\\theta(y|x,s)=\\prod_&#123;t=1&#125;^m p_\\theta(y_t|z,y_1,...,y_&#123;t-1&#125;) 我们定义网络的预测输出为f_(x,x) 3.4 Discriminator Network因为大部分情况下缺少并行语料来进行有监督训练，我们提出了使用判别网络去从非平行语料中学习监督信息。我们构建了两种方法 为了保留内容信息，当我们将迁移语句$\\hat{y}=f_\\theta(x,\\hat{s})$送到带有原始标签s的Style Transfromer中时，我们训练网络重构原始输入语句x 对于风格控制，我们训练了一个判别网络去更好地控制被生成的句子。 判别网络是另一个TRM encoder,学习区分不同句子的风格。Style TRM网络接收风格监督信息从这个判别网络中。我们实验了两种判别网络： Conditional Discriminator 一个句子x和一个风格s被输入到判别网络$d_{\\phi}(x,s)$判别器需要输出句子x是否含有相关的风格。在判别器训练阶段，数据集x中的真是句子和重构句子$y=f_{\\theta}(x,s)$被标记为正样本，迁移的句子$\\hat{y}=f_\\theta(x,\\hat{s})$被标记为负样本。在Style TRM训练阶段，网络$f_\\theta$被训练最大化正样本概率当输入$\\hat{y}=f_\\theta(x,\\hat{s})$和$\\hat s$到判别器中。 Multi-class Discriminator 仅有一个句子x被输入到判别器$d_{\\phi}(x)$，判别器的目标是去回答此句话的风格。更具体地说，判别器是具有K+1类的分类器。前K类代表K种不同的风格，最后一类代表$f_\\theta(x,\\hat s)$生成的数据，也常被称为假样本。在判别器训练阶段，我们标记真实句子x和重构句子$y=f_{\\theta}(x,s)$为相关的风格，至于被迁移的句子$\\hat{y}=f_\\theta(x,\\hat{s})$被标记为类别0。在Style TRM学习阶段，我们训练网络$f_\\theta(x,\\hat{s})$最大化代表风格$\\hat s$的概率。 3.5 Learning Algorithm该模型的训练算法可分为两部分：判别器学习和风格变换网络学习。架构图如图2所示 3.5.1 Discriminator Learning我们训练我们的判别器来区分真实句字x和重构句子$y=f_{\\theta}(x,s)$与迁移句子$\\hat{y}=f_\\theta(x,\\hat{s})$。损失函数为交叉熵算法细节： 3.5.2 Style Transformer Learning根据$s=\\hat s$和$s!=\\hat s$分为两种情况。 Self Reconstruction 对于$s=\\hat s$，输入句子x和风格s都是来于相同数据集，可以直接训练Style TRM，去重构输入句子，通过最小化-log likehood: L_&#123;self&#125;(\\theta)=-p_&#123;\\theta&#125;(y=x|x,s) 对于$s!=\\hat s$，没有监督数据，不能直接获得loss，因此提出两种不同的训练loss： Cycle Reconstruction为了鼓励生成的句子保留输入句子x中的信息，我们将生成的句$\\hat{y}=f_\\theta(x,\\hat{s})$以x的风格输入给Style TRM，并训练我们的网络通过最小化负对数似然来重构原始输入句子： Style Controlling如果我们只训练我们的Style TRM从转移句$\\hat{y}=f_\\theta(x,\\hat{s})$重构输入句x，网络只能学习将输入复制到输出。为了处理这个退化问题，我们进一步为生成的句子添加了风格控制损失。即将网络生成的句子$\\hat y$输入到判别器中，以最大限度地提高$\\hat s$的概率。对于conditional discriminator，Style TRM的目标是在使用样式标签$\\hat s$输入到判别器时最小化类1的负对数似然性： 在multi-class discriminator的情况下，Style TRM以最小化对应类型的风格$\\hat s$的负对数似然： 组合loss之后，Style TRM学习过程如下： 3.5.3 Summarization and Discussion与GANs的训练过程类似（Goodfellow et al.，2014），在每个训练迭代中，我们首先执行$n_d$步鉴别器学习以获得更好的鉴别器，然后训练我们的Style TRM$n_f$步以提高其性能。算法3对训练过程进行了总结 由于自然语言的离散性，对于生成的句子$\\hat{y}=f_\\theta(x,\\hat{s})$，我们不能通过离散样本直接传播来自判别器的梯度。在我们的实验中，我们还观察到Gumbel-Softmax技巧会减慢模型的收敛速度，并且没有给模型带来太多的性能改进。基于以上原因，我们将fθ生成的softmax分布视为一个“软”生成句子，并将该分布输入给下游网络，以保持整个训练过程的连续性。当使用这种近似时，我们也将我们的解码器网络从贪婪解码切换到连续解码。也就是说，在每一个时间步，我们将整个softmax分布（等式（2））输入到网络，而不是将在先前预测步骤中具有最大概率的令牌馈送到网络。解码器利用这个分布，从输入的嵌入矩阵计算加权平均嵌入。 4 Experiment4.1 DatasetsYelp Review Dataset (Yelp) Yelp数据集是由Yelp数据集挑战提供的，由带有情绪标签（负面或正面）的餐馆和商业评论组成。在之前的工作之后，我们使用了Li等人（2018）提供的数据集。此外，它还为测试集提供人类参考句。 IMDb Movie Review Dataset (IMDb) IMDb数据集由在线用户撰写的影评组成。为了获得高质量的数据集，我们使用了Maas等人（2011）提供的极性电影评论。基于此数据集，我们通过以下步骤构建了一个高度极性的句子级风格转换数据集：1）在原始训练集上微调一个BERT（Devlin et al.，2018）分类器，在测试集上达到95%的准确率；2）将原始数据集中的每个评论分成几个句子；3） 通过我们微调的BERT分类器过滤掉置信阈值低于0.9的句子；4）删除不常见单词的句子。最后，这个数据集包含366K、4k和2k个句子，分别用于训练、验证和测试. 4.2 Evaluation目标转移句应该是一个流利的、内容完整的、具有目标风格的句子。为了评估不同模型的表现，我们在前人工作的基础上，比较了生成样本的三个不同维度：1）风格控制，2）内容保留(BLEU)，3）流畅性(perplexity)。 4.3 Training Details在所有的实验中，对于编码器、解码器和鉴别器，我们都使用了四层TRM，每层有四个注意头。Transformer中的隐藏大小、嵌入大小和位置编码大小都是256维。另一个包含256个隐藏单元的嵌入矩阵用来表示不同的风格，作为输入语句的额外标记输入到编码器中。并且位置编码不用于样式标记。对于鉴别器，类似于Radford et al.（2018）和Devlin et al.（2018），我们进一步在输入中添加标记，并将相应位置的输出向量馈送到表示鉴别器输出的softmax分类器中。 4.4 Experiment Results实验结果一些迁移例子 5.Conclusions and Future Work本文我们提出了Style Transfomer，实验结果在两个数据集中展示出了我们模型比之前的SOTA模型有竞争力的结果，特别地是，由于我们提出的方法没有假设一个分离的潜在表示来操纵句子的风格，我们的模型可以在两个数据集上得到更好的内容保留。","categories":[{"name":"文本生成","slug":"文本生成","permalink":"http://renxingkai.github.io/categories/文本生成/"}],"tags":[{"name":"文本风格迁移","slug":"文本风格迁移","permalink":"http://renxingkai.github.io/tags/文本风格迁移/"}],"author":"CinKate"},{"title":"常见的评估指标","slug":"metrics","date":"2021-01-06T19:29:52.000Z","updated":"2021-01-11T03:10:07.812Z","comments":true,"path":"2021/01/07/metrics/","link":"","permalink":"http://renxingkai.github.io/2021/01/07/metrics/","excerpt":"","text":"准确率，精确率，F1值，ROC，AUC，P-R，mAP(图来自网络，侵删)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://renxingkai.github.io/categories/机器学习/"}],"tags":[{"name":"评估指标","slug":"评估指标","permalink":"http://renxingkai.github.io/tags/评估指标/"}],"author":"CinKate"},{"title":"搜索中的NLP技术","slug":"searchandnlp","date":"2020-12-26T10:25:23.000Z","updated":"2021-01-06T11:33:26.871Z","comments":true,"path":"2020/12/26/searchandnlp/","link":"","permalink":"http://renxingkai.github.io/2020/12/26/searchandnlp/","excerpt":"","text":"搜索技术除了涉及基础的搜索算法，也涉及到很多NLP技术，本文转载于，只是做个学习记录，侵删。 推荐系统被捧为目前算法领域的主流，推荐系统不需要用户主动进行操作就能获取自己喜欢的东西，但是实际上，搜索系统在很长一段时间占据了重要位置，大到百度的大搜，小到音乐、视频、电商、应用商店等，都有各种各样的搜索引擎，这些搜索搜索能更为精准直接的满足用户需求，即使是推荐系统如日中天，目前也仍会有搜索的一席之地。今天我来为大家介绍，搜索系统中涉及的算法问题，也让大家了解，搜索中需要什么算法。 数据预处理模块这个很好理解吧，用户query进来，一般要做如下处理: 大小写转化。 删除标点符号（当然有的分析会保留标点，但是建议在这个场景下还是去掉更好）。 繁体转简体。 数字处理，转中文，转阿拉伯数字，甚至罗马字等，部分情况要删除数字。 长度截断。（总不能让用户输入一本西游记，然后你还在整吧？） 理解模块query理解其实是一个非常重要的模块，好的query理解能为后续的工作提供很大的支持。这部分往往不直接应对下游，而是按需使用，有点像辅助吧。 分词。基操。 关键词抽取。会涉及丢词、排序等功能。 命名实体识别。一方面协助用户识别，另一方面可能会涉及数据库查询匹配的内容。在垂搜中比较常见，大搜也有但是相比之下没那么精准和常见。 紧密度分析，避免由于切词出现错误导致词义偏移的问题，这个其实并不少见，尤其是在人名上，这个是别的精准度会很低，近期的如“武大靖”，会被分为“武大 靖”，“曾舜晞”直接被分为了3个字，挺头疼的。 非法信息过滤。例如黄色、暴力血腥、反动等。 改写模块改写模块其实非常关键，这是连接用户query和数据库底层数据的桥梁，数据库的存储有特定的形式，但是用户不会按照你的底层数据结构去写，例如，用户不见得会输入和平精英，而是吃鸡，数据库里可不见得会存吃鸡对吧，所以这块很重要。 同义词改写。上面的吃鸡就要改写为和平精英，这个需要通过同义词挖掘等方式去构造词典实现。 拼音改写。数据库是罗密欧与朱丽叶，但是用户输入的是罗密欧与朱莉业，拼音改写其实颇为常见，用户经常由于找不到需要的结果或者不知道应该需要哪个，于是直接输入后开始搜索。 前缀补全。非常常见，用户输入射雕，射雕英雄传就要出了，这个一般的方法也是构造词典，另外有一个很重要的需要了解的就是前缀树，这个能让你查询的时间非常低的水平（只和query长度本身有关）。 丢词和留词。结合上述关键词提取和命名实体识别完成，有些不必要的词汇需要被删除，例如“李小璐到底怎么了”，整个句子只有李小璐是关键词，其他词如果也通过and逻辑召回，就没有信息召回了，这时候其实可以直接删除或者将降级到or逻辑。留词和丢词相反，丢词如果是做减法，留词就是做加法。 近义词召回。这个召回不是从数据库中召回，而是召回近义词，具体的方法是通过embedding方法转化词汇，然后通过ball tree、simhash的方式召回与之意思相近的词汇，该模式虽然比较激进，但是能一定程度增加召回，有一定效果。 意图识别。与其说是意图，我更喜欢理解为这是直接针对底层数据结构产生的需要解决的问题，query这是一条，但是数据库有好几个，我们要去哪个数据库搜，这是需要识别的，而这个数据库的设计往往和品类、意图有关，找酒店、找景点都是不同的，所以此时就要进行意图识别，一般地方法是抽象为文本分类，但是很多时候语义本身是无法体现出真实意图的，例如少年的你，语义上其实很难分析出，有时候更复杂的会夹带一些实体识别、词性、词典之类的信息。 召回模块结合命名实体识别、改写结果，然后开始召回，模式比较多，包括但不限于下面形式： Elastic Search，著名的搜索平台ES，有些时候甚至简单的搜索平台直接用它管整个搜索引擎但是要精做，就只能把它当做底层的数据库和搜索平台。 关系数据库。MySQL之类的，但是在我的理解速度和并发性都不是特别好。 Redis。KV形式的数据库，速度很快，但是Key匹配必须是精确匹配，这需要改写模块具有很强的能力。 说白了，就是把用上面流程处理过的query放到数据库里面查，这个其实就是召回。 排序模块内容是召回回来了，其实怎么展现给用户呢？这里是需要深度挖掘用户的实际需求的，很多时候甚至需要做个性化，但是更多时候是我们能够得到的只有短短的一句用户query，那么，我们就要好好利用好这个query，来做好排序，让用户喜欢的（当然，还有广告商喜欢的哈哈哈）东西放在前面，实际上就是一个学习排序的问题了，这个我之前也做过一些简单的介绍，那么在特征层面，可以考虑如下的信息： query层面： 本身的embedding，后续迭代可以走elmo后逐渐形成的pre-train的模式。 词性、实体、词权重、offset等序列标注得到的结果。 document层面。 如果文档本身有文本，最好是标题类的，也把embeding引入，和query层面那种一样。如果只有query文本和document文本，其实就是个文本相似度模型了对吧。 综合层面： query和document的ctr、cqr、bm25，句向量余弦相似度等匹配信息。 其他层面： 意图识别结果。 模型上，DSSM系列似乎是比较流行的方法，但是提取一些特定的特征，有点的时候简单的LR、XGBOOST就能一定程度把问题解决好了。 而在排序模块中，还会涉及一些硬规则等。 搜索引导模块query suggestion是一个上述搜索过程非常类似的模块，只不过处理的结果大部分是放在离线，在线是指一个查字典的过程，那么离线部分，其实是做了一整个搜索过程的：预处理、query理解、改写、召回、排序。 预处理：和上面的预处理类似，不赘述。 query理解，和上述内容类似，但是有的时候会简化，直接进入改写模块。 改写模块，会进行容忍度更高的前缀匹配，去找回一些用户可能会喜欢的内容，这时候往往还会带上统计特征，从整体层面上看用户喜欢的内容，毕竟不同时间用户喜欢的可能不同，例如对热点新闻的偏好。 召回和排序模块。召回模块不是召回数据了，而是召回相似的doc信息，也可以是相似的历史用户query，尤其是后者，如果是后者，则要确认历史用户query是有结果的。 其他辅助模块显然，整个搜索系统远远不止这些内容，在算法视角下，其实还需要很多辅助模块协助我们进行算法开发。 日志模块。无论是线上的运行日志，还是线下的模型实验和离线模型运行，都需要日志协助，对于线上运行和离线模型运行而言，出现错误可以方便追溯，找到ERROR出现的时间和具体问题，而线下的模型试验能方便我们计算运行时间、找到bug，而非每次训练模型的时候功亏一篑才来找问题。 实验模块。由于算法策略存在很多不确定性，无论是算法内存占用和时间，还是算法实际结果，因此需要利用AB实验的方式来验证，快速进行分析，对有问题的算法及时下线检查原因。 数据分析和报表模块。结合实验模块，需要对日常用户数据进行分析，这个比较好理解，不赘述。 特征生产模块。特征生产涉及两块，一个是线上的实时计算，对于一些实时数据，是需要即时生成的，例如微博热搜里面就需要一些诸如5分钟搜索量之类的特征，这个只能从实时计算模块中获取；另一个是离线模块，为了进行离线模型训练，需要定时将生肉（原始数据）转化为特征，方便进行下一步计算，如果这个能变成日常任务，那开发人员就不需要临时造轮子执行，还要长时间等待。 定时任务模块。很多任务其实是定时开始的，报表生成、特征生产，甚至是一些实时任务（说白了就是短时间内的计算），因此有定时任务模块去管理这些定时任务，将会非常方便。 模型管理模块。首先一个项目中可能存在大量模型，然后因为各个模型训练需要花费大量资源，还要结合AB试验，另外还有模型的更新和保存机制（防止模型加载失败导致的无模型可用），因此需要构建模型模块统一管理模型，这个不是每个部门都有，或者说每个模型都是各自管理，但是有一个比较好管理模型平台是非常高效的。 数据平台。额，简单地说就是写SQL的地方，但是也有类似ETL之类的内容，和特征生产模块很接近。 小结搜索作为一个完整地系统，难度甚至比搜索要困难，在更多的场景下，搜索系统只能针对短短几个字的query进行分析，从而在海量数据中找到用户需要的东西，而由于是用户主动输入的，所以用户的预期非常高，但因为是用户主动的行为，所以不会太过复杂，甚至可能会有各种各样的歧义，只有详细分析和挖掘才能得到，因此在我看来，搜索这个事情非常具有挑战性，即使这个东西已经发展多年，年龄比推荐系统更大，还是很多可挖掘的地方。","categories":[{"name":"搜索","slug":"搜索","permalink":"http://renxingkai.github.io/categories/搜索/"}],"tags":[{"name":"search","slug":"search","permalink":"http://renxingkai.github.io/tags/search/"}],"author":"CinKate"},{"title":"Git常用复习","slug":"git-learn","date":"2020-12-23T20:55:42.000Z","updated":"2020-12-23T12:57:43.570Z","comments":true,"path":"2020/12/24/git-learn/","link":"","permalink":"http://renxingkai.github.io/2020/12/24/git-learn/","excerpt":"","text":"版本控制的分类 本地版本控制，如RCS。最常见，最简单的版本控制方法。 集中版本控制，如SVN。所有的版本数据都保存在服务器上，协同开发者从服务器上同步更新或上传自己的修改。必须联网。。。 分布式版本控制，如Git。所有版本信息仓库全部同步在本地的每个用户中，可以在本地查看所有版本历史，可以在本地离线提交，有网了之后push到服务器。但是每个人都有全部代码，存在安全隐患。 Git原理 git add files 将本地文件添加到暂存区stage，git commit 将暂存区文件提交到本地的git仓库，git push将本地仓库文件提交到远程仓库，如下图左边，右边对应反向操作 Workspace：工作目录，平时存放代码的位置 Index/Stage：暂存区，用于临时存放你的改动，事实上只是一个文件，保存即将提交到文件列表信息 Repository：仓库区(或本地仓库)，就是安全存放数据的位置，这里面有你提交到所有版本的数据。其中HEAD指向最新放入仓库的版本 Remote：远程仓库，托管代码的服务器，可以简单地认为是你项目组中的一台电脑用于远程数据交换 Git提交时忽略的文件在主目录下建立”.gitinore”文件，此文件以下规则常用#为注释*.txt #忽略所有.txt结尾的文件,这样的话，上传就不会被选中!lib.txt #但lib.txt除外/temp #仅忽略项目根目录下的TODO文件，不包括其他目录tempbuild/ #忽略build/目录下的所有文件doc/*.txt #会忽略 doc/notes.txt 但不包括 doc/server/arch.txt Git分支#列出所有本地分支git branch#列出所有远程分支git branch -r#新建一个分支git branch [branch-name]#虽然新建了但是默认还是在master分支下显示#新建一个分支并切换到该分支git checkout -b [branch-name]#合并指定分支到当前分支git merge [branch]#删除分支git branch -d [branch-name]#删除远程分支git push origin --delete [branch-name]git branch -dr [remote/branch] 多个分支如果并行执行，就会导致代码不会冲突，会同时存在多个版本！ web-api -A组开发 web-admin -B组开发 B会调用A web-app -C组开发 C会调用B和A的代码 如果冲突了要进行协商！ 如果同一个文件在合并分支时都被修改了，则会引起冲突：解决的办法是我们可以修改冲突文件后重新提交！选择要保留他的代码还是你的代码！ master主分支应该非常稳定，用来发布新版本，一般情况下不允许在上面工作，工作一般情况下在新建的dev分支上工作，工作完后，比如上要发布，或者说dev分值代码稳定后可以合并到主分支master上来。","categories":[{"name":"后台学习","slug":"后台学习","permalink":"http://renxingkai.github.io/categories/后台学习/"}],"tags":[{"name":"git","slug":"git","permalink":"http://renxingkai.github.io/tags/git/"}],"author":"CinKate"},{"title":"Docker基础学习","slug":"docker-learning","date":"2020-12-01T11:07:19.000Z","updated":"2020-12-22T09:10:06.146Z","comments":true,"path":"2020/12/01/docker-learning/","link":"","permalink":"http://renxingkai.github.io/2020/12/01/docker-learning/","excerpt":"","text":"Docker的常用命令帮助命令docker version #显示docker的版本信息docker info #显示docker的系统信息，包括镜像和容器的数量docker 命令 --help #帮助命令 帮助文档的地址：https://docs.docker.com/engine/reference/commandline/docker/ 镜像命令docker images #查看所有本地主机上的镜像docker images #查看所有本地主机上的镜像[root@VM-0-15-centos ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEhello-world latest bf756fb1ae65 11 months ago 13.3kB#解释REPOSITORY #镜像的仓库TAG #镜像的标签IMAGE ID #镜像的IDCREATED #镜像的创建时间SIZE #镜像的大小#可选项Options: -a, --all #列出所有镜像 --digests Show digests -q, --quiet #仅显示镜像的ID docker search #搜索镜像[root@VM-0-15-centos ~]# docker search tensorflowNAME DESCRIPTION STARS OFFICIAL AUTOMATEDtensorflow/tensorflow Official Docker images for the machine learn… 1799 jupyter/tensorflow-notebook Jupyter Notebook Scientific Python Stack w/ … 248 tensorflow/serving Official images for TensorFlow Serving (http… 102 rocm/tensorflow Tensorflow with ROCm backend support 58 xblaster/tensorflow-jupyter Dockerized Jupyter with tensorflow 54 [OK]floydhub/tensorflow tensorflow 26 [OK]bitnami/tensorflow-serving Bitnami Docker Image for TensorFlow Serving 14 [OK]opensciencegrid/tensorflow-gpu TensorFlow GPU set up for OSG 12 docker pull #下载镜像#docker pull mysql 镜像名[:tag][root@VM-0-15-centos ~]# docker pull mysqlUsing default tag: latest #不写tag，默认latestlatest: Pulling from library/mysql852e50cd189d: Pull complete #分层下载,docker 镜像的核心 联合文件系统29969ddb0ffb: Pull complete a43f41a44c48: Pull complete 5cdd802543a3: Pull complete b79b040de953: Pull complete 938c64119969: Pull complete 7689ec51a0d9: Pull complete a880ba7c411f: Pull complete 984f656ec6ca: Pull complete 9f497bce458a: Pull complete b9940f97694b: Pull complete 2f069358dc96: Pull complete Digest: sha256:4bb2e81a40e9d0d59bd8e3dc2ba5e1f2197696f6de39a91e90798dd27299b093 #签名Status: Downloaded newer image for mysql:latestdocker.io/library/mysql:latest #真实地址#指定版本docker pull mysql:5.7[root@VM-0-15-centos ~]# docker pull mysql:5.75.7: Pulling from library/mysql852e50cd189d: Already exists 29969ddb0ffb: Already exists a43f41a44c48: Already exists 5cdd802543a3: Already exists b79b040de953: Already exists 938c64119969: Already exists 7689ec51a0d9: Already exists 36bd6224d58f: Pull complete cab9d3fa4c8c: Pull complete 1b741e1c47de: Pull complete aac9d11987ac: Pull complete Digest: sha256:8e2004f9fe43df06c3030090f593021a5f283d028b5ed5765cc24236c2c4d88eStatus: Downloaded newer image for mysql:5.7docker.io/library/mysql:5.7 docker rmi #删除镜像#按docker id镜像删除[root@VM-0-15-centos ~]# docker rmi -f ae0658fdbad5 Untagged: mysql:5.7Untagged: mysql@sha256:8e2004f9fe43df06c3030090f593021a5f283d028b5ed5765cc24236c2c4d88eDeleted: sha256:ae0658fdbad5fb1c9413c998d8a573eeb5d16713463992005029c591e6400d02Deleted: sha256:a2cf831f4221764f4484ff0df961b54f1f949ed78220de1b24046843c55ac40fDeleted: sha256:0a7adcc95a91b1ec2beab283e0bfce5ccd6df590bd5a5e894954fcf27571e7f5Deleted: sha256:0fae465cbacf7c99aa90bc286689bc88a35d086f37fd931e03966d312d5dfb10Deleted: sha256:23af125b9e54a94c064bdfacc2414b1c8fba288aff48308e8117beb08b38cb19#递归删除所有的镜像[root@VM-0-15-centos ~]# docker rmi -f $(docker images -aq)Untagged: mysql:latestUntagged: mysql@sha256:4bb2e81a40e9d0d59bd8e3dc2ba5e1f2197696f6de39a91e90798dd27299b093Deleted: sha256:dd7265748b5dc3211208fb9aa232cef8d3fefd5d9a2a80d87407b8ea649e571cDeleted: sha256:aac9a624212bf416c3b41a62212caf12ed3c578d6b17b0f15be13a7dab56628dDeleted: sha256:1bf3ce09276e9e128108b166121e5d04abd16e7de7473b53b3018c6db0cf23ffDeleted: sha256:24c6444cea460c3cc2f4e0385e3e97819a0672a54a361921f95d4582583abd59Deleted: sha256:77585ebe3eaa035694084b3c5937fe82b8972aae1e6c6070fc4d7bc391d10928Deleted: sha256:1cfd539163ceb17f7bb85a0da968714fe9258b75dbf73f5ad45392a45cfd34b7Deleted: sha256:c37f414ac8d2b5e5d39f159a6dffd30b279c1268f30186cee5da721e451726eaDeleted: sha256:955b3c214bccf3ee2a7930768137fd7ed6a72677334be67a07c78a622abd318aDeleted: sha256:a2e35a0fdb20100365e2fb26c65357fcf926ac7990bf9074a51cbac5a8358d7eDeleted: sha256:8c3a028fc66f360ce6ce6c206786df68fac4c24257474cbe4f67eda0ac21efd6Deleted: sha256:0a6d37fabaceb4faa555e729a7d97cb6ee193cb97789a213907a3d3c156d7e35Deleted: sha256:579519c51de1afe1e29d284b1741af239a307975197cf6ce213a70068d923231Deleted: sha256:f5600c6330da7bb112776ba067a32a9c20842d6ecc8ee3289f1a713b644092f8Untagged: hello-world:latestUntagged: hello-world@sha256:e7c70bb24b462baa86c102610182e3efcb12a04854e8c582838d92970a09f323Deleted: sha256:bf756fb1ae65adf866bd8c456593cd24beb6a0a061dedf42b26a993176745f6b 容器命令有了镜像才能创建容器，linux，下载一个centos镜像来测试 [root@VM-0-15-centos ~]# docker pull centos 新建容器并启动 docker run [可选参数] image#参数说明--name=&quot;Name&quot; #容器名字-d #使用后台方式运行，类似nohup-it #使用交互方式运行，进入容器查看内容-p #指定容器端口 -p 8080:8080 -p 主机端口:容器端口 (常用) -p 容器端口 容器端口-P #随机指定端口 启动并进入容器 [root@VM-0-15-centos ~]# docker run -it centos /bin/bash[root@1faa1a0b849e /]# [root@1faa1a0b849e /]# lsbin etc lib lost+found mnt proc run srv tmp vardev home lib64 media opt root sbin sys usr#退出容器[root@1faa1a0b849e /]# exitexit[root@VM-0-15-centos ~]# ls 列出所有运行中的容器 #列出当前在运行的容器[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES#列出所有运行过的容器[root@VM-0-15-centos ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1faa1a0b849e centos &quot;/bin/bash&quot; 4 minutes ago Exited (0) 55 seconds ago frosty_cori0d482f3a4d60 bf756fb1ae65 &quot;/hello&quot; 34 hours ago Exited (0) 34 hours ago nifty_visvesvarayae05f6bfe06e0 bf756fb1ae65 &quot;/hello&quot; 34 hours ago Exited (0) 34 hours ago amazing_diffie8a40e14a2c36 bf756fb1ae65 &quot;/hello&quot; 34 hours ago Exited (0) 34 hours ago goofy_liskov#列出最近创建的容器[root@VM-0-15-centos ~]# docker ps -n=1CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1faa1a0b849e centos &quot;/bin/bash&quot; 5 minutes ago Exited (0) About a minute ago frosty_cori#只显示容器的编号[root@VM-0-15-centos ~]# docker ps -aq1faa1a0b849e0d482f3a4d60e05f6bfe06e08a40e14a2c36 退出容器 exit #直接容器停止并退出CTRL+P+Q #容器不停止并退出[root@VM-0-15-centos ~]# docker run -it centos /bin/bash[root@e5a4a59f28ff /]# [root@VM-0-15-centos ~]# [root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe5a4a59f28ff centos &quot;/bin/bash&quot; 21 seconds ago Up 20 seconds agitated_ritchie 删除容器 docker rm 容器id #删除指定的容器,不能删除正在运行的容器，如要删除，加-f#递归删除所有docker容器docker rm -f $(docker ps -aq) #删除所有容器docker ps -a -q|xargs docker rm[root@VM-0-15-centos ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe5a4a59f28ff centos &quot;/bin/bash&quot; 5 minutes ago Up 5 minutes agitated_ritchiec4fd9794c39a centos &quot;/bin/bash&quot; 5 minutes ago Exited (0) 5 minutes ago dreamy_lumiere1faa1a0b849e centos &quot;/bin/bash&quot; 16 minutes ago Exited (0) 12 minutes ago frosty_cori0d482f3a4d60 bf756fb1ae65 &quot;/hello&quot; 34 hours ago Exited (0) 34 hours ago nifty_visvesvarayae05f6bfe06e0 bf756fb1ae65 &quot;/hello&quot; 34 hours ago Exited (0) 34 hours ago amazing_diffie8a40e14a2c36 bf756fb1ae65 &quot;/hello&quot; 34 hours ago Exited (0) 34 hours ago goofy_liskov[root@VM-0-15-centos ~]# docker rm e5a4a59f28ffError response from daemon: You cannot remove a running container e5a4a59f28ffc678ab9ea5ed7a9af825ff053e1816cd94c90880b3d2a87997df. Stop the container before attempting removal or force remove[root@VM-0-15-centos ~]# docker rm -f $(docker ps -aq)e5a4a59f28ffc4fd9794c39a1faa1a0b849e0d482f3a4d60e05f6bfe06e08a40e14a2c36[root@VM-0-15-centos ~]# docker ps -aq[root@VM-0-15-centos ~]# 启动和停止容器的操作 docker start 容器id #启动 docker restart 容器id #重启docker stop 容器id #停止当前运行的容器docker kill 容器id #强制停止当前容器 常用其他命令后台启动容器 docker run -d centos #后台启动 #docker ps 时候，发现centos会停止，因为容器后台运行，就必须要一个前台进程，docker发现没有应用，就会自动停止 查看日志 docker logs -f -t --tail 10 容器id #发现没有日志 查看容器中的进程信息 docker top 容器id [root@VM-0-15-centos ~]# docker run -it centos /bin/bash[root@ba393184f315 /]# [root@VM-0-15-centos ~]# [root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESba393184f315 centos &quot;/bin/bash&quot; 9 seconds ago Up 8 seconds wizardly_cerf[root@VM-0-15-centos ~]# docker top ba393184f315UID PID PPID C STIME TTY TIME CMDroot 25856 25840 0 09:46 pts/0 00:00:00 /bin/bash 查看镜像的元数据 docker inspect 容器id [root@VM-0-15-centos ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESba393184f315 centos &quot;/bin/bash&quot; 3 minutes ago Up 3 minutes wizardly_cerf[root@VM-0-15-centos ~]# docker inspect ba393184f315[ &#123; &quot;Id&quot;: &quot;ba393184f3157b86b24aa040b24b4e169c3d1aed0ca56edee01596a7f833fd73&quot;, &quot;Created&quot;: &quot;2020-12-01T01:46:02.700180427Z&quot;, &quot;Path&quot;: &quot;/bin/bash&quot;, &quot;Args&quot;: [], &quot;State&quot;: &#123; &quot;Status&quot;: &quot;running&quot;, &quot;Running&quot;: true, &quot;Paused&quot;: false, &quot;Restarting&quot;: false, &quot;OOMKilled&quot;: false, &quot;Dead&quot;: false, &quot;Pid&quot;: 25856, &quot;ExitCode&quot;: 0, &quot;Error&quot;: &quot;&quot;, &quot;StartedAt&quot;: &quot;2020-12-01T01:46:02.955068318Z&quot;, &quot;FinishedAt&quot;: &quot;0001-01-01T00:00:00Z&quot; &#125;, &quot;Image&quot;: &quot;sha256:0d120b6ccaa8c5e149176798b3501d4dd1885f961922497cd0abef155c869566&quot;, &quot;ResolvConfPath&quot;: &quot;/var/lib/docker/containers/ba393184f3157b86b24aa040b24b4e169c3d1aed0ca56edee01596a7f833fd73/resolv.conf&quot;, &quot;HostnamePath&quot;: &quot;/var/lib/docker/containers/ba393184f3157b86b24aa040b24b4e169c3d1aed0ca56edee01596a7f833fd73/hostname&quot;, &quot;HostsPath&quot;: &quot;/var/lib/docker/containers/ba393184f3157b86b24aa040b24b4e169c3d1aed0ca56edee01596a7f833fd73/hosts&quot;, &quot;LogPath&quot;: &quot;/var/lib/docker/containers/ba393184f3157b86b24aa040b24b4e169c3d1aed0ca56edee01596a7f833fd73/ba393184f3157b86b24aa040b24b4e169c3d1aed0ca56edee01596a7f833fd73-json.log&quot;, &quot;Name&quot;: &quot;/wizardly_cerf&quot;, &quot;RestartCount&quot;: 0, &quot;Driver&quot;: &quot;overlay2&quot;, &quot;Platform&quot;: &quot;linux&quot;, &quot;MountLabel&quot;: &quot;&quot;, &quot;ProcessLabel&quot;: &quot;&quot;, &quot;AppArmorProfile&quot;: &quot;&quot;, &quot;ExecIDs&quot;: null, &quot;HostConfig&quot;: &#123; &quot;Binds&quot;: null, &quot;ContainerIDFile&quot;: &quot;&quot;, &quot;LogConfig&quot;: &#123; &quot;Type&quot;: &quot;json-file&quot;, &quot;Config&quot;: &#123;&#125; &#125;, &quot;NetworkMode&quot;: &quot;default&quot;, &quot;PortBindings&quot;: &#123;&#125;, &quot;RestartPolicy&quot;: &#123; &quot;Name&quot;: &quot;no&quot;, &quot;MaximumRetryCount&quot;: 0 &#125;, &quot;AutoRemove&quot;: false, &quot;VolumeDriver&quot;: &quot;&quot;, &quot;VolumesFrom&quot;: null, &quot;CapAdd&quot;: null, &quot;CapDrop&quot;: null, &quot;Capabilities&quot;: null, &quot;Dns&quot;: [], &quot;DnsOptions&quot;: [], &quot;DnsSearch&quot;: [], &quot;ExtraHosts&quot;: null, &quot;GroupAdd&quot;: null, &quot;IpcMode&quot;: &quot;private&quot;, &quot;Cgroup&quot;: &quot;&quot;, &quot;Links&quot;: null, &quot;OomScoreAdj&quot;: 0, &quot;PidMode&quot;: &quot;&quot;, &quot;Privileged&quot;: false, &quot;PublishAllPorts&quot;: false, &quot;ReadonlyRootfs&quot;: false, &quot;SecurityOpt&quot;: null, &quot;UTSMode&quot;: &quot;&quot;, &quot;UsernsMode&quot;: &quot;&quot;, &quot;ShmSize&quot;: 67108864, &quot;Runtime&quot;: &quot;runc&quot;, &quot;ConsoleSize&quot;: [ 0, 0 ], &quot;Isolation&quot;: &quot;&quot;, &quot;CpuShares&quot;: 0, &quot;Memory&quot;: 0, &quot;NanoCpus&quot;: 0, &quot;CgroupParent&quot;: &quot;&quot;, &quot;BlkioWeight&quot;: 0, &quot;BlkioWeightDevice&quot;: [], &quot;BlkioDeviceReadBps&quot;: null, &quot;BlkioDeviceWriteBps&quot;: null, &quot;BlkioDeviceReadIOps&quot;: null, &quot;BlkioDeviceWriteIOps&quot;: null, &quot;CpuPeriod&quot;: 0, &quot;CpuQuota&quot;: 0, &quot;CpuRealtimePeriod&quot;: 0, &quot;CpuRealtimeRuntime&quot;: 0, &quot;CpusetCpus&quot;: &quot;&quot;, &quot;CpusetMems&quot;: &quot;&quot;, &quot;Devices&quot;: [], &quot;DeviceCgroupRules&quot;: null, &quot;DeviceRequests&quot;: null, &quot;KernelMemory&quot;: 0, &quot;KernelMemoryTCP&quot;: 0, &quot;MemoryReservation&quot;: 0, &quot;MemorySwap&quot;: 0, &quot;MemorySwappiness&quot;: null, &quot;OomKillDisable&quot;: false, &quot;PidsLimit&quot;: null, &quot;Ulimits&quot;: null, &quot;CpuCount&quot;: 0, &quot;CpuPercent&quot;: 0, &quot;IOMaximumIOps&quot;: 0, &quot;IOMaximumBandwidth&quot;: 0, &quot;MaskedPaths&quot;: [ &quot;/proc/asound&quot;, &quot;/proc/acpi&quot;, &quot;/proc/kcore&quot;, &quot;/proc/keys&quot;, &quot;/proc/latency_stats&quot;, &quot;/proc/timer_list&quot;, &quot;/proc/timer_stats&quot;, &quot;/proc/sched_debug&quot;, &quot;/proc/scsi&quot;, &quot;/sys/firmware&quot; ], &quot;ReadonlyPaths&quot;: [ &quot;/proc/bus&quot;, &quot;/proc/fs&quot;, &quot;/proc/irq&quot;, &quot;/proc/sys&quot;, &quot;/proc/sysrq-trigger&quot; ] &#125;, &quot;GraphDriver&quot;: &#123; &quot;Data&quot;: &#123; &quot;LowerDir&quot;: &quot;/var/lib/docker/overlay2/78d46a1dd3114ea4472382c70d189091005ab80b93b328fdf6917ea374e09135-init/diff:/var/lib/docker/overlay2/9977c7654d8482400f51116d18f3c0562764ac2c1082c8de455e689c10593b30/diff&quot;, &quot;MergedDir&quot;: &quot;/var/lib/docker/overlay2/78d46a1dd3114ea4472382c70d189091005ab80b93b328fdf6917ea374e09135/merged&quot;, &quot;UpperDir&quot;: &quot;/var/lib/docker/overlay2/78d46a1dd3114ea4472382c70d189091005ab80b93b328fdf6917ea374e09135/diff&quot;, &quot;WorkDir&quot;: &quot;/var/lib/docker/overlay2/78d46a1dd3114ea4472382c70d189091005ab80b93b328fdf6917ea374e09135/work&quot; &#125;, &quot;Name&quot;: &quot;overlay2&quot; &#125;, &quot;Mounts&quot;: [], &quot;Config&quot;: &#123; &quot;Hostname&quot;: &quot;ba393184f315&quot;, &quot;Domainname&quot;: &quot;&quot;, &quot;User&quot;: &quot;&quot;, &quot;AttachStdin&quot;: true, &quot;AttachStdout&quot;: true, &quot;AttachStderr&quot;: true, &quot;Tty&quot;: true, &quot;OpenStdin&quot;: true, &quot;StdinOnce&quot;: true, &quot;Env&quot;: [ &quot;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&quot; ], &quot;Cmd&quot;: [ &quot;/bin/bash&quot; ], &quot;Image&quot;: &quot;centos&quot;, &quot;Volumes&quot;: null, &quot;WorkingDir&quot;: &quot;&quot;, &quot;Entrypoint&quot;: null, &quot;OnBuild&quot;: null, &quot;Labels&quot;: &#123; &quot;org.label-schema.build-date&quot;: &quot;20200809&quot;, &quot;org.label-schema.license&quot;: &quot;GPLv2&quot;, &quot;org.label-schema.name&quot;: &quot;CentOS Base Image&quot;, &quot;org.label-schema.schema-version&quot;: &quot;1.0&quot;, &quot;org.label-schema.vendor&quot;: &quot;CentOS&quot; &#125; &#125;, &quot;NetworkSettings&quot;: &#123; &quot;Bridge&quot;: &quot;&quot;, &quot;SandboxID&quot;: &quot;913e925d94cac8aaa4ead2ed29197af7d07d4fbfd35625642019116a42cff156&quot;, &quot;HairpinMode&quot;: false, &quot;LinkLocalIPv6Address&quot;: &quot;&quot;, &quot;LinkLocalIPv6PrefixLen&quot;: 0, &quot;Ports&quot;: &#123;&#125;, &quot;SandboxKey&quot;: &quot;/var/run/docker/netns/913e925d94ca&quot;, &quot;SecondaryIPAddresses&quot;: null, &quot;SecondaryIPv6Addresses&quot;: null, &quot;EndpointID&quot;: &quot;9da28d1e996685ffa1a39d26fe042919d661c4b4c8267f3470f2042ee78810ed&quot;, &quot;Gateway&quot;: &quot;172.18.0.1&quot;, &quot;GlobalIPv6Address&quot;: &quot;&quot;, &quot;GlobalIPv6PrefixLen&quot;: 0, &quot;IPAddress&quot;: &quot;172.18.0.2&quot;, &quot;IPPrefixLen&quot;: 16, &quot;IPv6Gateway&quot;: &quot;&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:12:00:02&quot;, &quot;Networks&quot;: &#123; &quot;bridge&quot;: &#123; &quot;IPAMConfig&quot;: null, &quot;Links&quot;: null, &quot;Aliases&quot;: null, &quot;NetworkID&quot;: &quot;df77c62f666112b12946dbbbc16f1dcdbbcc8548e3fa56519fd062e5331a0c08&quot;, &quot;EndpointID&quot;: &quot;9da28d1e996685ffa1a39d26fe042919d661c4b4c8267f3470f2042ee78810ed&quot;, &quot;Gateway&quot;: &quot;172.18.0.1&quot;, &quot;IPAddress&quot;: &quot;172.18.0.2&quot;, &quot;IPPrefixLen&quot;: 16, &quot;IPv6Gateway&quot;: &quot;&quot;, &quot;GlobalIPv6Address&quot;: &quot;&quot;, &quot;GlobalIPv6PrefixLen&quot;: 0, &quot;MacAddress&quot;: &quot;02:42:ac:12:00:02&quot;, &quot;DriverOpts&quot;: null &#125; &#125; &#125; &#125;] 进入当前运行的容器 通常容器都是后台运行的，有时我们需要进入容器修改一些配置 docker exec -it 容器id bashShell [root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESba393184f315 centos \"/bin/bash\" 11 minutes ago Up 11 minutes wizardly_cerf[root@VM-0-15-centos ~]# docker exec -it ba393184f315 /bin/bash[root@ba393184f315 /]# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 01:46 pts/0 00:00:00 /bin/bashroot 14 0 0 01:57 pts/1 00:00:00 /bin/bashroot 27 14 0 01:57 pts/1 00:00:00 ps -ef docker attach 容器id [root@VM-0-15-centos ~]# docker attach ba393184f315[root@ba393184f315 /]# docker exec 进入容器后，开启新的终端，可以在里面操作docker attach 进入容器正在执行的终端 从容器拷贝文件到主机 docker cp 容器id:容器内路径 目的的主机路径 [root@VM-0-15-centos home]# docker attach b67b64394534[root@b67b64394534 /]# cd /home [root@b67b64394534 home]# ls[root@b67b64394534 home]# touch rx.java[root@b67b64394534 home]# read escape sequence[root@VM-0-15-centos home]# [root@VM-0-15-centos home]# docker cp b67b64394534:/home/rx.java /home[root@VM-0-15-centos home]# lsrx.java rxk.java Test1-Docker部属nginx#下载Nginx[root@VM-0-15-centos ~]# docker pull nginxUsing default tag: latestlatest: Pulling from library/nginx852e50cd189d: Pull complete 571d7e852307: Pull complete addb10abd9cb: Pull complete d20aa7ccdb77: Pull complete 8b03f1e11359: Pull complete Digest: sha256:6b1daa9462046581ac15be20277a7c75476283f969cb3a61c8725ec38d3b01c3Status: Downloaded newer image for nginx:latestdocker.io/library/nginx:latest#查看目前运行镜像[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb67b64394534 centos \"/bin/bash\" 10 hours ago Up 10 hours suspicious_sutherland[root@VM-0-15-centos ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEnginx latest bc9a0695f571 6 days ago 133MBcentos latest 0d120b6ccaa8 3 months ago 215MB#以后台方式运行镜像，并且将nginx的80端口映射到本地3344端口，同时给此镜像一个名字--name nginx01[root@VM-0-15-centos ~]# docker run -d --name nginx01 -p 3344:80 nginx5a3e8a8e16e5e1f46b8bbe6c23662aebd3c13dc5cacfd63fb24d0703c1235819[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5a3e8a8e16e5 nginx \"/docker-entrypoint.…\" 2 minutes ago Up 2 minutes 0.0.0.0:3344-&gt;80/tcp nginx01b67b64394534 centos \"/bin/bash\" 10 hours ago Up 10 hours suspicious_sutherland#请求本地3344端口，显示nginx，成功运行[root@VM-0-15-centos ~]# curl localhost:3344&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;#进入容器[root@VM-0-15-centos ~]# docker exec -it nginx01 /bin/bashroot@5a3e8a8e16e5:/# whereis nginxnginx: /usr/sbin/nginx /usr/lib/nginx /etc/nginx /usr/share/nginx#关闭容器[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5a3e8a8e16e5 nginx \"/docker-entrypoint.…\" 33 minutes ago Up 33 minutes 0.0.0.0:3344-&gt;80/tcp nginx01b67b64394534 centos \"/bin/bash\" 10 hours ago Up 10 hours suspicious_sutherland[root@VM-0-15-centos ~]# docker stop nginx01nginx01[root@VM-0-15-centos ~]# docker stop centosError response from daemon: No such container: centos[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb67b64394534 centos \"/bin/bash\" 10 hours ago Up 10 hours suspicious_sutherland[root@VM-0-15-centos ~]# docker stop b67b64394534b67b64394534 Test2-Docker部属tomcat#下载并启动tomcat (加了--rm 意味着用完就会删除)[root@VM-0-15-centos ~]# docker run -it --rm tomcat:9.0Unable to find image 'tomcat:9.0' locally9.0: Pulling from library/tomcat756975cb9c7e: Pull complete d77915b4e630: Pull complete 5f37a0a41b6b: Pull complete 96b2c1e36db5: Pull complete 27a2d52b526e: Pull complete a867dba77389: Pull complete 0939c055fb79: Pull complete 0b0694ce0ae2: Pull complete 81a5f8099e05: Pull complete c3d7917d545e: Pull complete#正常下载[root@VM-0-15-centos ~]# docker pull tomcat:9.09.0: Pulling from library/tomcatDigest: sha256:a319b10d8729817c7ce0bcc2343a6f97711c7870395019340d96b6aafd6ccbeaStatus: Image is up to date for tomcat:9.0docker.io/library/tomcat:9.0#将tomcat 8080端口映射到外部3355端口[root@VM-0-15-centos ~]# docker run -d -p 3355:8080 --name tomcat01 tomcataf2d7437747112b51f83c556b1790503f41e2632bbbe965e00a8bff3d3268c92#进入tomcat[root@VM-0-15-centos ~]# docker exec -it tomcat01 /bin/bashroot@af2d74377471:/usr/local/tomcat# lsBUILDING.txt LICENSE README.md RUNNING.txt conf logs temp webapps.distCONTRIBUTING.md NOTICE RELEASE-NOTES bin lib native-jni-lib webapps work Test3-Docker部属ES+kibanaES暴露的端口较多，十分耗内存，ES的数据一般挂载在安全目录 # --net somenetwork网络配置docker run -d --name elasticsearch --net somenetwork -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" elasticsearch:tag#下载并启动ESdocker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" elasticsearch:7.6.2#启动之后 服务器会较为卡顿#查看CPU和内存状态#docker stats #增加内存限制 -e环境配置修改docker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -e ES_JAVA_OPTS=\"-Xms64m -Xmx512m\" elasticsearch:7.6.2CONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDSdb5428acc88d elasticsearch02 0.77% 358.6MiB / 1.795GiB 19.51% 656B / 0B 6.41MB / 729kB 43^C[root@VM-0-15-centos ~]# curl localhost:9200&#123; \"name\" : \"db5428acc88d\", \"cluster_name\" : \"docker-cluster\", \"cluster_uuid\" : \"MqrmYUWDTT2jYZJHxsVU0A\", \"version\" : &#123; \"number\" : \"7.6.2\", \"build_flavor\" : \"default\", \"build_type\" : \"docker\", \"build_hash\" : \"ef48eb35cf30adf4db14086e8aabd07ef6fb113f\", \"build_date\" : \"2020-03-26T06:34:37.794943Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.4.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" &#125;, \"tagline\" : \"You Know, for Search\"&#125; 可视化启动portainer [root@VM-0-15-centos ~]# docker run -d -p 8088:9000 --restart=always -v /var/run/docker.sock:/var/run/docker.sock --privileged=true portainer/portainerUnable to find image 'portainer/portainer:latest' locallylatest: Pulling from portainer/portainerd1e017099d17: Pull complete 717377b83d5c: Pull complete Digest: sha256:f8c2b0a9ca640edf508a8a0830cf1963a1e0d2fd9936a64104b3f658e120b868Status: Downloaded newer image for portainer/portainer:latestcb19cef5927877f6e10e15013b81fffdaefd5cb2e426f34819fc869c109ab7b3 commit镜像#先启动tomcat 并确保在运行[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES675a7261e3b7 tomcat \"catalina.sh run\" 55 seconds ago Up 54 seconds 0.0.0.0:8080-&gt;8080/tcp heuristic_curie#进入tomcat[root@VM-0-15-centos ~]# docker exec -it 675a7261e3b7 /bin/bash#将webapps中的东西复制上去root@675a7261e3b7:/usr/local/tomcat# cd webappsroot@675a7261e3b7:/usr/local/tomcat/webapps# lsroot@675a7261e3b7:/usr/local/tomcat/webapps# cd ..root@675a7261e3b7:/usr/local/tomcat# cp -r webapps.dist/* webappsroot@675a7261e3b7:/usr/local/tomcat# cd webappsroot@675a7261e3b7:/usr/local/tomcat/webapps# lsROOT docs examples host-manager managerroot@675a7261e3b7:/usr/local/tomcat/webapps# exit[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES675a7261e3b7 tomcat \"catalina.sh run\" 7 minutes ago Up 7 minutes 0.0.0.0:8080-&gt;8080/tcp heuristic_curie#commit一个镜像,-a是作者,-m是描述[root@VM-0-15-centos ~]# docker commit -a=\"cinkate\" -m=\"add webapps\" 675a7261e3b7 tomcat02:1.0sha256:80b6126c75b93053ee013c1a724a10fb958ddab21e5b0f3f10a58dbb996c74eb[root@VM-0-15-centos ~]# docker imgaesdocker: 'imgaes' is not a docker command.See 'docker --help'#查看是否commit成功[root@VM-0-15-centos ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEtomcat02 1.0 80b6126c75b9 8 seconds ago 654MBnginx latest bc9a0695f571 8 days ago 133MBtomcat 9.0 e0bd8b34b4ea 2 weeks ago 649MBtomcat latest e0bd8b34b4ea 2 weeks ago 649MBcentos latest 0d120b6ccaa8 3 months ago 215MBportainer/portainer latest 62771b0b9b09 4 months ago 79.1MBelasticsearch 7.6.2 f29a1ee41030 8 months ago 791MB docker容器数据卷即使删除了容器，数据还是存在的，类似于数据持久化容器之间应该有一个数据共享的技术！Docker容器中产生的数据，同步到本地！ 这就是卷技术！目录的挂载，将容器的目录，挂载到Linux上面！ 总结：为了容器的持久化和同步操作~，容器间也是可以数据共享！ 使用数据卷 方式一：直接使用命令来挂载 #-v 主机目录，容器内目录#-p 主机端口，容器内端口[root@VM-0-15-centos ~]# docker run -it -v#将centos /home下的目录 挂载到 主机下/home/ceshi下[root@VM-0-15-centos home]# docker run -it -v /home/ceshi:/home centos /bin/bash[root@081342dcbd4f /]# #另一个terminal(本机的home目录)[root@VM-0-15-centos ~]# cd /home[root@VM-0-15-centos home]# lsceshi rx.java rxk.java#查看元信息，挂载成功 Mounts[root@VM-0-15-centos home]# docker inspect 081342dcbd4f[ &#123; \"Id\": \"081342dcbd4f81f7d9d8495b8741dc2270b88d89123f5f422802cece726e75ee\", \"Created\": \"2020-12-03T13:39:28.974545204Z\", \"Path\": \"/bin/bash\", \"Args\": [], \"State\": &#123; \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 1314, \"ExitCode\": 0, \"Error\": \"\", \"StartedAt\": \"2020-12-03T13:39:29.238082253Z\", \"FinishedAt\": \"0001-01-01T00:00:00Z\" &#125;, \"Image\": \"sha256:0d120b6ccaa8c5e149176798b3501d4dd1885f961922497cd0abef155c869566\", \"ResolvConfPath\": \"/var/lib/docker/containers/081342dcbd4f81f7d9d8495b8741dc2270b88d89123f5f422802cece726e75ee/resolv.conf\", \"HostnamePath\": \"/var/lib/docker/containers/081342dcbd4f81f7d9d8495b8741dc2270b88d89123f5f422802cece726e75ee/hostname\", \"HostsPath\": \"/var/lib/docker/containers/081342dcbd4f81f7d9d8495b8741dc2270b88d89123f5f422802cece726e75ee/hosts\", \"LogPath\": \"/var/lib/docker/containers/081342dcbd4f81f7d9d8495b8741dc2270b88d89123f5f422802cece726e75ee/081342dcbd4f81f7d9d8495b8741dc2270b88d89123f5f422802cece726e75ee-json.log\", \"Name\": \"/clever_lovelace\", \"RestartCount\": 0, \"Driver\": \"overlay2\", \"Platform\": \"linux\", \"MountLabel\": \"\", \"ProcessLabel\": \"\", \"AppArmorProfile\": \"\", \"ExecIDs\": null, \"HostConfig\": &#123; \"Binds\": [ \"/home/ceshi:/home\" ], \"ContainerIDFile\": \"\", \"LogConfig\": &#123; \"Type\": \"json-file\", \"Config\": &#123;&#125; &#125;, \"NetworkMode\": \"default\", \"PortBindings\": &#123;&#125;, \"RestartPolicy\": &#123; \"Name\": \"no\", \"MaximumRetryCount\": 0 &#125;, \"AutoRemove\": false, \"VolumeDriver\": \"\", \"VolumesFrom\": null, \"CapAdd\": null, \"CapDrop\": null, \"Capabilities\": null, \"Dns\": [], \"DnsOptions\": [], \"DnsSearch\": [], \"ExtraHosts\": null, \"GroupAdd\": null, \"IpcMode\": \"private\", \"Cgroup\": \"\", \"Links\": null, \"OomScoreAdj\": 0, \"PidMode\": \"\", \"Privileged\": false, \"PublishAllPorts\": false, \"ReadonlyRootfs\": false, \"SecurityOpt\": null, \"UTSMode\": \"\", \"UsernsMode\": \"\", \"ShmSize\": 67108864, \"Runtime\": \"runc\", \"ConsoleSize\": [ 0, 0 ], \"Isolation\": \"\", \"CpuShares\": 0, \"Memory\": 0, \"NanoCpus\": 0, \"CgroupParent\": \"\", \"BlkioWeight\": 0, \"BlkioWeightDevice\": [], \"BlkioDeviceReadBps\": null, \"BlkioDeviceWriteBps\": null, \"BlkioDeviceReadIOps\": null, \"BlkioDeviceWriteIOps\": null, \"CpuPeriod\": 0, \"CpuQuota\": 0, \"CpuRealtimePeriod\": 0, \"CpuRealtimeRuntime\": 0, \"CpusetCpus\": \"\", \"CpusetMems\": \"\", \"Devices\": [], \"DeviceCgroupRules\": null, \"DeviceRequests\": null, \"KernelMemory\": 0, \"KernelMemoryTCP\": 0, \"MemoryReservation\": 0, \"MemorySwap\": 0, \"MemorySwappiness\": null, \"OomKillDisable\": false, \"PidsLimit\": null, \"Ulimits\": null, \"CpuCount\": 0, \"CpuPercent\": 0, \"IOMaximumIOps\": 0, \"IOMaximumBandwidth\": 0, \"MaskedPaths\": [ \"/proc/asound\", \"/proc/acpi\", \"/proc/kcore\", \"/proc/keys\", \"/proc/latency_stats\", \"/proc/timer_list\", \"/proc/timer_stats\", \"/proc/sched_debug\", \"/proc/scsi\", \"/sys/firmware\" ], \"ReadonlyPaths\": [ \"/proc/bus\", \"/proc/fs\", \"/proc/irq\", \"/proc/sys\", \"/proc/sysrq-trigger\" ] &#125;, \"GraphDriver\": &#123; \"Data\": &#123; \"LowerDir\": \"/var/lib/docker/overlay2/519ce3c698154cf997249d71ab41e9203f413522d25e777dc5f8544f2ba22c9e-init/diff:/var/lib/docker/overlay2/9977c7654d8482400f51116d18f3c0562764ac2c1082c8de455e689c10593b30/diff\", \"MergedDir\": \"/var/lib/docker/overlay2/519ce3c698154cf997249d71ab41e9203f413522d25e777dc5f8544f2ba22c9e/merged\", \"UpperDir\": \"/var/lib/docker/overlay2/519ce3c698154cf997249d71ab41e9203f413522d25e777dc5f8544f2ba22c9e/diff\", \"WorkDir\": \"/var/lib/docker/overlay2/519ce3c698154cf997249d71ab41e9203f413522d25e777dc5f8544f2ba22c9e/work\" &#125;, \"Name\": \"overlay2\" &#125;, \"Mounts\": [ &#123; \"Type\": \"bind\", \"Source\": \"/home/ceshi\", \"Destination\": \"/home\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"rprivate\" &#125; ], \"Config\": &#123; \"Hostname\": \"081342dcbd4f\", \"Domainname\": \"\", \"User\": \"\", \"AttachStdin\": true, \"AttachStdout\": true, \"AttachStderr\": true, \"Tty\": true, \"OpenStdin\": true, \"StdinOnce\": true, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], \"Cmd\": [ \"/bin/bash\" ], \"Image\": \"centos\", \"Volumes\": null, \"WorkingDir\": \"\", \"Entrypoint\": null, \"OnBuild\": null, \"Labels\": &#123; \"org.label-schema.build-date\": \"20200809\", \"org.label-schema.license\": \"GPLv2\", \"org.label-schema.name\": \"CentOS Base Image\", \"org.label-schema.schema-version\": \"1.0\", \"org.label-schema.vendor\": \"CentOS\" &#125; &#125;, \"NetworkSettings\": &#123; \"Bridge\": \"\", \"SandboxID\": \"7cc48bb3849fdd8ca238a8ba1f09409e01e7c50856abbccf23d59caaf5997c8d\", \"HairpinMode\": false, \"LinkLocalIPv6Address\": \"\", \"LinkLocalIPv6PrefixLen\": 0, \"Ports\": &#123;&#125;, \"SandboxKey\": \"/var/run/docker/netns/7cc48bb3849f\", \"SecondaryIPAddresses\": null, \"SecondaryIPv6Addresses\": null, \"EndpointID\": \"6bcd4d80b78f3f8ddeb0ee5468ba2e738509eafd745ca2a5e969ff0fa6dceb08\", \"Gateway\": \"172.18.0.1\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"IPAddress\": \"172.18.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"MacAddress\": \"02:42:ac:12:00:02\", \"Networks\": &#123; \"bridge\": &#123; \"IPAMConfig\": null, \"Links\": null, \"Aliases\": null, \"NetworkID\": \"df77c62f666112b12946dbbbc16f1dcdbbcc8548e3fa56519fd062e5331a0c08\", \"EndpointID\": \"6bcd4d80b78f3f8ddeb0ee5468ba2e738509eafd745ca2a5e969ff0fa6dceb08\", \"Gateway\": \"172.18.0.1\", \"IPAddress\": \"172.18.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"MacAddress\": \"02:42:ac:12:00:02\", \"DriverOpts\": null &#125; &#125; &#125; &#125;]#容器的home目录下[root@081342dcbd4f home]# touch test.java[root@081342dcbd4f home]# lstest.java#主机挂载的ceshi目录下[root@VM-0-15-centos home]# cd ceshi/[root@VM-0-15-centos ceshi]# lstest.java 关闭容器之后，在主机中挂载的目录中创建，修改文件，都是可以同步到容器中的目录。 Test4–安装MySQL#获取镜像[root@VM-0-15-centos home]# docker pull mysql:5.75.7: Pulling from library/mysql852e50cd189d: Already exists 29969ddb0ffb: Pull complete a43f41a44c48: Pull complete 5cdd802543a3: Pull complete b79b040de953: Pull complete 938c64119969: Pull complete 7689ec51a0d9: Pull complete 36bd6224d58f: Pull complete cab9d3fa4c8c: Pull complete 1b741e1c47de: Pull complete aac9d11987ac: Pull complete Digest: sha256:8e2004f9fe43df06c3030090f593021a5f283d028b5ed5765cc24236c2c4d88eStatus: Downloaded newer image for mysql:5.7docker.io/library/mysql:5.7#运行容器，需要做数据挂载# -d后台启动# -p端口映射# -v 挂载配置文件 和 数据文件# -e 修改环境变量，修改密码# --name 重命名[root@VM-0-15-centos home]# docker run -d -p 3310:3306 -v /home/mysql/conf:/etc/mysql/conf.d -v /home/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 --name mysql01 mysql:5.72f81782daf7d49e73365da7def107120a8f79c71b9c5bb21a4fc082f2ebef664#启动成功后，使用本地数据库管理软件可以进行正常连接 假设将容器删除，挂载在本地的数据卷不会丢失，实现了容器的持久化技术。 具名和匿名挂载#匿名挂载-v不写本地路径，会匿名进行挂载docker run -d -P --name nginx01 -v /etc/nginx nginx#具名挂载docker run -d -P --name nginx02 -v juming-nginx:/etc/nginx nginx#通过-v 卷名:容器内路径#查看这个卷所有docker容器内的卷，没有指定目录的情况下：都是在/var/lib/docker/volumes/xxxx/_data通过具名挂载，可以方便的找到一个卷，大多数情况使用具名挂载 如何确定是具名还是匿名挂载？还是指定路径挂载？ -v 容器内路径 #匿名挂载-v 卷名:容器内路径 #具名挂载-v /宿主机路径:容器内路径 #指定路径挂载 拓展 #通过 -v 容器内路径:ro rw 可以改变读写权限#ro readonly#rw readwrite#一旦设定了容器权限，容器对我们挂载出来的内容就有限定了！#ro 说明这个路径只能通过宿主机来操作，容器内部无法操作！docker run -d -P --name nginx02 -v juming-nginx:/etc/nginx:ro nginxdocker run -d -P --name nginx02 -v juming-nginx:/etc/nginx:rw nginx 初始Dockerfile 方式二：Dockerfile就是用来构建docker镜像的构建文件！通过这个脚本，可以生成镜像，脚本为一个个命令，每个命令都是一层。 #构建一个rxk/centos[root@VM-0-15-centos home]# cd docker-test-volume/[root@VM-0-15-centos docker-test-volume]# ls[root@VM-0-15-centos docker-test-volume]# pwd/home/docker-test-volume[root@VM-0-15-centos docker-test-volume]# [root@VM-0-15-centos docker-test-volume]# nano dockerfile1#文件中的内容[root@VM-0-15-centos docker-test-volume]# cat dockerfile1 FROM centosVOLUME [\"volume01\",\"volume02\"]CMD echo \"-----end-------\"CMD /bin/bash[root@VM-0-15-centos docker-test-volume]# docker build -f dockerfile1 -t /rxk/centos .invalid argument \"/rxk/centos\" for \"-t, --tag\" flag: invalid reference formatSee 'docker build --help'.[root@VM-0-15-centos docker-test-volume]# docker build -f dockerfile1 -t rxk/centos .Sending build context to Docker daemon 2.048kBStep 1/4 : FROM centos ---&gt; 0d120b6ccaa8Step 2/4 : VOLUME [\"volume01\",\"volume02\"] ---&gt; Running in 634e59a80996Removing intermediate container 634e59a80996 ---&gt; 02a6fde9ba35Step 3/4 : CMD echo \"-----end-------\" ---&gt; Running in 3fe4feb278dfRemoving intermediate container 3fe4feb278df ---&gt; d64085501c38Step 4/4 : CMD /bin/bash ---&gt; Running in 09d704b706feRemoving intermediate container 09d704b706fe ---&gt; 7735159af50cSuccessfully built 7735159af50cSuccessfully tagged rxk/centos:latest#查看当前镜像[root@VM-0-15-centos docker-test-volume]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZErxk/centos latest 7735159af50c 5 minutes ago 215MBtomcat02 1.0 80b6126c75b9 7 hours ago 654MBnginx latest bc9a0695f571 8 days ago 133MBmysql 5.7 ae0658fdbad5 12 days ago 449MBtomcat 9.0 e0bd8b34b4ea 2 weeks ago 649MBtomcat latest e0bd8b34b4ea 2 weeks ago 649MBcentos latest 0d120b6ccaa8 3 months ago 215MBportainer/portainer latest 62771b0b9b09 4 months ago 79.1MBelasticsearch 7.6.2 f29a1ee41030 8 months ago 791MB#进入自己[root@VM-0-15-centos docker-test-volume]# docker run -it 7735159af50c /bin/bash#可以看到挂载的两个volume卷[root@d6463f570ce4 /]# ls -ltotal 56lrwxrwxrwx 1 root root 7 May 11 2019 bin -&gt; usr/bindrwxr-xr-x 5 root root 360 Dec 3 15:21 devdrwxr-xr-x 1 root root 4096 Dec 3 15:21 etcdrwxr-xr-x 2 root root 4096 May 11 2019 homelrwxrwxrwx 1 root root 7 May 11 2019 lib -&gt; usr/liblrwxrwxrwx 1 root root 9 May 11 2019 lib64 -&gt; usr/lib64drwx------ 2 root root 4096 Aug 9 21:40 lost+founddrwxr-xr-x 2 root root 4096 May 11 2019 mediadrwxr-xr-x 2 root root 4096 May 11 2019 mntdrwxr-xr-x 2 root root 4096 May 11 2019 optdr-xr-xr-x 91 root root 0 Dec 3 15:21 procdr-xr-x--- 2 root root 4096 Aug 9 21:40 rootdrwxr-xr-x 11 root root 4096 Aug 9 21:40 runlrwxrwxrwx 1 root root 8 May 11 2019 sbin -&gt; usr/sbindrwxr-xr-x 2 root root 4096 May 11 2019 srvdr-xr-xr-x 13 root root 0 Dec 3 15:21 sysdrwxrwxrwt 7 root root 4096 Aug 9 21:40 tmpdrwxr-xr-x 12 root root 4096 Aug 9 21:40 usrdrwxr-xr-x 20 root root 4096 Aug 9 21:40 vardrwxr-xr-x 2 root root 4096 Dec 3 15:21 volume01drwxr-xr-x 2 root root 4096 Dec 3 15:21 volume02#这个卷Volume一定和外部有一个同步的目录[root@d6463f570ce4 volume01]# touch container.txt[root@d6463f570ce4 volume01]# lscontainer.txt[root@d6463f570ce4 volume01]# [root@VM-0-15-centos docker-test-volume]# [root@VM-0-15-centos docker-test-volume]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESd6463f570ce4 7735159af50c \"/bin/bash\" 7 minutes ago Up 7 minutes zen_faraday[root@VM-0-15-centos docker-test-volume]# docker inspect d6463f570ce4[ &#123; \"Id\": \"d6463f570ce423c10082cce5414bf5ba70b8cb9f0ab47df998fde77b8e965881\", \"Created\": \"2020-12-03T15:21:34.913373526Z\", \"Path\": \"/bin/bash\", \"Args\": [], \"State\": &#123; \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 15638, \"ExitCode\": 0, \"Error\": \"\", \"StartedAt\": \"2020-12-03T15:21:35.202804066Z\", \"FinishedAt\": \"0001-01-01T00:00:00Z\" &#125;, \"Image\": \"sha256:7735159af50c4ff09863f0a97c37447043ac0eca6657913a4e33c65b0be4a24d\", \"ResolvConfPath\": \"/var/lib/docker/containers/d6463f570ce423c10082cce5414bf5ba70b8cb9f0ab47df998fde77b8e965881/resolv.conf\", \"HostnamePath\": \"/var/lib/docker/containers/d6463f570ce423c10082cce5414bf5ba70b8cb9f0ab47df998fde77b8e965881/hostname\", \"HostsPath\": \"/var/lib/docker/containers/d6463f570ce423c10082cce5414bf5ba70b8cb9f0ab47df998fde77b8e965881/hosts\", \"LogPath\": \"/var/lib/docker/containers/d6463f570ce423c10082cce5414bf5ba70b8cb9f0ab47df998fde77b8e965881/d6463f570ce423c10082cce5414bf5ba70b8cb9f0ab47df998fde77b8e965881-json.log\", \"Name\": \"/zen_faraday\", \"RestartCount\": 0, \"Driver\": \"overlay2\", \"Platform\": \"linux\", \"MountLabel\": \"\", \"ProcessLabel\": \"\", \"AppArmorProfile\": \"\", \"ExecIDs\": null, \"HostConfig\": &#123; \"Binds\": null, \"ContainerIDFile\": \"\", \"LogConfig\": &#123; \"Type\": \"json-file\", \"Config\": &#123;&#125; &#125;, \"NetworkMode\": \"default\", \"PortBindings\": &#123;&#125;, \"RestartPolicy\": &#123; \"Name\": \"no\", \"MaximumRetryCount\": 0 &#125;, \"AutoRemove\": false, \"VolumeDriver\": \"\", \"VolumesFrom\": null, \"CapAdd\": null, \"CapDrop\": null, \"Capabilities\": null, \"Dns\": [], \"DnsOptions\": [], \"DnsSearch\": [], \"ExtraHosts\": null, \"GroupAdd\": null, \"IpcMode\": \"private\", \"Cgroup\": \"\", \"Links\": null, \"OomScoreAdj\": 0, \"PidMode\": \"\", \"Privileged\": false, \"PublishAllPorts\": false, \"ReadonlyRootfs\": false, \"SecurityOpt\": null, \"UTSMode\": \"\", \"UsernsMode\": \"\", \"ShmSize\": 67108864, \"Runtime\": \"runc\", \"ConsoleSize\": [ 0, 0 ], \"Isolation\": \"\", \"CpuShares\": 0, \"Memory\": 0, \"NanoCpus\": 0, \"CgroupParent\": \"\", \"BlkioWeight\": 0, \"BlkioWeightDevice\": [], \"BlkioDeviceReadBps\": null, \"BlkioDeviceWriteBps\": null, \"BlkioDeviceReadIOps\": null, \"BlkioDeviceWriteIOps\": null, \"CpuPeriod\": 0, \"CpuQuota\": 0, \"CpuRealtimePeriod\": 0, \"CpuRealtimeRuntime\": 0, \"CpusetCpus\": \"\", \"CpusetMems\": \"\", \"Devices\": [], \"DeviceCgroupRules\": null, \"DeviceRequests\": null, \"KernelMemory\": 0, \"KernelMemoryTCP\": 0, \"MemoryReservation\": 0, \"MemorySwap\": 0, \"MemorySwappiness\": null, \"OomKillDisable\": false, \"PidsLimit\": null, \"Ulimits\": null, \"CpuCount\": 0, \"CpuPercent\": 0, \"IOMaximumIOps\": 0, \"IOMaximumBandwidth\": 0, \"MaskedPaths\": [ \"/proc/asound\", \"/proc/acpi\", \"/proc/kcore\", \"/proc/keys\", \"/proc/latency_stats\", \"/proc/timer_list\", \"/proc/timer_stats\", \"/proc/sched_debug\", \"/proc/scsi\", \"/sys/firmware\" ], \"ReadonlyPaths\": [ \"/proc/bus\", \"/proc/fs\", \"/proc/irq\", \"/proc/sys\", \"/proc/sysrq-trigger\" ] &#125;, \"GraphDriver\": &#123; \"Data\": &#123; \"LowerDir\": \"/var/lib/docker/overlay2/16864c6fe5942b10aff31e1156dbb08a055e31975f8725d25a205bd9f76b6f23-init/diff:/var/lib/docker/overlay2/9977c7654d8482400f51116d18f3c0562764ac2c1082c8de455e689c10593b30/diff\", \"MergedDir\": \"/var/lib/docker/overlay2/16864c6fe5942b10aff31e1156dbb08a055e31975f8725d25a205bd9f76b6f23/merged\", \"UpperDir\": \"/var/lib/docker/overlay2/16864c6fe5942b10aff31e1156dbb08a055e31975f8725d25a205bd9f76b6f23/diff\", \"WorkDir\": \"/var/lib/docker/overlay2/16864c6fe5942b10aff31e1156dbb08a055e31975f8725d25a205bd9f76b6f23/work\" &#125;, \"Name\": \"overlay2\" &#125;, \"Mounts\": [ &#123; \"Type\": \"volume\", \"Name\": \"4bb7c13d437e16eb97bfcc71aa2faac18594ded837fcefc6d266e8ad36845c6a\", \"Source\": \"/var/lib/docker/volumes/4bb7c13d437e16eb97bfcc71aa2faac18594ded837fcefc6d266e8ad36845c6a/_data\", \"Destination\": \"volume01\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" &#125;, &#123; \"Type\": \"volume\", \"Name\": \"0eca9d146b2fd63f434657516305c20241da68936914059527e728865109e3f1\", \"Source\": \"/var/lib/docker/volumes/0eca9d146b2fd63f434657516305c20241da68936914059527e728865109e3f1/_data\", \"Destination\": \"volume02\", \"Driver\": \"local\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"\" &#125; ], \"Config\": &#123; \"Hostname\": \"d6463f570ce4\", \"Domainname\": \"\", \"User\": \"\", \"AttachStdin\": true, \"AttachStdout\": true, \"AttachStderr\": true, \"Tty\": true, \"OpenStdin\": true, \"StdinOnce\": true, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], \"Cmd\": [ \"/bin/bash\" ], \"Image\": \"7735159af50c\", \"Volumes\": &#123; \"volume01\": &#123;&#125;, \"volume02\": &#123;&#125; &#125;, \"WorkingDir\": \"\", \"Entrypoint\": null, \"OnBuild\": null, \"Labels\": &#123; \"org.label-schema.build-date\": \"20200809\", \"org.label-schema.license\": \"GPLv2\", \"org.label-schema.name\": \"CentOS Base Image\", \"org.label-schema.schema-version\": \"1.0\", \"org.label-schema.vendor\": \"CentOS\" &#125; &#125;, \"NetworkSettings\": &#123; \"Bridge\": \"\", \"SandboxID\": \"cf151107d8fa56fd220695eed265b431618350222be583761b13354867477e39\", \"HairpinMode\": false, \"LinkLocalIPv6Address\": \"\", \"LinkLocalIPv6PrefixLen\": 0, \"Ports\": &#123;&#125;, \"SandboxKey\": \"/var/run/docker/netns/cf151107d8fa\", \"SecondaryIPAddresses\": null, \"SecondaryIPv6Addresses\": null, \"EndpointID\": \"1c1bab6bce8a119f4731a2555a39256791d792539b260d2fd230aea73940d75b\", \"Gateway\": \"172.18.0.1\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"IPAddress\": \"172.18.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"MacAddress\": \"02:42:ac:12:00:02\", \"Networks\": &#123; \"bridge\": &#123; \"IPAMConfig\": null, \"Links\": null, \"Aliases\": null, \"NetworkID\": \"df77c62f666112b12946dbbbc16f1dcdbbcc8548e3fa56519fd062e5331a0c08\", \"EndpointID\": \"1c1bab6bce8a119f4731a2555a39256791d792539b260d2fd230aea73940d75b\", \"Gateway\": \"172.18.0.1\", \"IPAddress\": \"172.18.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"MacAddress\": \"02:42:ac:12:00:02\", \"DriverOpts\": null &#125; &#125; &#125; &#125;] 数据卷容器不同容器之间同步数据 #docker01已经启动了[root@VM-0-15-centos _data]# docker run -it --name docker02 --volumes-from docker01 rxk/centos#--volumes-from类似于 docker02继承docker01#即使docker01被删了，docker02中卷的文件也是存在的，类似硬链接 DockerFiledockerfile是用来构建docker镜像的文件！命令参数脚本！ 构建步骤： 1.编写一个dockerfile文件 2.docker build构建成为一个镜像 3.docker run运行镜像 4.docker push发布镜像(DockerHub、阿里云镜像仓库！) 很多官方镜像都是基础包，很多功能没有，通常会自己搭建自己的镜像。 DockerFile的构建过程基础知识 1.每个保留关键字(指令)都必须是大写字母 2.执行从上到下顺序执行 3.# 表示注释 4.每一个指令都会创建提交一个新的镜像层 并提交 DockerFile:构建文件，定义了一切的步骤，源代码。 DockerImages:通过DockerFile构建生成的镜像，最终发布和运行的产品 Docker容器：容器就是镜像运行起来提供服务的 DockerFile的指令FROM # 基础镜像，一切从这里开始构建MAINTAINER #镜像是谁写的，姓名+邮箱RUN #镜像构建的时候需要运行的命令ADD #步骤：构建基于tomcat的镜像，进行添加！ADD为添加内容WORKDIR #镜像的工作目录VOLUME #挂载的容器卷EXPOSE #指定暴露端口CMD #指定这个容器启动时候要运行的命令，只有最后一个会生效，可被替代ENTRYPOINT #指定这个容器启动的时候要运行的命令，可以追加命令ONBUILD #当构建一个被继承DockerFile， 这个时候就会运行ONBUILD命令。触发指令COPY #类似ADD命令 ，将文件拷贝到镜像中ENV #构建时候设置环境变量 构建自己的centos 构建自己的centos #1.编写dockerfile的文件FROM centosMAINTAINER kuangshen&lt;179049243@qq.com&gt;ENV MYPATH /usr/localWORKDIR $MYPATHRUN yum -y install vimRUN yum -y install net-toolsEXPOSE 80CMD echo $MYPATHCMD echo \"----end----\"CMD /bin/bash#2.通过文件构建镜像# docker build -f dockerfile文件路径 -t 镜像名:[tag].#docker build -f my-dockerfile -t mycentos:0.1 .[root@VM-0-15-centos dockerfile]# docker build -f my-dockerfile -t mycentos:0.1 .Sending build context to Docker daemon 2.048kBStep 1/10 : FROM centos ---&gt; 0d120b6ccaa8Step 2/10 : MAINTAINER kuangshen&lt;179049243@qq.com&gt; ---&gt; Running in 3dd9d64bfbffRemoving intermediate container 3dd9d64bfbff ---&gt; e56a7b5f722bStep 3/10 : ENV MYPATH /usr/local ---&gt; Running in 7707c6ee0441Removing intermediate container 7707c6ee0441 ---&gt; b67dab755631Step 4/10 : WORKDIR $MYPATH ---&gt; Running in 1797c746ab6bRemoving intermediate container 1797c746ab6b ---&gt; 3ed5689097b4Step 5/10 : RUN yum -y install vim ---&gt; Running in 5791c022d132CentOS-8 - AppStream 969 kB/s | 6.2 MB 00:06 CentOS-8 - Base 950 kB/s | 2.3 MB 00:02 CentOS-8 - Extras 11 kB/s | 8.1 kB 00:00 Dependencies resolved.================================================================================ Package Arch Version Repository Size================================================================================Installing: vim-enhanced x86_64 2:8.0.1763-15.el8 AppStream 1.4 MInstalling dependencies: gpm-libs x86_64 1.20.7-15.el8 AppStream 39 k vim-common x86_64 2:8.0.1763-15.el8 AppStream 6.3 M vim-filesystem noarch 2:8.0.1763-15.el8 AppStream 48 k which x86_64 2.21-12.el8 BaseOS 49 kTransaction Summary================================================================================Install 5 PackagesTotal download size: 7.8 MInstalled size: 30 MDownloading Packages:(1/5): gpm-libs-1.20.7-15.el8.x86_64.rpm 327 kB/s | 39 kB 00:00 (2/5): vim-filesystem-8.0.1763-15.el8.noarch.rp 827 kB/s | 48 kB 00:00 (3/5): which-2.21-12.el8.x86_64.rpm 316 kB/s | 49 kB 00:00 (4/5): vim-enhanced-8.0.1763-15.el8.x86_64.rpm 1.2 MB/s | 1.4 MB 00:01 (5/5): vim-common-8.0.1763-15.el8.x86_64.rpm 983 kB/s | 6.3 MB 00:06 --------------------------------------------------------------------------------Total 974 kB/s | 7.8 MB 00:08 warning: /var/cache/dnf/AppStream-02e86d1c976ab532/packages/gpm-libs-1.20.7-15.el8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID 8483c65d: NOKEYCentOS-8 - AppStream 1.6 MB/s | 1.6 kB 00:00 Importing GPG key 0x8483C65D: Userid : \"CentOS (CentOS Official Signing Key) &lt;security@centos.org&gt;\" Fingerprint: 99DB 70FA E1D7 CE22 7FB6 4882 05B5 55B3 8483 C65D From : /etc/pki/rpm-gpg/RPM-GPG-KEY-centosofficialKey imported successfullyRunning transaction checkTransaction check succeeded.Running transaction testTransaction test succeeded.Running transaction Preparing : 1/1 Installing : which-2.21-12.el8.x86_64 1/5 Installing : vim-filesystem-2:8.0.1763-15.el8.noarch 2/5 Installing : vim-common-2:8.0.1763-15.el8.x86_64 3/5 Installing : gpm-libs-1.20.7-15.el8.x86_64 4/5 Running scriptlet: gpm-libs-1.20.7-15.el8.x86_64 4/5 Installing : vim-enhanced-2:8.0.1763-15.el8.x86_64 5/5 Running scriptlet: vim-enhanced-2:8.0.1763-15.el8.x86_64 5/5 Running scriptlet: vim-common-2:8.0.1763-15.el8.x86_64 5/5 Verifying : gpm-libs-1.20.7-15.el8.x86_64 1/5 Verifying : vim-common-2:8.0.1763-15.el8.x86_64 2/5 Verifying : vim-enhanced-2:8.0.1763-15.el8.x86_64 3/5 Verifying : vim-filesystem-2:8.0.1763-15.el8.noarch 4/5 Verifying : which-2.21-12.el8.x86_64 5/5 Installed: gpm-libs-1.20.7-15.el8.x86_64 vim-common-2:8.0.1763-15.el8.x86_64 vim-enhanced-2:8.0.1763-15.el8.x86_64 vim-filesystem-2:8.0.1763-15.el8.noarch which-2.21-12.el8.x86_64 Complete!Removing intermediate container 5791c022d132 ---&gt; 05dc9bde286eStep 6/10 : RUN yum -y install net-tools ---&gt; Running in 900fc6c0b10eLast metadata expiration check: 0:00:15 ago on Thu Dec 10 15:07:33 2020.Dependencies resolved.================================================================================ Package Architecture Version Repository Size================================================================================Installing: net-tools x86_64 2.0-0.52.20160912git.el8 BaseOS 322 kTransaction Summary================================================================================Install 1 PackageTotal download size: 322 kInstalled size: 942 kDownloading Packages:net-tools-2.0-0.52.20160912git.el8.x86_64.rpm 969 kB/s | 322 kB 00:00 --------------------------------------------------------------------------------Total 181 kB/s | 322 kB 00:01 Running transaction checkTransaction check succeeded.Running transaction testTransaction test succeeded.Running transaction Preparing : 1/1 Installing : net-tools-2.0-0.52.20160912git.el8.x86_64 1/1 Running scriptlet: net-tools-2.0-0.52.20160912git.el8.x86_64 1/1 Verifying : net-tools-2.0-0.52.20160912git.el8.x86_64 1/1 Installed: net-tools-2.0-0.52.20160912git.el8.x86_64 Complete!Removing intermediate container 900fc6c0b10e ---&gt; 3f8796ecbdeaStep 7/10 : EXPOSE 80 ---&gt; Running in 37e6f3792548Removing intermediate container 37e6f3792548 ---&gt; 2d37cf00b36bStep 8/10 : CMD echo $MYPATH ---&gt; Running in 7e4ff55e4b24Removing intermediate container 7e4ff55e4b24 ---&gt; d74bce644e74Step 9/10 : CMD echo \"----end----\" ---&gt; Running in fc3301142b73Removing intermediate container fc3301142b73 ---&gt; 2c7e307a481cStep 10/10 : CMD /bin/bash ---&gt; Running in 4e5c6edd147cRemoving intermediate container 4e5c6edd147c ---&gt; 071c7fb7d0f9Successfully built 071c7fb7d0f9Successfully tagged mycentos:0.1#3.测试镜像[root@VM-0-15-centos dockerfile]# docker run -it mycentos:0.1[root@5b68abc14a5a local]# pwd/usr/local[root@5b68abc14a5a local]# vimsh: wq: command not foundshell returned 127Press ENTER or type command to continuesh: wq: command not foundshell returned 127Press ENTER or type command to continue 镜像构建历史查看 [root@VM-0-15-centos dockerfile]# docker history 071c7fb7d0f9IMAGE CREATED CREATED BY SIZE COMMENT071c7fb7d0f9 6 minutes ago /bin/sh -c #(nop) CMD [\"/bin/sh\" \"-c\" \"/bin… 0B 2c7e307a481c 6 minutes ago /bin/sh -c #(nop) CMD [\"/bin/sh\" \"-c\" \"echo… 0B d74bce644e74 6 minutes ago /bin/sh -c #(nop) CMD [\"/bin/sh\" \"-c\" \"echo… 0B 2d37cf00b36b 6 minutes ago /bin/sh -c #(nop) EXPOSE 80 0B 3f8796ecbdea 6 minutes ago /bin/sh -c yum -y install net-tools 23.1MB 05dc9bde286e 6 minutes ago /bin/sh -c yum -y install vim 57.7MB 3ed5689097b4 7 minutes ago /bin/sh -c #(nop) WORKDIR /usr/local 0B b67dab755631 7 minutes ago /bin/sh -c #(nop) ENV MYPATH=/usr/local 0B e56a7b5f722b 7 minutes ago /bin/sh -c #(nop) MAINTAINER kuangshen&lt;1790… 0B 0d120b6ccaa8 4 months ago /bin/sh -c #(nop) CMD [\"/bin/bash\"] 0B &lt;missing&gt; 4 months ago /bin/sh -c #(nop) LABEL org.label-schema.sc… 0B &lt;missing&gt; 4 months ago /bin/sh -c #(nop) ADD file:538afc0c5c964ce0d… 215MB 实战：Dockerfile制作tomcat镜像1、准备镜像文件tomcat压缩包，jdk的压缩包！2、编写dockerfile文件。 Docker0网络详解#删除所有镜像[root@VM-0-15-centos ~]# docker rmi -f $(docker images -aq)Untagged: rxk/centos:latestDeleted: sha256:7735159af50c4ff09863f0a97c37447043ac0eca6657913a4e33c65b0be4a24dDeleted: sha256:d64085501c38257f9828c1dc9717f26e20df9bcb8d1b958b7f49ea5bb0425ae0Deleted: sha256:02a6fde9ba357c11d75c6d828c8fee80e1e5e77c4aedb3a83c7e302fc1745d40Untagged: tomcat02:1.0Deleted: sha256:80b6126c75b93053ee013c1a724a10fb958ddab21e5b0f3f10a58dbb996c74ebDeleted: sha256:20e9e09986f8d937efa6f2ad283aa9a1b837cc5cc5d4d76e6105f83e38e8f3deUntagged: nginx:latestUntagged: nginx@sha256:6b1daa9462046581ac15be20277a7c75476283f969cb3a61c8725ec38d3b01c3Deleted: sha256:bc9a0695f5712dcaaa09a5adc415a3936ccba13fc2587dfd76b1b8aeea3f221cUntagged: mysql:5.7Untagged: mysql@sha256:8e2004f9fe43df06c3030090f593021a5f283d028b5ed5765cc24236c2c4d88eDeleted: sha256:ae0658fdbad5fb1c9413c998d8a573eeb5d16713463992005029c591e6400d02Untagged: tomcat:9.0Untagged: tomcat:latestUntagged: tomcat@sha256:a319b10d8729817c7ce0bcc2343a6f97711c7870395019340d96b6aafd6ccbeaDeleted: sha256:e0bd8b34b4ea904874e55eae50e8987815030d140f9773a4b61759f4f85bf38dUntagged: portainer/portainer:latestUntagged: portainer/portainer@sha256:f8c2b0a9ca640edf508a8a0830cf1963a1e0d2fd9936a64104b3f658e120b868Deleted: sha256:62771b0b9b0973a3e8e95595534a1240d8cfd968d30ec82dc0393ce0a256c5f3Untagged: elasticsearch:7.6.2Untagged: elasticsearch@sha256:1b09dbd93085a1e7bca34830e77d2981521a7210e11f11eda997add1c12711faDeleted: sha256:f29a1ee41030e3963026369105f3bee76d75fdecbeca07932ac054126be7bff9Error response from daemon: conflict: unable to delete 071c7fb7d0f9 (cannot be forced) - image is being used by running container 5b68abc14a5aError response from daemon: conflict: unable to delete 3f8796ecbdea (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete d74bce644e74 (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete 2d37cf00b36b (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete 2c7e307a481c (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete 05dc9bde286e (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete 3ed5689097b4 (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete e56a7b5f722b (cannot be forced) - image has dependent child imagesError response from daemon: conflict: unable to delete b67dab755631 (cannot be forced) - image has dependent child imagesError: No such image: 02a6fde9ba35Error: No such image: d64085501c38Error: No such image: e0bd8b34b4eaError response from daemon: conflict: unable to delete 0d120b6ccaa8 (cannot be forced) - image has dependent child images 三个网络 #启动一个tomcat[root@VM-0-15-centos ~]# docker run -d -P --name tomcat02 tomcat942539fdda0c018ffa3750661373e624c4f2226f5d9532323d5c42c1daea98db[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES942539fdda0c tomcat \"catalina.sh run\" 3 seconds ago Up 3 seconds 0.0.0.0:32768-&gt;8080/tcp tomcat02#查看容器的内部网络地址[root@VM-0-15-centos ~]# docker exec -it tomcat02 ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever112: eth0@if113: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.18.0.2/16 brd 172.18.255.255 scope global eth0 valid_lft forever preferred_lft forever linux可以ping通docker容器内部 原理 1.我们每安装一个docker容器，docker就会给docker容器分配一个ip，我们只要安装了docker，就会有一个网卡，docker0，使用桥接模式，使用技术是veth-pair技术！ 再次测试ip addr: 2.再启动一个容器测试，又会多一对网卡，这样一对对的网卡，其实是veth-pair技术，就是一对的虚拟设备，他们都是成对出现，一段连着协议，一段彼此相连，veth-pair充当一个桥梁，链接各种虚拟网络设备 3.测试下tomcat02和tomcat03能否ping通，确实可以ping通 容器和容器之间是可以互相ping通的 [root@VM-0-15-centos ~]# docker exec -it tomcat03 ping 172.18.0.2PING 172.18.0.2 (172.18.0.2) 56(84) bytes of data.64 bytes from 172.18.0.2: icmp_seq=1 ttl=64 time=0.076 ms64 bytes from 172.18.0.2: icmp_seq=2 ttl=64 time=0.054 ms64 bytes from 172.18.0.2: icmp_seq=3 ttl=64 time=0.053 ms^C--- 172.18.0.2 ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 1001msrtt min/avg/max/mdev = 0.053/0.061/0.076/0.010 ms 结论：tomcat02和tomcat03共用一个路由器，docker0 所有容器不指定网络的情况下，都是由docker0路由的，docker会给我们容器分配一个默认的可用IP 当把docker容器删除之后，ip addr之后 容器ip就没了 [root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESbe137857cf4e tomcat \"catalina.sh run\" 29 minutes ago Up 29 minutes 0.0.0.0:32769-&gt;8080/tcp tomcat03942539fdda0c tomcat \"catalina.sh run\" 37 minutes ago Up 37 minutes 0.0.0.0:32768-&gt;8080/tcp tomcat02[root@VM-0-15-centos ~]# docker rm -f be137857cf4ebe137857cf4e[root@VM-0-15-centos ~]# docker rm -f 942539fdda0c942539fdda0c[root@VM-0-15-centos ~]# ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 52:54:00:83:af:57 brd ff:ff:ff:ff:ff:ff inet 172.17.0.15/20 brd 172.17.15.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::5054:ff:fe83:af57/64 scope link valid_lft forever preferred_lft forever3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:18:37:f9:06 brd ff:ff:ff:ff:ff:ff inet 172.18.0.1/16 brd 172.18.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:18ff:fe37:f906/64 scope link valid_lft forever preferred_lft forever Docker –link 容器互联使用服务名去ping通tomcat docker run -d -P tomcat03 --link tomcat02 tomcatdocker exec -it tomcat03 ping tomcat02#此时3能ping 通2，但是2 ping不通3 Docker自定义网络查看docker所有网络 [root@VM-0-15-centos ~]# docker network lsNETWORK ID NAME DRIVER SCOPEdf77c62f6661 bridge bridge localc916a7eaedff host host local00b73cedd526 none null local 网络模式 bridge:桥接 bridge(默认) none:不配置网络 host:和宿主机共享网络 container:容器内网络连通(使用较少) 创建自己的自定义网络 –driver bridge 默认桥接 –subnet 192.168.0.0/16 子网地址 –gateway 192.168.0.1 网关地址 [root@VM-0-15-centos ~]# docker network create --driver bridge --subnet 192.168.0.0/16 --gateway 192.168.0.1 mynet[root@VM-0-15-centos ~]# docker network lsNETWORK ID NAME DRIVER SCOPEdf77c62f6661 bridge bridge localc916a7eaedff host host localf5100a69756f mynet bridge local00b73cedd526 none null local[root@VM-0-15-centos ~]# docker network inspect mynet[ &#123; \"Name\": \"mynet\", \"Id\": \"f5100a69756fbedf28ab9ad7ecbf354417328efc06d7f0bfae40a0d4c7d33de4\", \"Created\": \"2020-12-22T11:03:05.693800981+08:00\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": &#123; \"Driver\": \"default\", \"Options\": &#123;&#125;, \"Config\": [ &#123; \"Subnet\": \"192.168.0.0/16\", \"Gateway\": \"192.168.0.1\" &#125; ] &#125;, \"Internal\": false, \"Attachable\": false, \"Ingress\": false, \"ConfigFrom\": &#123; \"Network\": \"\" &#125;, \"ConfigOnly\": false, \"Containers\": &#123;&#125;, \"Options\": &#123;&#125;, \"Labels\": &#123;&#125; &#125;]#启动两个容器[root@VM-0-15-centos ~]# docker run -d -P --name tomcat-net-01 --network mynet tomcat8fc4551bbad9d836cea86a3feb957596af6e1310781eadee1baacbb177d84a33[root@VM-0-15-centos ~]# docker run -d -P --name tomcat-net-02 --network mynet tomcat01a3d2a47af6a79689dc85e0a110d576cbdfa8bb979d25d7f493b13157b3e26a[root@VM-0-15-centos ~]# docker network inspect mynet[ &#123; \"Name\": \"mynet\", \"Id\": \"f5100a69756fbedf28ab9ad7ecbf354417328efc06d7f0bfae40a0d4c7d33de4\", \"Created\": \"2020-12-22T11:03:05.693800981+08:00\", \"Scope\": \"local\", \"Driver\": \"bridge\", \"EnableIPv6\": false, \"IPAM\": &#123; \"Driver\": \"default\", \"Options\": &#123;&#125;, \"Config\": [ &#123; \"Subnet\": \"192.168.0.0/16\", \"Gateway\": \"192.168.0.1\" &#125; ] &#125;, \"Internal\": false, \"Attachable\": false, \"Ingress\": false, \"ConfigFrom\": &#123; \"Network\": \"\" &#125;, \"ConfigOnly\": false, \"Containers\": &#123; \"01a3d2a47af6a79689dc85e0a110d576cbdfa8bb979d25d7f493b13157b3e26a\": &#123; \"Name\": \"tomcat-net-02\", \"EndpointID\": \"25be3c90c1e1a4b74f5fc9e10a59c50029ca7ff3bf692d7de636738edb3457b3\", \"MacAddress\": \"02:42:c0:a8:00:03\", \"IPv4Address\": \"192.168.0.3/16\", \"IPv6Address\": \"\" &#125;, \"8fc4551bbad9d836cea86a3feb957596af6e1310781eadee1baacbb177d84a33\": &#123; \"Name\": \"tomcat-net-01\", \"EndpointID\": \"a83c09df1a4595735453dafdde352123dd024707400716fbab256c7e3c02ea4a\", \"MacAddress\": \"02:42:c0:a8:00:02\", \"IPv4Address\": \"192.168.0.2/16\", \"IPv6Address\": \"\" &#125; &#125;, \"Options\": &#123;&#125;, \"Labels\": &#123;&#125; &#125;]#再次测试，不再使用--link 也是可以连接通的，不管使用ip还是服务名[root@VM-0-15-centos ~]# docker exec -it tomcat-net-01 ping 192.168.0.3PING 192.168.0.3 (192.168.0.3) 56(84) bytes of data.64 bytes from 192.168.0.3: icmp_seq=1 ttl=64 time=0.085 ms64 bytes from 192.168.0.3: icmp_seq=2 ttl=64 time=0.055 ms64 bytes from 192.168.0.3: icmp_seq=3 ttl=64 time=0.053 ms^C--- 192.168.0.3 ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 1000msrtt min/avg/max/mdev = 0.053/0.064/0.085/0.016 ms[root@VM-0-15-centos ~]# docker exec -it tomcat-net-01 ping tomcat-net-02PING tomcat-net-02 (192.168.0.3) 56(84) bytes of data.64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=1 ttl=64 time=0.050 ms64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=2 ttl=64 time=0.052 ms64 bytes from tomcat-net-02.mynet (192.168.0.3): icmp_seq=3 ttl=64 time=0.053 ms^C--- tomcat-net-02 ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 2msrtt min/avg/max/mdev = 0.050/0.051/0.053/0.008 ms 我们自定义的网络docker 都已经帮我们维护好了对应的关系，推荐这样使用网络！ 好处：不同的集群使用不同的网络，保证集群是安全和健康 网络连通把一个容器连接到网络上 #将tomcat01连接到mynet网络下，一个容器两个ip docker network connect mynet tomcat01 Redis集群实战通过脚本创建六个redis配置 for port in $(seq 1 6); \\do \\mkdir -p /mydata/redis/node-$&#123;port&#125;/conftouch /mydata/redis/node-$&#123;port&#125;/conf/redis.confcat &lt;&lt; EOF &gt;/mydata/redis/node-$&#123;port&#125;/conf/redis.confport 6379bind 0.0.0.0cluster-enabled yescluster-config-file nodes.confcluster-node-timeout 5000cluster-announce-ip 172.38.0.1$&#123;port&#125;cluster-announce-port 6379cluster-announce-bus-port 16379daemonize noappendonly yesEOFdone 启动redis docker run -p 6371:6379 -p 16371:16379 --name redis-1 -v /mydata/redis/node-1/data:/data -v /mydata/redis/node-1/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.11 redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.confdocker run -p 6372:6379 -p 16372:16379 --name redis-2 -v /mydata/redis/node-2/data:/data -v /mydata/redis/node-2/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.12 redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.confdocker run -p 6373:6379 -p 16373:16379 --name redis-3 -v /mydata/redis/node-3/data:/data -v /mydata/redis/node-3/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.13 redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.confdocker run -p 6374:6379 -p 16374:16379 --name redis-4 -v /mydata/redis/node-4/data:/data -v /mydata/redis/node-4/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.14 redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.confdocker run -p 6375:6379 -p 16375:16379 --name redis-5 -v /mydata/redis/node-5/data:/data -v /mydata/redis/node-5/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.15 redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.confdocker run -p 6376:6379 -p 16376:16379 --name redis-6 -v /mydata/redis/node-6/data:/data -v /mydata/redis/node-6/conf/redis.conf:/etc/redis/redis.conf -d --net redis --ip 172.38.0.16 redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.conf#正常启动之后[root@VM-0-15-centos ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe1535aac14ce redis:5.0.9-alpine3.11 \"docker-entrypoint.s…\" 22 seconds ago Up 22 seconds 0.0.0.0:6376-&gt;6379/tcp, 0.0.0.0:16376-&gt;16379/tcp redis-6fcf18ac7033d redis:5.0.9-alpine3.11 \"docker-entrypoint.s…\" 31 seconds ago Up 30 seconds 0.0.0.0:6375-&gt;6379/tcp, 0.0.0.0:16375-&gt;16379/tcp redis-5b17f66d41588 redis:5.0.9-alpine3.11 \"docker-entrypoint.s…\" 37 seconds ago Up 37 seconds 0.0.0.0:6374-&gt;6379/tcp, 0.0.0.0:16374-&gt;16379/tcp redis-4afb2f4e6484e redis:5.0.9-alpine3.11 \"docker-entrypoint.s…\" 44 seconds ago Up 44 seconds 0.0.0.0:6373-&gt;6379/tcp, 0.0.0.0:16373-&gt;16379/tcp redis-3ef59db5b0cc0 redis:5.0.9-alpine3.11 \"docker-entrypoint.s…\" 54 seconds ago Up 53 seconds 0.0.0.0:6372-&gt;6379/tcp, 0.0.0.0:16372-&gt;16379/tcp redis-20eca360704d7 redis:5.0.9-alpine3.11 \"docker-entrypoint.s…\" About a minute ago Up About a minute 0.0.0.0:6371-&gt;6379/tcp, 0.0.0.0:16371-&gt;16379/tcp redis-1#进入redis-1[root@VM-0-15-centos ~]# docker exec -it redis-1 /bin/sh/data # #创建redis集群[root@VM-0-15-centos ~]# docker exec -it redis-1 /bin/sh/data # redis-cli --cluster create 172.38.0.11:6379 172.38.0.12:6379 172.38.0.13:6379 172.38.0.14:6379 172.38.0.15:6379 172.38.0.16:6379 --cluster-replicas 1&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Master[0] -&gt; Slots 0 - 5460Master[1] -&gt; Slots 5461 - 10922Master[2] -&gt; Slots 10923 - 16383Adding replica 172.38.0.15:6379 to 172.38.0.11:6379Adding replica 172.38.0.16:6379 to 172.38.0.12:6379Adding replica 172.38.0.14:6379 to 172.38.0.13:6379M: f70e64b741bad779e2b545b1429b47b2732a9a13 172.38.0.11:6379 slots:[0-5460] (5461 slots) masterM: de19aadcb9d25a77ca0e5a23f1c45f31a6ece84d 172.38.0.12:6379 slots:[5461-10922] (5462 slots) masterM: 43b45750adfaea90314c724dd1b800621f6b40bf 172.38.0.13:6379 slots:[10923-16383] (5461 slots) masterS: 13b65b47fcac7126bf6ca15a322037571c90464f 172.38.0.14:6379 replicates 43b45750adfaea90314c724dd1b800621f6b40bfS: 03e58c163b80baf32990d6faf3a104d97e14f1e4 172.38.0.15:6379 replicates f70e64b741bad779e2b545b1429b47b2732a9a13S: b80d23b1516da408f9f9bad22007ce4389e529ca 172.38.0.16:6379 replicates de19aadcb9d25a77ca0e5a23f1c45f31a6ece84dCan I set the above configuration? (type 'yes' to accept): yes&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join...&gt;&gt;&gt; Performing Cluster Check (using node 172.38.0.11:6379)M: f70e64b741bad779e2b545b1429b47b2732a9a13 172.38.0.11:6379 slots:[0-5460] (5461 slots) master 1 additional replica(s)M: de19aadcb9d25a77ca0e5a23f1c45f31a6ece84d 172.38.0.12:6379 slots:[5461-10922] (5462 slots) master 1 additional replica(s)S: 13b65b47fcac7126bf6ca15a322037571c90464f 172.38.0.14:6379 slots: (0 slots) slave replicates 43b45750adfaea90314c724dd1b800621f6b40bfS: 03e58c163b80baf32990d6faf3a104d97e14f1e4 172.38.0.15:6379 slots: (0 slots) slave replicates f70e64b741bad779e2b545b1429b47b2732a9a13M: 43b45750adfaea90314c724dd1b800621f6b40bf 172.38.0.13:6379 slots:[10923-16383] (5461 slots) master 1 additional replica(s)S: b80d23b1516da408f9f9bad22007ce4389e529ca 172.38.0.16:6379 slots: (0 slots) slave replicates de19aadcb9d25a77ca0e5a23f1c45f31a6ece84d[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered./data # redis-cli -c127.0.0.1:6379&gt; cluster infocluster_state:okcluster_slots_assigned:16384cluster_slots_ok:16384cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:6cluster_size:3cluster_current_epoch:6cluster_my_epoch:1cluster_stats_messages_ping_sent:149cluster_stats_messages_pong_sent:140cluster_stats_messages_sent:289cluster_stats_messages_ping_received:135cluster_stats_messages_pong_received:149cluster_stats_messages_meet_received:5cluster_stats_messages_received:289127.0.0.1:6379&gt; cluster nodesde19aadcb9d25a77ca0e5a23f1c45f31a6ece84d 172.38.0.12:6379@16379 master - 0 1608624786213 2 conn61-1092213b65b47fcac7126bf6ca15a322037571c90464f 172.38.0.14:6379@16379 slave 43b45750adfaea90314c724ddf6b40bf 0 1608624785000 4 connected03e58c163b80baf32990d6faf3a104d97e14f1e4 172.38.0.15:6379@16379 slave f70e64b741bad779e2b545b1432a9a13 0 1608624785000 5 connected43b45750adfaea90314c724dd1b800621f6b40bf 172.38.0.13:6379@16379 master - 0 1608624785210 3 conn923-16383b80d23b1516da408f9f9bad22007ce4389e529ca 172.38.0.16:6379@16379 slave de19aadcb9d25a77ca0e5a23f6ece84d 0 1608624785712 6 connectedf70e64b741bad779e2b545b1429b47b2732a9a13 172.38.0.11:6379@16379 myself,master - 0 1608624785000cted 0-5460127.0.0.1:6379&gt; set a b-&gt; Redirected to slot [15495] located at 172.38.0.13:6379OK172.38.0.13:6379&gt; get a^C#即使把redis-3 stop了，仍然可以通过其他slaver找到key a所对应的值b，主从配置成功/data # redis-cli -c127.0.0.1:6379&gt; get a-&gt; Redirected to slot [15495] located at 172.38.0.14:6379\"b\"172.38.0.14:6379&gt; 搭建redis集群完成","categories":[{"name":"后台学习","slug":"后台学习","permalink":"http://renxingkai.github.io/categories/后台学习/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://renxingkai.github.io/tags/docker/"}],"author":"CinKate"},{"title":"《A Comparative Study of Word Embeddings for Reading Comprehension》论文阅读","slug":"WordEmbeddingsforReadingComprehension","date":"2019-05-11T19:43:34.000Z","updated":"2020-05-17T16:12:00.854Z","comments":true,"path":"2019/05/12/WordEmbeddingsforReadingComprehension/","link":"","permalink":"http://renxingkai.github.io/2019/05/12/WordEmbeddingsforReadingComprehension/","excerpt":"","text":"众所周知，预训练好的词向量有不同的维度，比如预训练好的GloVe词向量有从50-300维等的词向量表示，但这些不同维度的表示有什么区别，以及在什么时候该用什么维度的词向量（虽然各论文中大家大多用了300维的词向量），这些问题我也确实不太清除。这篇论文解答了我的这些困惑，写的还是很精彩的。 论文原链接 Let’s have a look: 这篇论文主要解决了两个问题点： - 在阅读理解任务中应该使用什么样的预训练词向量 - 测试阶段对于OOV词应怎样处理 所用的数据集和模型数据集： Who-Did-What(WDW) 完型填空类的数据集，从新闻故事中构建 Children’s Book Test(CBT) 从children’s book构建，此论文中仅用了CBT-NE，即数据集中答案为命名实体的数据 模型：Stanford AR、GA Reader 词向量对比： GloVe (50-300) word2vec (300) 实验和结果 词向量的对比 第一个结果：使用在合适的语料库上训练的词向量可以比随机初始化提高3-6％。然而，用于预训练的语料库和方法是重要的选择：例如，在CBT上训练的word2vec词向量比随机词向量执行效果更差。 另请注意，在每种情况下，GloVe词向量都优于在同一语料库中训练的word2vec嵌入。 但是由于词向量对训练参数很敏感，并不能说GloVe一定比word2vec好，但确实从各个论文中也可以看出，一般会优先选择GloVe。 第二个结果：对比GloVe不同维度对实验结果的影响(50-300)：随着词向量维度的增加，实验结果性能是下降的。但是即使使用了300维的GloVe词向量，实验结果仍然比word2vec词向量效果好。 第三个结果：在使用原始语料进行训练时，要先将停用词去掉，与停用词的共现提供关于特定单词的语义的很少有意义的信息，因此具有高百分比的语料库可能不会产生高质量的向量。训练词向量时，超参数调节很重要。 处理OOV词 第一个结果：一般对于OOV词的处理都是赋予一个固定大小不变的词向量（UNK）。这种方法忽略了这样一个事实，即分配为UNK的许多单词可能已经训练过VG中可用的词向量。实验中在测试时，任何新token将被分配其GloVe向量（如果存在）或UNK的向量。 第二个结果，不是为所有OOV词分配一个共同的UNK向量，而是为它们分配未经训练但唯一的随机向量可能更好。此方法在训练时访问测试集词表是没必要的。 总结作者已经证明，用于初始化单词向量的预训练词向量的选择对于阅读理解的神经网络模型的性能具有显著影响。在测试时处理OOV词的方法也是如此。 根据作者的实验，我们建议使用现成的GloVe词向量，并在测试时将预先训练的GloVe向量（如果可用）或随机但唯一的向量分配给OOV词。","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"词向量相关","slug":"词向量相关","permalink":"http://renxingkai.github.io/tags/词向量相关/"}],"author":"CinKate"},{"title":"FastQA学习","slug":"fastqa","date":"2019-04-21T10:52:36.000Z","updated":"2020-05-17T16:11:59.241Z","comments":true,"path":"2019/04/21/fastqa/","link":"","permalink":"http://renxingkai.github.io/2019/04/21/fastqa/","excerpt":"","text":"现在大多数的阅读理解系统都是 top-down 的形式构建的，也就是说一开始就提出了一个很复杂的结构（一般经典的就是 emedding-, encoding-, interaction-, answer-layer ），然后通过 ablation study，不断的减少一些模块配置来验证想法，大多数的创新点都在 interaction 层，比如BIDAF、R-Net等等，大量的工作都在问题和文章的交互query-aware表示上创新，类似人类做阅读理解问题的思路“重复多读文章”，“带着问题读文章”等等，普通的“阅读理解思路”也都被实现了，这篇论文作者发现了很多看似复杂的问题其实通过简单的 context/type matching heruistic 就可以解出来了，过程是选择满足条件的 answer spans: 与 question 对应的 answer type 匹配，比如说问 when 就回答 time； 与重要的 question words 位置上临近； 添加问题单词是否出现在文章中这一“重要”特征；并没有使用复杂的question与context的交互，就取得了在SQuAD榜上与SOTA接近的结果，这篇论文之后，后来的研究者们在做MRC时也会将基础特征加入到embedding中进行共同训练，开源链接。 以下是阅读源码的一些总结： 1.Highway Network的使用Highway Network主要解决的问题是，网络深度加深，梯度信息回流受阻造成网络训练困难的问题。 当网络加深，训练的误差反而上升了，而加入了Highway Network之后，这个问题得到了缓解。一般来说，深度网络训练困难是由于梯度回流受阻的问题，可能浅层网络没有办法得到调整。Highway Network 受LSTM启发，增加了一个门函数，让网络的输出由两部分组成，分别是网络的直接输入以及输入变形后的部分。 网络中把此层放在embedding层后面 import tensorflow as tffrom keras import backend as Kfrom keras.engine.topology import Layerfrom keras.layers import Lambda, Wrapperclass Highway(Layer): def __init__(self, hidden_size, **kwargs): self.hidden_size = hidden_size super().__init__(**kwargs) def build(self, input_shape): self.projection = self.add_weight(name=&apos;projection&apos;, shape=(1, input_shape[-1], self.hidden_size), initializer=&apos;glorot_uniform&apos;) self.W_h = self.add_weight(name=&apos;W_h&apos;, shape=(1, self.hidden_size, self.hidden_size), initializer=&apos;glorot_uniform&apos;) self.b_h = self.add_weight(name=&apos;b_h&apos;, shape=(self.hidden_size,), initializer=&apos;zeros&apos;) self.W_t = self.add_weight(name=&apos;W_t&apos;, shape=(1, self.hidden_size, self.hidden_size), initializer=&apos;glorot_uniform&apos;) self.b_t = self.add_weight(name=&apos;b_t&apos;, shape=(self.hidden_size,), initializer=&apos;zeros&apos;) def call(self, x): x = K.conv1d(x, self.projection) H = tf.nn.tanh(K.bias_add(K.conv1d(x, self.W_h), self.b_h)) T = tf.nn.sigmoid(K.bias_add(K.conv1d(x, self.W_t), self.b_t)) return T * x + (1 - T) * H def compute_output_shape(self, input_shape): batch, seq_len, d = input_shape return (batch, seq_len, self.hidden_size) 2.tf.sequence_mask的学习这个操作和one hot也很像，但是指定的不是index而是从前到后有多少个True，返回的是True和False。 sq_mask = tf.sequence_mask([1, 3, 2], 5)print(sess.run(sq_mask)) 输出： [[True, False, False, False, False],[True, True, True, False, False],[True, True, False, False, False]] 3.tf.expand_dims()学习TensorFlow中，想要维度增加一维，可以使用 tf.expand_dims(input, dim, name=None) 函数。当然，我们常用tf.reshape(input,shape=[])也可以达到相同效果，但是有些时候在构建图的过程中，placeholder没有被feed具体的值，这时就会包下面的错误：TypeError: Expected binary or unicode string, got 1 在这种情况下，我们就可以考虑使用expand_dims来将维度加1。比如我自己代码中遇到的情况，在对图像维度降到二维做特定操作后，要还原成四维[batch, height, width, channels]，前后各增加一维。如果用reshape，则因为上述原因报错 给出官方的例子： # &apos;t&apos; is a tensor of shape [2]shape(expand_dims(t, 0)) ==&gt; [1, 2]shape(expand_dims(t, 1)) ==&gt; [2, 1]shape(expand_dims(t, -1)) ==&gt; [2, 1]# &apos;t2&apos; is a tensor of shape [2, 3, 5]shape(expand_dims(t2, 0)) ==&gt; [1, 2, 3, 5]shape(expand_dims(t2, 2)) ==&gt; [2, 3, 1, 5]shape(expand_dims(t2, 3)) ==&gt; [2, 3, 5, 1] Args: input: A Tensor. dim: A Tensor. Must be one of the following types: int32, int64. 0-D (scalar). Specifies the dimension index at which to expand the shape of input. name: A name for the operation (optional).Returns: A Tensor. Has the same type as input. Contains the same data as input, but its shape has an additional dimension of size 1 added. 4.tf.tile()学习推荐博客tf.tile( input, #输入 multiples, #某一维度上复制的次数 name=None ) import tensorflow as tfa = tf.constant([[1, 2], [3, 4], [5, 6]], dtype=tf.float32)a1 = tf.tile(a, [2, 3])a2 = tf.tile(a, [1, 2])with tf.Session() as sess: print(sess.run(a)) print(sess.run(a1)) print(sess.run(a2)) 输出： [[1. 2.] [3. 4.] [5. 6.]] [[1. 2. 1. 2. 1. 2.] [3. 4. 3. 4. 3. 4.] [5. 6. 5. 6. 5. 6.] [1. 2. 1. 2. 1. 2.] [3. 4. 3. 4. 3. 4.] [5. 6. 5. 6. 5. 6.]][[1. 2. 1. 2.] [3. 4. 3. 4.] [5. 6. 5. 6.]] 5.tf.equal()学习 equal，相等的意思。顾名思义，就是判断，x, y 是不是相等，它的判断方法不是整体判断，而是逐个元素进行判断，如果相等就是 True，不相等，就是 False。 由于是逐个元素判断，所以 x，y 的维度要一致。 例子： import tensorflow as tfa = [[1,2,3],[4,5,6]]b = [[1,0,3],[1,5,1]]with tf.Session() as sess: print(sess.run(tf.equal(a,b))) 输出： [[ True False True] [False True False]] 6.tf.reduce_any()学习在boolean张量的维度上计算元素的 “逻辑或” x = tf.constant([[True, True], [False, False]])with tf.Session() as sess: print(tf.reduce_any(x)) # True print(tf.reduce_any(x, 0)) # [True, True] print(tf.reduce_any(x, 1)) # [True, False] 7.tf.squeeze()学习该函数返回一个张量，这个张量是将原始input中所有维度为1的那些维都删掉的结果axis可以用来指定要删掉的为1的维度，此处要注意指定的维度必须确保其是1，否则会报错squeeze( input, axis=None, name=None, squeeze_dims=None) 例子：# &apos;t&apos; 是一个维度是[1, 2, 1, 3, 1, 1]的张量tf.shape(tf.squeeze(t)) # [2, 3]， 默认删除所有为1的维度# &apos;t&apos; 是一个维度[1, 2, 1, 3, 1, 1]的张量tf.shape(tf.squeeze(t, [2, 4])) # [1, 2, 3, 1]，标号从零开始，只删掉了2和4维的1 8.RepeatVector层RepeatVector层将输入重复n次keras.layers.core.RepeatVector(n) 参数 n：整数，重复的次数 输入shape形如（nb_samples, features）的2D张量 输出shape形如（nb_samples, n, features）的3D张量 例子 model = Sequential()model.add(Dense(32, input_dim=32))# now: model.output_shape == (None, 32)# note: `None` is the batch dimensionmodel.add(RepeatVector(3))# now: model.output_shape == (None, 3, 32) 9.tf.gather()学习类似于数组的索引，可以把向量中某些索引值提取出来，得到新的向量，适用于要提取的索引为不连续的情况。这个函数似乎只适合在一维的情况下使用。 import tensorflow as tf a = tf.Variable([[1,2,3,4,5], [6,7,8,9,10], [11,12,13,14,15]])index_a = tf.Variable([0,2]) b = tf.Variable([1,2,3,4,5,6,7,8,9,10])index_b = tf.Variable([2,4,6,8]) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(sess.run(tf.gather(a, index_a))) print(sess.run(tf.gather(b, index_b))) # [[ 1 2 3 4 5]# [11 12 13 14 15]] # [3 5 7 9] tf.gather_nd同上，但允许在多维上进行索引，例子只展示了一种很简单的用法，","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"阅读理解","slug":"阅读理解","permalink":"http://renxingkai.github.io/tags/阅读理解/"}],"author":"CinKate"},{"title":"基于NLTK的TF-IDF关键词抽取","slug":"tfidfkeyextraction","date":"2019-04-10T14:46:21.000Z","updated":"2020-05-17T16:12:00.477Z","comments":true,"path":"2019/04/10/tfidfkeyextraction/","link":"","permalink":"http://renxingkai.github.io/2019/04/10/tfidfkeyextraction/","excerpt":"","text":"基于nltk总结了用TF-IDF提取关键词的方法，同时总结了文本标准化（预处理），SVD分解、基于TF-IDF、词频等的关键词抽取 SVD奇异值分解from scipy.sparse.linalg import svdsimport reimport nltkimport unicodedatadef low_rank_svd(matrix,singular_count=2): u,s,vt=svds(matrix,k=singular_count) return u,s,vt 删除换行,进行分句def parse_document(document): document=re.sub(&apos;\\n&apos;,&apos; &apos;,document) if isinstance(document,str): document=document elif isinstance(document,unicode): return unicodedata.normalize(&apos;NFKD&apos;,document).encode(&apos;ascii&apos;,&apos;ignore&apos;) else: raise ValueError(&apos;Document is not string or unicode!&apos;) document=document.strip() sentences=nltk.sent_tokenize(document) sentences=[sentence.strip() for sentence in sentences] return sentences 转移HTML标签from html.parser import HTMLParser html_parser=HTMLParser()def unescape_html(parser,text): return parser.unescape_html(text) 缩写词表CONTRACTION_MAP = &#123;&quot;ain&apos;t&quot;: &quot;is not&quot;,&quot;aren&apos;t&quot;: &quot;are not&quot;,&quot;can&apos;t&quot;: &quot;cannot&quot;,&quot;can&apos;t&apos;ve&quot;: &quot;cannot have&quot;,&quot;&apos;cause&quot;: &quot;because&quot;,&quot;could&apos;ve&quot;: &quot;could have&quot;,&quot;couldn&apos;t&quot;: &quot;could not&quot;,&quot;couldn&apos;t&apos;ve&quot;: &quot;could not have&quot;,&quot;didn&apos;t&quot;: &quot;did not&quot;,&quot;doesn&apos;t&quot;: &quot;does not&quot;,&quot;don&apos;t&quot;: &quot;do not&quot;,&quot;hadn&apos;t&quot;: &quot;had not&quot;,&quot;hadn&apos;t&apos;ve&quot;: &quot;had not have&quot;,&quot;hasn&apos;t&quot;: &quot;has not&quot;,&quot;haven&apos;t&quot;: &quot;have not&quot;,&quot;he&apos;d&quot;: &quot;he would&quot;,&quot;he&apos;d&apos;ve&quot;: &quot;he would have&quot;,&quot;he&apos;ll&quot;: &quot;he will&quot;,&quot;he&apos;ll&apos;ve&quot;: &quot;he he will have&quot;,&quot;he&apos;s&quot;: &quot;he is&quot;,&quot;how&apos;d&quot;: &quot;how did&quot;,&quot;how&apos;d&apos;y&quot;: &quot;how do you&quot;,&quot;how&apos;ll&quot;: &quot;how will&quot;,&quot;how&apos;s&quot;: &quot;how is&quot;,&quot;I&apos;d&quot;: &quot;I would&quot;,&quot;I&apos;d&apos;ve&quot;: &quot;I would have&quot;,&quot;I&apos;ll&quot;: &quot;I will&quot;,&quot;I&apos;ll&apos;ve&quot;: &quot;I will have&quot;,&quot;I&apos;m&quot;: &quot;I am&quot;,&quot;I&apos;ve&quot;: &quot;I have&quot;,&quot;i&apos;d&quot;: &quot;i would&quot;,&quot;i&apos;d&apos;ve&quot;: &quot;i would have&quot;,&quot;i&apos;ll&quot;: &quot;i will&quot;,&quot;i&apos;ll&apos;ve&quot;: &quot;i will have&quot;,&quot;i&apos;m&quot;: &quot;i am&quot;,&quot;i&apos;ve&quot;: &quot;i have&quot;,&quot;isn&apos;t&quot;: &quot;is not&quot;,&quot;it&apos;d&quot;: &quot;it would&quot;,&quot;it&apos;d&apos;ve&quot;: &quot;it would have&quot;,&quot;it&apos;ll&quot;: &quot;it will&quot;,&quot;it&apos;ll&apos;ve&quot;: &quot;it will have&quot;,&quot;it&apos;s&quot;: &quot;it is&quot;,&quot;let&apos;s&quot;: &quot;let us&quot;,&quot;ma&apos;am&quot;: &quot;madam&quot;,&quot;mayn&apos;t&quot;: &quot;may not&quot;,&quot;might&apos;ve&quot;: &quot;might have&quot;,&quot;mightn&apos;t&quot;: &quot;might not&quot;,&quot;mightn&apos;t&apos;ve&quot;: &quot;might not have&quot;,&quot;must&apos;ve&quot;: &quot;must have&quot;,&quot;mustn&apos;t&quot;: &quot;must not&quot;,&quot;mustn&apos;t&apos;ve&quot;: &quot;must not have&quot;,&quot;needn&apos;t&quot;: &quot;need not&quot;,&quot;needn&apos;t&apos;ve&quot;: &quot;need not have&quot;,&quot;o&apos;clock&quot;: &quot;of the clock&quot;,&quot;oughtn&apos;t&quot;: &quot;ought not&quot;,&quot;oughtn&apos;t&apos;ve&quot;: &quot;ought not have&quot;,&quot;shan&apos;t&quot;: &quot;shall not&quot;,&quot;sha&apos;n&apos;t&quot;: &quot;shall not&quot;,&quot;shan&apos;t&apos;ve&quot;: &quot;shall not have&quot;,&quot;she&apos;d&quot;: &quot;she would&quot;,&quot;she&apos;d&apos;ve&quot;: &quot;she would have&quot;,&quot;she&apos;ll&quot;: &quot;she will&quot;,&quot;she&apos;ll&apos;ve&quot;: &quot;she will have&quot;,&quot;she&apos;s&quot;: &quot;she is&quot;,&quot;should&apos;ve&quot;: &quot;should have&quot;,&quot;shouldn&apos;t&quot;: &quot;should not&quot;,&quot;shouldn&apos;t&apos;ve&quot;: &quot;should not have&quot;,&quot;so&apos;ve&quot;: &quot;so have&quot;,&quot;so&apos;s&quot;: &quot;so as&quot;,&quot;that&apos;d&quot;: &quot;that would&quot;,&quot;that&apos;d&apos;ve&quot;: &quot;that would have&quot;,&quot;that&apos;s&quot;: &quot;that is&quot;,&quot;there&apos;d&quot;: &quot;there would&quot;,&quot;there&apos;d&apos;ve&quot;: &quot;there would have&quot;,&quot;there&apos;s&quot;: &quot;there is&quot;,&quot;they&apos;d&quot;: &quot;they would&quot;,&quot;they&apos;d&apos;ve&quot;: &quot;they would have&quot;,&quot;they&apos;ll&quot;: &quot;they will&quot;,&quot;they&apos;ll&apos;ve&quot;: &quot;they will have&quot;,&quot;they&apos;re&quot;: &quot;they are&quot;,&quot;they&apos;ve&quot;: &quot;they have&quot;,&quot;to&apos;ve&quot;: &quot;to have&quot;,&quot;wasn&apos;t&quot;: &quot;was not&quot;,&quot;we&apos;d&quot;: &quot;we would&quot;,&quot;we&apos;d&apos;ve&quot;: &quot;we would have&quot;,&quot;we&apos;ll&quot;: &quot;we will&quot;,&quot;we&apos;ll&apos;ve&quot;: &quot;we will have&quot;,&quot;we&apos;re&quot;: &quot;we are&quot;,&quot;we&apos;ve&quot;: &quot;we have&quot;,&quot;weren&apos;t&quot;: &quot;were not&quot;,&quot;what&apos;ll&quot;: &quot;what will&quot;,&quot;what&apos;ll&apos;ve&quot;: &quot;what will have&quot;,&quot;what&apos;re&quot;: &quot;what are&quot;,&quot;what&apos;s&quot;: &quot;what is&quot;,&quot;what&apos;ve&quot;: &quot;what have&quot;,&quot;when&apos;s&quot;: &quot;when is&quot;,&quot;when&apos;ve&quot;: &quot;when have&quot;,&quot;where&apos;d&quot;: &quot;where did&quot;,&quot;where&apos;s&quot;: &quot;where is&quot;,&quot;where&apos;ve&quot;: &quot;where have&quot;,&quot;who&apos;ll&quot;: &quot;who will&quot;,&quot;who&apos;ll&apos;ve&quot;: &quot;who will have&quot;,&quot;who&apos;s&quot;: &quot;who is&quot;,&quot;who&apos;ve&quot;: &quot;who have&quot;,&quot;why&apos;s&quot;: &quot;why is&quot;,&quot;why&apos;ve&quot;: &quot;why have&quot;,&quot;will&apos;ve&quot;: &quot;will have&quot;,&quot;won&apos;t&quot;: &quot;will not&quot;,&quot;won&apos;t&apos;ve&quot;: &quot;will not have&quot;,&quot;would&apos;ve&quot;: &quot;would have&quot;,&quot;wouldn&apos;t&quot;: &quot;would not&quot;,&quot;wouldn&apos;t&apos;ve&quot;: &quot;would not have&quot;,&quot;y&apos;all&quot;: &quot;you all&quot;,&quot;y&apos;all&apos;d&quot;: &quot;you all would&quot;,&quot;y&apos;all&apos;d&apos;ve&quot;: &quot;you all would have&quot;,&quot;y&apos;all&apos;re&quot;: &quot;you all are&quot;,&quot;y&apos;all&apos;ve&quot;: &quot;you all have&quot;,&quot;you&apos;d&quot;: &quot;you would&quot;,&quot;you&apos;d&apos;ve&quot;: &quot;you would have&quot;,&quot;you&apos;ll&quot;: &quot;you will&quot;,&quot;you&apos;ll&apos;ve&quot;: &quot;you will have&quot;,&quot;you&apos;re&quot;: &quot;you are&quot;,&quot;you&apos;ve&quot;: &quot;you have&quot;&#125; 文本标准化import stringfrom nltk.stem import WordNetLemmatizerstopword_list = nltk.corpus.stopwords.words(&apos;english&apos;)wnl = WordNetLemmatizer()html_parser = HTMLParser() 文本分词def tokenize_text(text): tokens = nltk.word_tokenize(text) tokens = [token.strip() for token in tokens] return tokens 扩展缩写def expand_contractions(text, contraction_mapping): contractions_pattern = re.compile(&apos;(&#123;&#125;)&apos;.format(&apos;|&apos;.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL) def expand_match(contraction): match = contraction.group(0) first_char = match[0] expanded_contraction = contraction_mapping.get(match if contraction_mapping.get(match) else contraction_mapping.get(match.lower())) expanded_contraction = first_char+expanded_contraction[1:] return expanded_contraction expanded_text = contractions_pattern.sub(expand_match, text) expanded_text = re.sub(&quot;&apos;&quot;, &quot;&quot;, expanded_text) return expanded_text 标记文本词性from pattern.en import tagfrom nltk.corpus import wordnet as wn# 标记文本词性def pos_tag_text(text): def penn_to_wn_tags(pos_tag): if pos_tag.startswith(&apos;J&apos;): return wn.ADJ elif pos_tag.startswith(&apos;V&apos;): return wn.VERB elif pos_tag.startswith(&apos;N&apos;): return wn.NOUN elif pos_tag.startswith(&apos;R&apos;): return wn.ADV else: return None tagged_text = tag(text) tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag)) for word, pos_tag in tagged_text] return tagged_lower_text 基于词性标签提取主干词def lemmatize_text(text): pos_tagged_text = pos_tag_text(text) lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag else word for word, pos_tag in pos_tagged_text] lemmatized_text = &apos; &apos;.join(lemmatized_tokens) return lemmatized_text 删除特殊字符def remove_special_characters(text): tokens = tokenize_text(text) pattern = re.compile(&apos;[&#123;&#125;]&apos;.format(re.escape(string.punctuation))) filtered_tokens = filter(None, [pattern.sub(&apos; &apos;, token) for token in tokens]) filtered_text = &apos; &apos;.join(filtered_tokens) return filtered_text 删除停用词def remove_stopwords(text): tokens = tokenize_text(text) filtered_tokens = [token for token in tokens if token not in stopword_list] filtered_text = &apos; &apos;.join(filtered_tokens) return filtered_text 转移HTML标签def unescape_html(parser, text): return parser.unescape(text) 标准化文本(合并执行上面流程)def normalize_corpus(corpus, lemmatize=True, tokenize=False): normalized_corpus = [] for text in corpus: text = html_parser.unescape(text) text = expand_contractions(text, CONTRACTION_MAP) if lemmatize: text = lemmatize_text(text) else: text = text.lower() text = remove_special_characters(text) text = remove_stopwords(text) if tokenize: text = tokenize_text(text) normalized_corpus.append(text) else: normalized_corpus.append(text) return normalized_corpus 文本特征提取 基于词项次数的二值特征 基于词袋模型的频率特征 TF-IDF权重特征 构建特征矩阵binary、frequency、tfidffrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizerdef build_feature_matrix(documents,feature_type=&apos;frequency&apos;): feature_type=feature_type.lower().strip() if feature_type==&apos;binary&apos;: vectorizer=CountVectorizer(binary=True,min_df=1,ngram_range=(1,1)) elif feature_type==&apos;frequency&apos;: vectorizer=CountVectorizer(binary=False,min_df=1,ngram_range=(1,1)) elif feature_type==&apos;tfidf&apos;: vectorizer=TfidfVectorizer(min_df=1,ngram_range=(1,1)) else: raise Exception(&quot;Wrong feature type entered. Possible values: &apos;binary&apos;, &apos;frequency&apos;, &apos;tfidf&apos;&quot;) feature_matrix=vectorizer.fit_transform(documents).astype(float) return vectorizer,feature_matrix 关键短语提取词项搭配from nltk.corpus import gutenbergimport nltkfrom operator import itemgetteralice = gutenberg.sents(fileids=&apos;carroll-alice.txt&apos;)alice = [&apos; &apos;.join(ts) for ts in alice]norm_alice = normalize_corpus(alice, lemmatize=False) 将语料压缩成1个大的文本串def flatten_corpus(corpus): return &apos; &apos;.join([document.strip() for document in corpus]) 计算n元分词（比较巧妙）def compute_ngrams(sequence,n):# print([sequence[index:] for index in range(n)])# print(list(zip(*[sequence[index:] for index in range(n)]))) #解压时仅按最小元素数组数量进行解压 return zip(*[sequence[index:] for index in range(n)]) 获取n元分词def get_top_ngram(corpus,ngram_val=1,limit=5): corpus=flatten_corpus(corpus) tokens=nltk.word_tokenize(corpus) ngrams=compute_ngrams(tokens,ngram_val) #获取单词频率 ngrams_freq_dist=nltk.FreqDist(ngrams) #排序频率 sorted_ngrams_fd=sorted(ngrams_freq_dist.items(),key=itemgetter(1),reverse=True) sorted_ngrams=sorted_ngrams_fd[0:limit] sorted_ngrams=[(&apos; &apos;.join(text),freq) for text,freq in sorted_ngrams] return sorted_ngrams 输出频率前10的二元分词get_top_ngram(corpus=norm_alice,ngram_val=2,limit=10) 输出结果[(&apos;said alice&apos;, 123), (&apos;mock turtle&apos;, 56), (&apos;march hare&apos;, 31), (&apos;said king&apos;, 29), (&apos;thought alice&apos;, 26), (&apos;white rabbit&apos;, 22), (&apos;said hatter&apos;, 22), (&apos;said mock&apos;, 20), (&apos;said caterpillar&apos;, 18), (&apos;said gryphon&apos;, 18)] 频率前10的三元分词get_top_ngram(corpus=norm_alice,ngram_val=3,limit=10) 输出结果 [(&apos;said mock turtle&apos;, 20), (&apos;said march hare&apos;, 9), (&apos;poor little thing&apos;, 6), (&apos;little golden key&apos;, 5), (&apos;certainly said alice&apos;, 5), (&apos;white kid gloves&apos;, 5), (&apos;march hare said&apos;, 5), (&apos;mock turtle said&apos;, 5), (&apos;know said alice&apos;, 4), (&apos;might well say&apos;, 4)] 频率前10的一元分词get_top_ngram(corpus=norm_alice,ngram_val=1,limit=10) 输出结果[(&apos;said&apos;, 462), (&apos;alice&apos;, 398), (&apos;little&apos;, 128), (&apos;one&apos;, 104), (&apos;know&apos;, 88), (&apos;like&apos;, 85), (&apos;would&apos;, 83), (&apos;went&apos;, 83), (&apos;could&apos;, 77), (&apos;queen&apos;, 75)] 使用nltk的搭配查找器二元词项from nltk.collocations import BigramCollocationFinderfrom nltk.collocations import BigramAssocMeasuresfinder=BigramCollocationFinder.from_documents([item.split() for item in norm_alice])bigram_measures=BigramAssocMeasures()#使用原始频率进行查找finder.nbest(bigram_measures.raw_freq,10) 输出结果 [(&apos;said&apos;, &apos;alice&apos;), (&apos;mock&apos;, &apos;turtle&apos;), (&apos;march&apos;, &apos;hare&apos;), (&apos;said&apos;, &apos;king&apos;), (&apos;thought&apos;, &apos;alice&apos;), (&apos;said&apos;, &apos;hatter&apos;), (&apos;white&apos;, &apos;rabbit&apos;), (&apos;said&apos;, &apos;mock&apos;), (&apos;said&apos;, &apos;caterpillar&apos;), (&apos;said&apos;, &apos;gryphon&apos;)] 二元使用点互信息PMI进行查找搭配finder.nbest(bigram_measures.pmi,10) 三元词组from nltk.collocations import TrigramAssocMeasuresfrom nltk.collocations import TrigramCollocationFinderfinder=TrigramCollocationFinder.from_documents([item.split() for item in norm_alice])trigram_measures=TrigramAssocMeasures()#三元组频率finder.nbest(trigram_measures.raw_freq,10) 输出结果[(&apos;said&apos;, &apos;mock&apos;, &apos;turtle&apos;), (&apos;said&apos;, &apos;march&apos;, &apos;hare&apos;), (&apos;poor&apos;, &apos;little&apos;, &apos;thing&apos;), (&apos;little&apos;, &apos;golden&apos;, &apos;key&apos;), (&apos;march&apos;, &apos;hare&apos;, &apos;said&apos;), (&apos;mock&apos;, &apos;turtle&apos;, &apos;said&apos;), (&apos;white&apos;, &apos;kid&apos;, &apos;gloves&apos;), (&apos;beau&apos;, &apos;ootiful&apos;, &apos;soo&apos;), (&apos;certainly&apos;, &apos;said&apos;, &apos;alice&apos;), (&apos;might&apos;, &apos;well&apos;, &apos;say&apos;)] 三元使用点互信息PMI进行查找搭配finder.nbest(trigram_measures.pmi,10) 输出结果 [(&apos;accustomed&apos;, &apos;usurpation&apos;, &apos;conquest&apos;), (&apos;adjourn&apos;, &apos;immediate&apos;, &apos;adoption&apos;), (&apos;adoption&apos;, &apos;energetic&apos;, &apos;remedies&apos;), (&apos;ancient&apos;, &apos;modern&apos;, &apos;seaography&apos;), (&apos;apple&apos;, &apos;roast&apos;, &apos;turkey&apos;), (&apos;arithmetic&apos;, &apos;ambition&apos;, &apos;distraction&apos;), (&apos;brother&apos;, &apos;latin&apos;, &apos;grammar&apos;), (&apos;canvas&apos;, &apos;bag&apos;, &apos;tied&apos;), (&apos;cherry&apos;, &apos;tart&apos;, &apos;custard&apos;), (&apos;circle&apos;, &apos;exact&apos;, &apos;shape&apos;)] 基于权重标签的短语提取 使用浅层分析提取所有名词短语词块 计算每个词块的TF-IDF权重并返回最大加权短语 toy_text = &quot;&quot;&quot;Elephants are large mammals of the family Elephantidae and the order Proboscidea. Two species are traditionally recognised, the African elephant and the Asian elephant. Elephants are scattered throughout sub-Saharan Africa, South Asia, and Southeast Asia. Male African elephants are the largest extant terrestrial animals. All elephants have a long trunk used for many purposes, particularly breathing, lifting water and grasping objects. Their incisors grow into tusks, which can serve as weapons and as tools for moving objects and digging. Elephants&apos; large ear flaps help to control their body temperature. Their pillar-like legs can carry their great weight. African elephants have larger ears and concave backs while Asian elephants have smaller ears and convex or level backs. &quot;&quot;&quot; import numpy as npimport itertoolsfrom gensim import corpora, models 基本上，我们有一个已定义的语法模式来分块或提取名词短语。我们在同一模式中定义一个分块器，对于文档中的每个句子，首先用它的POS标签来标注它(因此，不应该对文本进行规范化)，然后构建一个具有名词短语的浅层分析树作为词块和其他全部基于POS标签的单词作为缝隙，缝隙是不属于任何词块的部分。完成此操作后，我们使用tree2conl1tags函数来生成(w, t，c)三元组，它们是的单词、POS标签和IOB格式的词块标签。删除所有词块带有’O ‘标签的标签，因为它们基本上是不属于任何词块的单词或词项。最后，从这些有效的词块中，组合分块的词项，并从每个词块分组中生成短语。 提取文档中的名词短语 v adj adv#提取文档中的名词短语 v adj advdef get_chunks(sentences, grammar = r&apos;NP: &#123;&lt;DT&gt;? &lt;JJ&gt;* &lt;NN.*&gt;+&#125;&apos;): all_chunks = [] chunker = nltk.chunk.regexp.RegexpParser(grammar) for sentence in sentences: tagged_sents = nltk.pos_tag_sents( [nltk.word_tokenize(sentence)]) chunks = [chunker.parse(tagged_sent) for tagged_sent in tagged_sents] wtc_sents = [nltk.chunk.tree2conlltags(chunk) for chunk in chunks] flattened_chunks = list( itertools.chain.from_iterable( wtc_sent for wtc_sent in wtc_sents) )# print(flattened_chunks)# print(flattened_chunks) valid_chunks_tagged = [(status, [wtc for wtc in chunk]) for status, chunk in itertools.groupby(flattened_chunks,lambda chunk: chunk != &apos;O&apos;)]# print(&apos;---&apos;*20)# print(valid_chunks_tagged) valid_chunks = [&apos; &apos;.join(word.lower() for word, tag, chunk in wtc_group if word.lower() not in stopword_list) for status, wtc_group in valid_chunks_tagged if status] all_chunks.append(valid_chunks) return all_chunks sentences = parse_document(toy_text) valid_chunks = get_chunks(sentences) 获取TF-IDF关键短语权重def get_tfidf_weighted_keyphrases(sentences, grammar=r&apos;NP: &#123;&lt;DT&gt;? &lt;JJ&gt;* &lt;NN.*&gt;+&#125;&apos;, top_n=10): valid_chunks = get_chunks(sentences, grammar=grammar) dictionary = corpora.Dictionary(valid_chunks) corpus = [dictionary.doc2bow(chunk) for chunk in valid_chunks] tfidf = models.TfidfModel(corpus) corpus_tfidf = tfidf[corpus] weighted_phrases = &#123;dictionary.get(id): round(value,3) for doc in corpus_tfidf for id, value in doc&#125; weighted_phrases = sorted(weighted_phrases.items(), key=itemgetter(1), reverse=True) return weighted_phrases[:top_n] 前两个关键短语get_tfidf_weighted_keyphrases(sentences, top_n=2) 输出结果 [(&apos;elephants large mammals family elephantidae order proboscidea .&apos;, 1.0), (&apos;two species traditionally recognised , african elephant asian elephant .&apos;, 1.0)] 其他语料实验get_tfidf_weighted_keyphrases(alice, top_n=10) 输出结果 [(&quot;[ alice &apos; adventures wonderland lewis carroll 1865 ]&quot;, 1.0), (&apos;chapter .&apos;, 1.0), (&apos;rabbit - hole&apos;, 1.0), (&quot;alice beginning get tired sitting sister bank , nothing : twice peeped book sister reading , pictures conversations , &apos; use book , &apos; thought alice &apos; without pictures conversation ? &apos;&quot;, 1.0), (&apos;considering mind ( well could , hot day made feel sleepy stupid ) , whether pleasure making daisy - chain would worth trouble getting picking daisies , suddenly white rabbit pink eyes ran close .&apos;, 1.0), (&quot;nothing remarkable ; alice think much way hear rabbit say , &apos; oh dear !&quot;, 1.0), (&apos;oh dear !&apos;, 1.0), (&quot;shall late ! &apos;&quot;, 1.0), (&apos;( thought afterwards , occurred ought wondered , time seemed quite natural ) ; rabbit actually took watch waistcoat - pocket , looked , hurried , alice started feet , flashed across mind never seen rabbit either waistcoat - pocket , watch take , burning curiosity , ran across field , fortunately time see pop large rabbit - hole hedge .&apos;, 1.0), (&apos;another moment went alice , never considering world get .&apos;, 1.0)]","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"关键词抽取","slug":"关键词抽取","permalink":"http://renxingkai.github.io/tags/关键词抽取/"}],"author":"CinKate"},{"title":"Word2Vec相关(用TFIDF加权词向量)","slug":"word-tfidf","date":"2019-04-05T10:34:06.000Z","updated":"2020-05-17T16:12:00.563Z","comments":true,"path":"2019/04/05/word-tfidf/","link":"","permalink":"http://renxingkai.github.io/2019/04/05/word-tfidf/","excerpt":"","text":"今天是快乐的清明节，而博主还在实验室敲代码，23333这次记录下Word2Vec相关的姿势~ Word2Vec模型直接用开源的gensism库进行词向量训练： import gensimimport nltkimport numpy as np#自制语料CORPUS = [&apos;the sky is blue&apos;,&apos;sky is blue and sky is beautiful&apos;,&apos;the beautiful sky is so blue&apos;,&apos;i love blue cheese&apos;]new_doc = [&apos;loving this blue sky today&apos;] 对语料进行分词 #tokenize corpusTOKENIZED_CORPUS=[nltk.word_tokenize(sentence) for sentence in CORPUS]tokenized_new_doc=[nltk.word_tokenize(sentence) for sentence in new_doc]print(TOKENIZED_CORPUS)print(tokenized_new_doc) 输出 [[&apos;the&apos;, &apos;sky&apos;, &apos;is&apos;, &apos;blue&apos;], [&apos;sky&apos;, &apos;is&apos;, &apos;blue&apos;, &apos;and&apos;, &apos;sky&apos;, &apos;is&apos;, &apos;beautiful&apos;], [&apos;the&apos;, &apos;beautiful&apos;, &apos;sky&apos;, &apos;is&apos;, &apos;so&apos;, &apos;blue&apos;], [&apos;i&apos;, &apos;love&apos;, &apos;blue&apos;, &apos;cheese&apos;]] [[&apos;loving&apos;, &apos;this&apos;, &apos;blue&apos;, &apos;sky&apos;, &apos;today&apos;]] 构建词向量 model=gensim.models.Word2Vec(TOKENIZED_CORPUS,size=10,window=10,min_count=2,sample=1e-3) 平均词向量来表示文档 #num_features表示的文本单词大小def average_word_vectors(words,model,vocabulary,num_features): feature_vector=np.zeros((num_features,),dtype=&apos;float64&apos;) nwords=0 for word in words: if word in vocabulary: nwords=nwords+1 feature_vector=np.add(feature_vector,model[word]) if nwords: feature_vector=np.divide(feature_vector,nwords) return feature_vectordef averaged_word_vectorizer(corpus,model,num_features): #get the all vocabulary vocabulary=set(model.wv.index2word) features=[average_word_vectors(tokenized_sentence,model,vocabulary,num_features) for tokenized_sentence in corpus] return np.array(features) avg_word_vec_features=averaged_word_vectorizer(TOKENIZED_CORPUS,model=model,num_features=10)print(avg_word_vec_features) 输出array([[-0.00710545, -0.01549264, 0.02188712, -0.00322829, 0.00586532, -0.00687592, 0.00339291, -0.01177494, 0.00265543, -0.00539964], [-0.0157312 , -0.01630003, 0.00551589, 0.00166568, 0.02385859, 0.0085727 , 0.02538068, -0.02266891, 0.02231819, -0.02521743], [-0.0070758 , -0.00578274, 0.01280785, -0.00960104, 0.00821758, -0.00023592, 0.01009926, -0.00624976, 0.00913788, -0.01323305], [ 0.01571231, -0.02214988, 0.02293927, -0.03584988, -0.02027377, 0.00031135, 0.00284845, 0.01365358, 0.00845861, -0.0247597 ]]) nd_avg_word_vec_features=averaged_word_vectorizer(corpus=tokenized_new_doc,model=model,num_features=10)print(nd_avg_word_vec_features) 输出array([[-0.00968785, -0.02889012, 0.02670473, -0.01596956, 0.00815679, -0.00325876, 0.02226594, -0.01347479, 0.01384218, -0.01042995]]) # TF-IDF加权平均词向量如果直接求平均效果不好的话，或者过于简单的话，可以对词求TFIDF，然后乘以相应的权重 def tfidf_wtd_avg_word_vectors(words,tfidf_vector,tfidf_vocabulary,model,num_features): word_tfidfs=[tfidf_vector[0,tfidf_vocabulary.get(word)] if tfidf_vocabulary.get(word) else 0 for word in words] word_tfidf_map=&#123;word:tfidf_val for word,tfidf_val in zip(words,word_tfidfs)&#125; feature_vector=np.zeros((num_features,),dtype=&apos;float64&apos;) vocabulary=set(model.wv.index2word) wts=0 for word in words: if word in vocabulary: word_vector=model[word] weighted_word_vector=word_tfidf_map[word]*word_vector wts=wts+word_tfidf_map[word] feature_vector=np.add(feature_vector,weighted_word_vector) if wts: feature_vector=np.divide(feature_vector,wts) return feature_vectordef tfidf_weighted_averaged_word_vectorizer(corpus,tfidf_vectors,tfidf_vocabulary,model,num_features): docs_tfidfs=[(doc,doc_tfidf) for doc,doc_tfidf in zip(corpus,tfidf_vectors)] features=[tfidf_wtd_avg_word_vectors(tokenized_sentence,tfidf,tfidf_vocabulary,model,num_features) for tokenized_sentence,tfidf in docs_tfidfs] return np.array(features) TFIDF预处理from sklearn.feature_extraction.text import TfidfTransformerfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.feature_extraction.text import CountVectorizerimport pandas as pddef tfidf_transformer(bow_matrix): transformer = TfidfTransformer(norm=&apos;l2&apos;, smooth_idf=True, use_idf=True) tfidf_matrix = transformer.fit_transform(bow_matrix) return transformer, tfidf_matrixdef tfidf_extractor(corpus, ngram_range=(1,1)): vectorizer = TfidfVectorizer(min_df=1, norm=&apos;l2&apos;, smooth_idf=True, use_idf=True, ngram_range=ngram_range) features = vectorizer.fit_transform(corpus) return vectorizer, featuresdef bow_extractor(corpus, ngram_range=(1,1)): #min_df为1说明文档中词频最小为1也会被考虑 #ngram_range可以设置(1,3)将建立包括所有unigram、bigram、trigram的向量空间 vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range) features = vectorizer.fit_transform(corpus) return vectorizer, featuresdef display_features(features, feature_names): df = pd.DataFrame(data=features, columns=feature_names) print(df) bow_vectorizer, bow_features = bow_extractor(CORPUS)feature_names = bow_vectorizer.get_feature_names()tfidf_trans, tfidf_features = tfidf_transformer(bow_features)tfidf_vectorizer, tdidf_features = tfidf_extractor(CORPUS)display_features(np.round(tdidf_features.todense(), 2), feature_names)nd_tfidf = tfidf_vectorizer.transform(new_doc)display_features(np.round(nd_tfidf.todense(), 2), feature_names) TFIDF加权词向量corpus_tfidf=tfidf_featuresvocab=tfidf_vectorizer.vocabulary_ wt_tfidf_word_vec_features=tfidf_weighted_averaged_word_vectorizer(corpus=TOKENIZED_CORPUS,tfidf_vectors=corpus_tfidf,tfidf_vocabulary=vocab,model=model,num_features=10)print(wt_tfidf_word_vec_features) 输出array([[-0.00728862, -0.01345045, 0.02334223, -0.00258989, 0.00500905, -0.00913428, 0.00057808, -0.01095917, -0.00025702, -0.00165257], [-0.02009719, -0.01936696, 0.0056747 , 0.00887485, 0.02952368, 0.00819392, 0.02715274, -0.0298718 , 0.02297843, -0.0237992 ], [-0.00721121, -0.00258696, 0.01239834, -0.01018197, 0.00795635, -0.00085167, 0.00906817, -0.00469667, 0.00799437, -0.01167674], [ 0.01571231, -0.02214988, 0.02293927, -0.03584988, -0.02027377, 0.00031135, 0.00284845, 0.01365358, 0.0084586 , -0.0247597 ]]) nd_wt_tfidf_word_vec_features=tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_new_doc,tfidf_vectors=nd_tfidf,tfidf_vocabulary=vocab,model=model,num_features=10)print(nd_wt_tfidf_word_vec_features) 输出 array([[-0.01223734, -0.02956665, 0.02708268, -0.01397412, 0.01101045, -0.00361711, 0.02421493, -0.01619775, 0.01438254, -0.00899163]])","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"词向量相关","slug":"词向量相关","permalink":"http://renxingkai.github.io/tags/词向量相关/"}],"author":"CinKate"},{"title":"《基于BiDAF多文档重排序的阅读理解模型》论文阅读","slug":"rbidaf","date":"2019-04-02T18:58:21.000Z","updated":"2020-05-17T16:12:00.222Z","comments":true,"path":"2019/04/03/rbidaf/","link":"","permalink":"http://renxingkai.github.io/2019/04/03/rbidaf/","excerpt":"","text":"0 引言目前的机器学习方法主要有两类：抽取式和生成式，抽取式通过给定问题以及相关的文章进行训练,让机器具备阅读的能力，并对提出的新问题,在相关文章中抽取出相应的答案。另一种是生成式,从理论_上来说不受知识的局限,对于问题自动生成答案,但是生成式有时产生的答案答非所问，句式不通,不能很好地体现出人类的思维逻辑以及自然表述的特点。 在本文中，提出了一种基于BiDAF模型的RBiDAF机器阅读理解模型。该模型是一-种抽取式的机器阅读理解模型,在BiDAF模型四层网络框架的基础，上添加了ParaRanking层，针对ParaRank-ing,本文提出了多特征融合的ParaRanking 算法，此外本文还在答案预测层,提出了基于先验知识的多答案交叉验证算法,进而对答案进行综合预测。 1 机器阅读理解和相关工作自斯坦福机器阅读理解数据集SQuAD问世以来,经过谷歌、微软、百度、科大讯飞、腾讯、斯坦福大学等在内的众多研究机构的不懈努力,形成了“词向量化-语义编码-语义交互-答案预测”这样一-套四层机器阅读理解模型体系。该体系的主要思想是:首先将自然文本表示为可计算的向量,其次融合问题向量与支撑文档向量来学习到语义交互信息,最后根据交互信息预测答案的位置或逐一输出最大概率的字词来生成答案。 词向量化层(Word-Embedder) 的作用是使用词向量技术将分词后的自然文本转化为稠密、连续且可计算的低维向量。 文本编码层(Encoder)的作用是进行语义加工，向量化层输出的结果是一串独立的词向量序列,而编码层根据这些词向量捕捉词与词的前后语义关系,并把这种语义关系融入词向量中,生成一串互相关联的文本编码序列。 语义交互层（Interaction-Layer)是整个模型体系中最重要的一环。在进入这一层之前，问题与给定支撑文档在大多数情况下是分别独立进行向量转化与语义编码的。当然，在有些模型中，问题词向量序列也会被提前融合到文档向量中。当前大部分研究工作集中在语义交互层的设计上，在这一层，将最终得到混合两者语义的交互向量。此外，交互向量也时常与未交互前的问题编码向量直接拼接，以强调问题语义的重要性。 答案预测层（Answer-Layer)负责根据语义交互向量产出 最终的答案。目前，答案预测模型主要是生成模型与边界模型（边界模型常用的有Pointer Network指出答案所在的开始、结束位置）。答案预测层将答案均视作一串词序列，生成模型逐个预测该序列中每个词应使用给定文档中哪个词进行填充，每次预测均基于之前预测完成的结果。边界模型则相当于一个简化的生成模型，其预先假定问题都可以使用给定文档中的一个连续短语或句子进行回答，因此只需预测答案的起点与终点词的位置即可。目前，边界模型的预测效率与结果均好于生成模型。 以下作者对比了近年来在SQuAD榜上的部分阅读理解模型： Match-LSTM(2016提出， 发表于ICLR 17)Match-LSTM是首个应用于SQuAD数据的端到端机器阅读理解模型，并成功超越原有使用人工特征进行答案抽取的基线模型。该模型的特点是：（1）在文本编码层使用单向LSTM进行语义建模；（2）在语义交互层对支撑文档中的每个词计算该词在问题编码向量上的注意力分配向量，将这一注意力分配向量与问题编码向量点乘获得文档词–问题交互向量，并再拼接上文档词编码向量，最后用一个新的单向LSTM网络对拼接后的向量进行二次语义编码；（3)用反向LSTM重复（1)、（2)操作，并将正反向二次语义编码向量拼接。 BIDAF(2016提出， 发表于ICLR 17)BIDAF可以视作对Match-LSTM匹配模型的改进。其主要变化在于：（1)词向量化层增加了对字的向量化；（2)在语义交互 充了问题中每个词在文档编码向量上的注意力分配向量， 以提升文档词-问题交互向量的语义交互程度；（3）改用双向LSTM网络二次语义编码。与单向LSTM相比，双向LSTM可以同时提取到每个词相关的上下文信息。 R-Net(发表于ACL 17)R-Net是对Match-LSTM匹配模型的改进。这一模型最大的特点是采用了双语义交互层设计。在一级语义交互层，R-Net仿照Match-LSTM实现将问题信息融入到每个文档词中去；而在二级语义交互层，R-Net则使用相同办法将已经获得的文档词–问题语义编码向量再度与问题编码向量二次融合，进一步加强语义匹配。 QANet(发表于ICLR 18)QANet则是一种在BIDAF模型基础上为追求效率而设计的模型。该模型非常创新地在文本编码层使用CNN与Multi-Head Self-Attention机制实现语义编码，由于CNN可以捕捉局部特征、Self-Attention能够捕捉全局特征，因此完全可以用它们替代传统的LSTM网络。此外，由于CNN的建模效率显著高于LSTM网络，该模型以在更大规模的数据集上进行深度学习——泛化能力得到了进一步提升。这一模型可以在SQuAD数据集上达到训练速度提高3〜13倍！推理速度提高4~9倍，且获得与先前基于LSTM网络媲美的精度。 V-net(百度公司发表于ACL 18)V-net是一种新的多文档校验深度神经网络建模方法，该模型通过注意力使不同候选文档抽取的答案能够互相印证，从而预测出更好的答案。 2 数据探索和数据处理百度数据集与其他数据集很大的区别在于，每篇文章中包含了很多个段落，而SQuAD数据集的支撑文档直接是一个最相关段落，微软数据集MS MARCO则是若干篇只有一个段落的文章。因此，在百度机器阅读理解任务中，需要在主流四层体系的基础上，增加一个段落定位层。 在DuReader原文中提到，使用recall指标增加的段落定位层，并使用recall指标进行段落选择，可以使模型的效果至少10% 3 RBiDAF模型设计与实现本文提出的基于BiDAF模型的RBiDAF模型，主要是在BiDAF模型的基础上添加了ParaRanking，在该层提出了ParaRanking算法，从而对候选段落进行排序（ParaRanking)操作，进而筛选出含答案概率更高的候选段落。 此外在答案预测层，提出了基于先验知识的多答案交叉验证（MACVerify)算法，从而对答案进行综合预测。 3.1 ParaRanking算法DuReader数据集中，每一个问题对应多个段落,尤其是在Search数据集中，问题和段落的比接近1:57,所以应该尽量检索出含有答案的段落,从而减小候选段落集的数据规模。在这里本文提出了多特征融合的ParaRanking算法,图8是ParaRanking算法的大体架构,主要包括段落过滤、段落重组、语义匹配、最大覆盖度、特征加权以及多文档投票。 3.1.1 段落过滤 本文利用特征工程根据问题类型对不相关段落进行过滤,例如,实体类型的问题中,问题中的关键词是“联系方式”、“热线”，那么本文利用正则表达式将不含电话号码的段落进行过滤，最终本文设计了23条规则对段落进行初步过滤。 3.1.2段落重组 DuReader数据集中的段落长度极度不平衡,有些段落的长度很短,这种情况会造成段落的上下文信息缺失，不利于模型的Match操作。而且本文通过观察训练集中答案的分布，发现有些答案是跨段落的，尤其是描述类的问题，所以如果仅仅以某-一个原始段落作为预测的输人，那么将无法解决答案跨段落的问题,因此本文将原始的段落进行重组，重组后长度控制在长度splice_ L之内。 3.1.3语义匹配 问题(question)与段落(paragraph)间的匹配不仅要考虑问题和段落之间的显式关系,还要考虑两者之间的隐式关系，即两者之间的语义关系。例如，question:北京2017年的商业住房的均价是多少?paragraph:据我所知是四万元一平。上例question和paragraph之间的最大覆盖度虽然为0,但是两者之间具有极大的语义相关性,并且“四万元一平”极有可能是答案。所以为了克服字词匹配上的弊端，本文选择利用深度神经网络计算question和para-graph之间的语义相关性。 由于ARC-II保留了词序信息,更具一般性，所以本文采用ARC-II文本匹配模型对question以及paragraph之间的语义相关度进行计算,在第一层中,首先把卷积窗口设定为k1,然后对句子Squestion和句子Sprangraph中所有组合的二维矩阵进行卷积,每一个二维矩阵输出一个值(文中把这个称作一维卷积,因为实际上是把组合中所有词语的vector排成一行进行的卷积计算),构成Layer-2,然后进行2X2的MaxPooling。后续的卷积层均是传统的二维卷积操作，与第一层卷积层后的简单MaxPooling方式不同,后续的卷积层的Pooling是一种动态Pooling方法。输出固定维度的向量,接着输人MLP层,最终得到文本相似度分数ps。 3.1.4 最大覆盖度 本文沿用了基线模型的最大覆盖度算法,DuReader的基线模型采用问题和段落的最大词级别的覆盖度算法对段落进行排序,然后对每一个篇章挑选top-1作为模型的输入,本文将问题与段落的最大覆盖度作为ParaRanking的一个重要特征值定义为pc,其中不同于基线模型中最大覆盖度算法的是,这里分别选择了词和字两个粒度进行最大覆盖度计算,两者相加作为最终pc的值。 3.1.5 特征加权 首先通过分析DuReader的训练集可知,在描述类问题的答案中存在大量列表类型的答案，所以本文针对描述类问题识别出段落中的列表信息,并根据这一特征对段落的ParaRanking值进行加权，定义权值为B。经过语义匹配、最大覆盖度计算以及特征加权可以得到问题和段落i的最终匹配得分，如式下式所示。 3.1.6 多文档投票 本文两次用到多文档投票，一次在ParaRanking操作中，一次在答案预测中，前后两次所用到的方法有些不同。使用多文档投票是基于某一问题的正确答案在多个段落中会多次出现这一假设。首先 定义候选段落集合为Dp，对于段落i属于Dp，那么每一个段落的投票得分如下式所示。 所以最终得分段落，的最终得分为: 其中，f函数是指数平滑函数，最终经过ParaRanking 算法 ，每一个段落,i(属于Dp)会生成一个分score，随后根据score选择输人模型的段落集合Df，并且Df数量远远小于Dp。 RBiDAF模型架构 5 总结与展望本文提出了一种基于BiDAF模型的RBiDAF机器阅读理解模型。首先对DuReader数据集进行分析并对数据进行清洗,从而提取出有利于模型训练的特征;然后本文对RBiDAF机器阅读理解模型进行相关设计和实现,该模型的创新点在于在BiDAF模型四层网络框架的基础上添加了ParaRanking层,在该层,本文提出了基于多特征融合的ParaRanking算法。此外本文还在答案预测层,提出了基于先验知识的MACVerify算法,利用该算法对答案进行综合预测。最后经过实验和分析,RBiDAF模型能够产生有效的答案。在未来的工作中，首先将尝试实验多种词嵌入方法,很多学者证实选择合适的词嵌人方法对该任务会产生很大的影响;其次尝试采用机器翻译模型与对抗式生成模型(GAN)增强训练语料;最后在文本交互层融合双向注意力(Bi-Attention)与多轮匹配机制(Multi-Matching),从而可以在多文档场景下取得更好的效果。 论文地址","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/categories/深度学习/"}],"tags":[{"name":"阅读理解","slug":"阅读理解","permalink":"http://renxingkai.github.io/tags/阅读理解/"},{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/tags/深度学习/"}],"author":"CinKate"},{"title":"英文文本预处理代码","slug":"textpreprocess","date":"2019-03-29T21:50:08.000Z","updated":"2020-05-17T16:12:00.814Z","comments":true,"path":"2019/03/30/textpreprocess/","link":"","permalink":"http://renxingkai.github.io/2019/03/30/textpreprocess/","excerpt":"","text":"贴一段在做Kaggle QIQC时别人开源的kernel英语文本预处理代码，在做英文nlp任务时还是很有用的~ import osimport reimport gcimport stringimport unicodedataimport operatorimport numpy as npimport pandas as pdfrom tqdm import tqdmtqdm.pandas()&quot;&quot;&quot;utils&quot;&quot;&quot;def load_data(datapath): print(&quot;loading data ......&quot;) df_train = pd.read_csv(os.path.join(datapath, &quot;train.csv&quot;)) df_test = pd.read_csv(os.path.join(datapath, &quot;test.csv&quot;)) print(&quot;train data with shape : &quot;, df_train.shape) print(&quot;test data with shape : &quot;, df_test.shape) return df_train, df_test&quot;&quot;&quot;nlp&quot;&quot;&quot;def clean_misspell(text): &quot;&quot;&quot; misspell list (quora vs. glove) &quot;&quot;&quot; misspell_to_sub = &#123; &apos;Terroristan&apos;: &apos;terrorist Pakistan&apos;, &apos;terroristan&apos;: &apos;terrorist Pakistan&apos;, &apos;BIMARU&apos;: &apos;Bihar, Madhya Pradesh, Rajasthan, Uttar Pradesh&apos;, &apos;Hinduphobic&apos;: &apos;Hindu phobic&apos;, &apos;hinduphobic&apos;: &apos;Hindu phobic&apos;, &apos;Hinduphobia&apos;: &apos;Hindu phobic&apos;, &apos;hinduphobia&apos;: &apos;Hindu phobic&apos;, &apos;Babchenko&apos;: &apos;Arkady Arkadyevich Babchenko faked death&apos;, &apos;Boshniaks&apos;: &apos;Bosniaks&apos;, &apos;Dravidanadu&apos;: &apos;Dravida Nadu&apos;, &apos;mysoginists&apos;: &apos;misogynists&apos;, &apos;MGTOWS&apos;: &apos;Men Going Their Own Way&apos;, &apos;mongloid&apos;: &apos;Mongoloid&apos;, &apos;unsincere&apos;: &apos;insincere&apos;, &apos;meninism&apos;: &apos;male feminism&apos;, &apos;jewplicate&apos;: &apos;jewish replicate&apos;, &apos;unoin&apos;: &apos;Union&apos;, &apos;daesh&apos;: &apos;Islamic State of Iraq and the Levant&apos;, &apos;Kalergi&apos;: &apos;Coudenhove-Kalergi&apos;, &apos;Bhakts&apos;: &apos;Bhakt&apos;, &apos;bhakts&apos;: &apos;Bhakt&apos;, &apos;Tambrahms&apos;: &apos;Tamil Brahmin&apos;, &apos;Pahul&apos;: &apos;Amrit Sanskar&apos;, &apos;SJW&apos;: &apos;social justice warrior&apos;, &apos;SJWs&apos;: &apos;social justice warrior&apos;, &apos; incel&apos;: &apos; involuntary celibates&apos;, &apos; incels&apos;: &apos; involuntary celibates&apos;, &apos;emiratis&apos;: &apos;Emiratis&apos;, &apos;weatern&apos;: &apos;western&apos;, &apos;westernise&apos;: &apos;westernize&apos;, &apos;Pizzagate&apos;: &apos;Pizzagate conspiracy theory&apos;, &apos;naïve&apos;: &apos;naive&apos;, &apos;Skripal&apos;: &apos;Sergei Skripal&apos;, &apos;Remainers&apos;: &apos;British remainer&apos;, &apos;remainers&apos;: &apos;British remainer&apos;, &apos;bremainer&apos;: &apos;British remainer&apos;, &apos;antibrahmin&apos;: &apos;anti Brahminism&apos;, &apos;HYPSM&apos;: &apos; Harvard, Yale, Princeton, Stanford, MIT&apos;, &apos;HYPS&apos;: &apos; Harvard, Yale, Princeton, Stanford&apos;, &apos;kompromat&apos;: &apos;compromising material&apos;, &apos;Tharki&apos;: &apos;pervert&apos;, &apos;tharki&apos;: &apos;pervert&apos;, &apos;mastuburate&apos;: &apos;masturbate&apos;, &apos;Zoë&apos;: &apos;Zoe&apos;, &apos;indans&apos;: &apos;Indian&apos;, &apos; xender&apos;: &apos; gender&apos;, &apos;Naxali &apos;: &apos;Naxalite &apos;, &apos;Naxalities&apos;: &apos;Naxalites&apos;, &apos;Bathla&apos;: &apos;Namit Bathla&apos;, &apos;Mewani&apos;: &apos;Indian politician Jignesh Mevani&apos;, &apos;clichéd&apos;: &apos;cliche&apos;, &apos;cliché&apos;: &apos;cliche&apos;, &apos;clichés&apos;: &apos;cliche&apos;, &apos;Wjy&apos;: &apos;Why&apos;, &apos;Fadnavis&apos;: &apos;Indian politician Devendra Fadnavis&apos;, &apos;Awadesh&apos;: &apos;Indian engineer Awdhesh Singh&apos;, &apos;Awdhesh&apos;: &apos;Indian engineer Awdhesh Singh&apos;, &apos;Khalistanis&apos;: &apos;Sikh separatist movement&apos;, &apos;madheshi&apos;: &apos;Madheshi&apos;, &apos;BNBR&apos;: &apos;Be Nice, Be Respectful&apos;, &apos;Bolsonaro&apos;: &apos;Jair Bolsonaro&apos;, &apos;XXXTentacion&apos;: &apos;Tentacion&apos;, &apos;Padmavat&apos;: &apos;Indian Movie Padmaavat&apos;, &apos;Žižek&apos;: &apos;Slovenian philosopher Slavoj Žižek&apos;, &apos;Adityanath&apos;: &apos;Indian monk Yogi Adityanath&apos;, &apos;Brexit&apos;: &apos;British Exit&apos;, &apos;Brexiter&apos;: &apos;British Exit supporter&apos;, &apos;Brexiters&apos;: &apos;British Exit supporters&apos;, &apos;Brexiteer&apos;: &apos;British Exit supporter&apos;, &apos;Brexiteers&apos;: &apos;British Exit supporters&apos;, &apos;Brexiting&apos;: &apos;British Exit&apos;, &apos;Brexitosis&apos;: &apos;British Exit disorder&apos;, &apos;brexit&apos;: &apos;British Exit&apos;, &apos;brexiters&apos;: &apos;British Exit supporters&apos;, &apos;jallikattu&apos;: &apos;Jallikattu&apos;, &apos;fortnite&apos;: &apos;Fortnite &apos;, &apos;Swachh&apos;: &apos;Swachh Bharat mission campaign &apos;, &apos;Quorans&apos;: &apos;Quoran&apos;, &apos;Qoura &apos;: &apos;Quora &apos;, &apos;quoras&apos;: &apos;Quora&apos;, &apos;Quroa&apos;: &apos;Quora&apos;, &apos;QUORA&apos;: &apos;Quora&apos;, &apos;narcissit&apos;: &apos;narcissist&apos;, # extra in sample &apos;Doklam&apos;: &apos;Tibet&apos;, &apos;Drumpf &apos;: &apos;Donald Trump fool &apos;, &apos;Drumpfs&apos;: &apos;Donald Trump fools&apos;, &apos;Strzok&apos;: &apos;Hillary Clinton scandal&apos;, &apos;rohingya&apos;: &apos;Rohingya &apos;, &apos;wumao &apos;: &apos;cheap Chinese stuff&apos;, &apos;wumaos&apos;: &apos;cheap Chinese stuff&apos;, &apos;Sanghis&apos;: &apos;Sanghi&apos;, &apos;Tamilans&apos;: &apos;Tamils&apos;, &apos;biharis&apos;: &apos;Biharis&apos;, &apos;Rejuvalex&apos;: &apos;hair growth formula&apos;, &apos;Feku&apos;: &apos;The Man of India &apos;, &apos;deplorables&apos;: &apos;deplorable&apos;, &apos;muhajirs&apos;: &apos;Muslim immigrant&apos;, &apos;Gujratis&apos;: &apos;Gujarati&apos;, &apos;Chutiya&apos;: &apos;Tibet people &apos;, &apos;Chutiyas&apos;: &apos;Tibet people &apos;, &apos;thighing&apos;: &apos;masturbate&apos;, &apos;卐&apos;: &apos;Nazi Germany&apos;, &apos;Pribumi&apos;: &apos;Native Indonesian&apos;, &apos;Gurmehar&apos;: &apos;Gurmehar Kaur Indian student activist&apos;, &apos;Novichok&apos;: &apos;Soviet Union agents&apos;, &apos;Khazari&apos;: &apos;Khazars&apos;, &apos;Demonetization&apos;: &apos;demonetization&apos;, &apos;demonetisation&apos;: &apos;demonetization&apos;, &apos;demonitisation&apos;: &apos;demonetization&apos;, &apos;demonitization&apos;: &apos;demonetization&apos;, &apos;demonetisation&apos;: &apos;demonetization&apos;, &apos;cryptocurrencies&apos;: &apos;cryptocurrency&apos;, &apos;Hindians&apos;: &apos;North Indian who hate British&apos;, &apos;vaxxer&apos;: &apos;vocal nationalist &apos;, &apos;remoaner&apos;: &apos;remainer &apos;, &apos;bremoaner&apos;: &apos;British remainer &apos;, &apos;Jewism&apos;: &apos;Judaism&apos;, &apos;Eroupian&apos;: &apos;European&apos;, &apos;WMAF&apos;: &apos;White male married Asian female&apos;, &apos;moeslim&apos;: &apos;Muslim&apos;, &apos;cishet&apos;: &apos;cisgender and heterosexual person&apos;, &apos;Eurocentric&apos;: &apos;Eurocentrism &apos;, &apos;Jewdar&apos;: &apos;Jew dar&apos;, &apos;Asifa&apos;: &apos;abduction, rape, murder case &apos;, &apos;marathis&apos;: &apos;Marathi&apos;, &apos;Trumpanzees&apos;: &apos;Trump chimpanzee fool&apos;, &apos;Crimean&apos;: &apos;Crimea people &apos;, &apos;atrracted&apos;: &apos;attract&apos;, &apos;LGBT&apos;: &apos;lesbian, gay, bisexual, transgender&apos;, &apos;Boshniak&apos;: &apos;Bosniaks &apos;, &apos;Myeshia&apos;: &apos;widow of Green Beret killed in Niger&apos;, &apos;demcoratic&apos;: &apos;Democratic&apos;, &apos;raaping&apos;: &apos;rape&apos;, &apos;Dönmeh&apos;: &apos;Islam&apos;, &apos;feminazism&apos;: &apos;feminism nazi&apos;, &apos;langague&apos;: &apos;language&apos;, &apos;Hongkongese&apos;: &apos;HongKong people&apos;, &apos;hongkongese&apos;: &apos;HongKong people&apos;, &apos;Kashmirians&apos;: &apos;Kashmirian&apos;, &apos;Chodu&apos;: &apos;fucker&apos;, &apos;penish&apos;: &apos;penis&apos;, &apos;micropenis&apos;: &apos;tiny penis&apos;, &apos;Madridiots&apos;: &apos;Real Madrid idiot supporters&apos;, &apos;Ambedkarite&apos;: &apos;Dalit Buddhist movement &apos;, &apos;ReleaseTheMemo&apos;: &apos;cry for the right and Trump supporters&apos;, &apos;harrase&apos;: &apos;harass&apos;, &apos;Barracoon&apos;: &apos;Black slave&apos;, &apos;Castrater&apos;: &apos;castration&apos;, &apos;castrater&apos;: &apos;castration&apos;, &apos;Rapistan&apos;: &apos;Pakistan rapist&apos;, &apos;rapistan&apos;: &apos;Pakistan rapist&apos;, &apos;Turkified&apos;: &apos;Turkification&apos;, &apos;turkified&apos;: &apos;Turkification&apos;, &apos;Dumbassistan&apos;: &apos;dumb ass Pakistan&apos;, &apos;facetards&apos;: &apos;Facebook retards&apos;, &apos;rapefugees&apos;: &apos;rapist refugee&apos;, &apos;superficious&apos;: &apos;superficial&apos;, # extra from kagglers &apos;colour&apos;: &apos;color&apos;, &apos;centre&apos;: &apos;center&apos;, &apos;favourite&apos;: &apos;favorite&apos;, &apos;travelling&apos;: &apos;traveling&apos;, &apos;counselling&apos;: &apos;counseling&apos;, &apos;theatre&apos;: &apos;theater&apos;, &apos;cancelled&apos;: &apos;canceled&apos;, &apos;labour&apos;: &apos;labor&apos;, &apos;organisation&apos;: &apos;organization&apos;, &apos;wwii&apos;: &apos;world war 2&apos;, &apos;citicise&apos;: &apos;criticize&apos;, &apos;youtu &apos;: &apos;youtube &apos;, &apos;sallary&apos;: &apos;salary&apos;, &apos;Whta&apos;: &apos;What&apos;, &apos;narcisist&apos;: &apos;narcissist&apos;, &apos;narcissit&apos;: &apos;narcissist&apos;, &apos;howdo&apos;: &apos;how do&apos;, &apos;whatare&apos;: &apos;what are&apos;, &apos;howcan&apos;: &apos;how can&apos;, &apos;howmuch&apos;: &apos;how much&apos;, &apos;howmany&apos;: &apos;how many&apos;, &apos;whydo&apos;: &apos;why do&apos;, &apos;doI&apos;: &apos;do I&apos;, &apos;theBest&apos;: &apos;the best&apos;, &apos;howdoes&apos;: &apos;how does&apos;, &apos;mastrubation&apos;: &apos;masturbation&apos;, &apos;mastrubate&apos;: &apos;masturbate&apos;, &apos;mastrubating&apos;: &apos;masturbating&apos;, &apos;pennis&apos;: &apos;penis&apos;, &apos;Etherium&apos;: &apos;Ethereum&apos;, &apos;bigdata&apos;: &apos;big data&apos;, &apos;2k17&apos;: &apos;2017&apos;, &apos;2k18&apos;: &apos;2018&apos;, &apos;qouta&apos;: &apos;quota&apos;, &apos;exboyfriend&apos;: &apos;ex boyfriend&apos;, &apos;airhostess&apos;: &apos;air hostess&apos;, &apos;whst&apos;: &apos;what&apos;, &apos;watsapp&apos;: &apos;whatsapp&apos;, # extra &apos;bodyshame&apos;: &apos;body shaming&apos;, &apos;bodyshoppers&apos;: &apos;body shopping&apos;, &apos;bodycams&apos;: &apos;body cams&apos;, &apos;Cananybody&apos;: &apos;Can any body&apos;, &apos;deadbody&apos;: &apos;dead body&apos;, &apos;deaddict&apos;: &apos;de addict&apos;, &apos;Northindian&apos;: &apos;North Indian &apos;, &apos;northindian&apos;: &apos;north Indian &apos;, &apos;northkorea&apos;: &apos;North Korea&apos;, &apos;Whykorean&apos;: &apos;Why Korean&apos;, &apos;koreaboo&apos;: &apos;Korea boo &apos;, &apos;Brexshit&apos;: &apos;British Exit bullshit&apos;, &apos;shithole&apos;: &apos; shithole &apos;, &apos;shitpost&apos;: &apos;shit post&apos;, &apos;shitslam&apos;: &apos;shit Islam&apos;, &apos;shitlords&apos;: &apos;shit lords&apos;, &apos;Fck&apos;: &apos;Fuck&apos;, &apos;fck&apos;: &apos;fuck&apos;, &apos;Clickbait&apos;: &apos;click bait &apos;, &apos;clickbait&apos;: &apos;click bait &apos;, &apos;mailbait&apos;: &apos;mail bait&apos;, &apos;healhtcare&apos;: &apos;healthcare&apos;, &apos;trollbots&apos;: &apos;troll bots&apos;, &apos;trollled&apos;: &apos;trolled&apos;, &apos;trollimg&apos;: &apos;trolling&apos;, &apos;cybertrolling&apos;: &apos;cyber trolling&apos;, &apos;sickular&apos;: &apos;India sick secular &apos;, &apos;suckimg&apos;: &apos;sucking&apos;, &apos;Idiotism&apos;: &apos;idiotism&apos;, &apos;Niggerism&apos;: &apos;Nigger&apos;, &apos;Niggeriah&apos;: &apos;Nigger&apos; &#125; misspell_re = re.compile(&apos;(%s)&apos; % &apos;|&apos;.join(misspell_to_sub.keys())) def _replace(match): &quot;&quot;&quot; reference: https://www.kaggle.com/hengzheng/attention-capsule-why-not-both-lb-0-694 # noqa &quot;&quot;&quot; try: word = misspell_to_sub.get(match.group(0)) except KeyError: word = match.group(0) print(&apos;!!Error: Could Not Find Key: &#123;&#125;&apos;.format(word)) return word return misspell_re.sub(_replace, text)def spacing_misspell(text): &quot;&quot;&quot; &apos;deadbody&apos; -&gt; &apos;dead body&apos; &quot;&quot;&quot; misspell_list = [ &apos;(F|f)uck&apos;, &apos;Trump&apos;, &apos;\\W(A|a)nti&apos;, &apos;(W|w)hy&apos;, &apos;(W|w)hat&apos;, &apos;How&apos;, &apos;care\\W&apos;, &apos;\\Wover&apos;, &apos;gender&apos;, &apos;people&apos;, ] misspell_re = re.compile(&apos;(%s)&apos; % &apos;|&apos;.join(misspell_list)) return misspell_re.sub(r&quot; \\1 &quot;, text)def clean_latex(text): &quot;&quot;&quot; convert r&quot;[math]\\vec&#123;x&#125; + \\vec&#123;y&#125;&quot; to English &quot;&quot;&quot; # edge case text = re.sub(r&apos;\\[math\\]&apos;, &apos; LaTex math &apos;, text) text = re.sub(r&apos;\\[\\/math\\]&apos;, &apos; LaTex math &apos;, text) text = re.sub(r&apos;\\\\&apos;, &apos; LaTex &apos;, text) pattern_to_sub = &#123; r&apos;\\\\mathrm&apos;: &apos; LaTex math mode &apos;, r&apos;\\\\mathbb&apos;: &apos; LaTex math mode &apos;, r&apos;\\\\boxed&apos;: &apos; LaTex equation &apos;, r&apos;\\\\begin&apos;: &apos; LaTex equation &apos;, r&apos;\\\\end&apos;: &apos; LaTex equation &apos;, r&apos;\\\\left&apos;: &apos; LaTex equation &apos;, r&apos;\\\\right&apos;: &apos; LaTex equation &apos;, r&apos;\\\\(over|under)brace&apos;: &apos; LaTex equation &apos;, r&apos;\\\\text&apos;: &apos; LaTex equation &apos;, r&apos;\\\\vec&apos;: &apos; vector &apos;, r&apos;\\\\var&apos;: &apos; variable &apos;, r&apos;\\\\theta&apos;: &apos; theta &apos;, r&apos;\\\\mu&apos;: &apos; average &apos;, r&apos;\\\\min&apos;: &apos; minimum &apos;, r&apos;\\\\max&apos;: &apos; maximum &apos;, r&apos;\\\\sum&apos;: &apos; + &apos;, r&apos;\\\\times&apos;: &apos; * &apos;, r&apos;\\\\cdot&apos;: &apos; * &apos;, r&apos;\\\\hat&apos;: &apos; ^ &apos;, r&apos;\\\\frac&apos;: &apos; / &apos;, r&apos;\\\\div&apos;: &apos; / &apos;, r&apos;\\\\sin&apos;: &apos; Sine &apos;, r&apos;\\\\cos&apos;: &apos; Cosine &apos;, r&apos;\\\\tan&apos;: &apos; Tangent &apos;, r&apos;\\\\infty&apos;: &apos; infinity &apos;, r&apos;\\\\int&apos;: &apos; integer &apos;, r&apos;\\\\in&apos;: &apos; in &apos;, &#125; # post process for look up pattern_dict = &#123;k.strip(&apos;\\\\&apos;): v for k, v in pattern_to_sub.items()&#125; # init re patterns = pattern_to_sub.keys() pattern_re = re.compile(&apos;(%s)&apos; % &apos;|&apos;.join(patterns)) def _replace(match): &quot;&quot;&quot; reference: https://www.kaggle.com/hengzheng/attention-capsule-why-not-both-lb-0-694 # noqa &quot;&quot;&quot; try: word = pattern_dict.get(match.group(0).strip(&apos;\\\\&apos;)) except KeyError: word = match.group(0) print(&apos;!!Error: Could Not Find Key: &#123;&#125;&apos;.format(word)) return word return pattern_re.sub(_replace, text)def normalize_unicode(text): &quot;&quot;&quot; unicode string normalization &quot;&quot;&quot; return unicodedata.normalize(&apos;NFKD&apos;, text)def remove_newline(text): &quot;&quot;&quot; remove \\n and \\t &quot;&quot;&quot; text = re.sub(&apos;\\n&apos;, &apos; &apos;, text) text = re.sub(&apos;\\t&apos;, &apos; &apos;, text) text = re.sub(&apos;\\b&apos;, &apos; &apos;, text) text = re.sub(&apos;\\r&apos;, &apos; &apos;, text) return textdef decontracted(text): &quot;&quot;&quot; de-contract the contraction &quot;&quot;&quot; # specific text = re.sub(r&quot;(W|w)on(\\&apos;|\\’)t&quot;, &quot;will not&quot;, text) text = re.sub(r&quot;(C|c)an(\\&apos;|\\’)t&quot;, &quot;can not&quot;, text) text = re.sub(r&quot;(Y|y)(\\&apos;|\\’)all&quot;, &quot;you all&quot;, text) text = re.sub(r&quot;(Y|y)a(\\&apos;|\\’)ll&quot;, &quot;you all&quot;, text) # general text = re.sub(r&quot;(I|i)(\\&apos;|\\’)m&quot;, &quot;i am&quot;, text) text = re.sub(r&quot;(A|a)in(\\&apos;|\\’)t&quot;, &quot;is not&quot;, text) text = re.sub(r&quot;n(\\&apos;|\\’)t&quot;, &quot; not&quot;, text) text = re.sub(r&quot;(\\&apos;|\\’)re&quot;, &quot; are&quot;, text) text = re.sub(r&quot;(\\&apos;|\\’)s&quot;, &quot; is&quot;, text) text = re.sub(r&quot;(\\&apos;|\\’)d&quot;, &quot; would&quot;, text) text = re.sub(r&quot;(\\&apos;|\\’)ll&quot;, &quot; will&quot;, text) text = re.sub(r&quot;(\\&apos;|\\’)t&quot;, &quot; not&quot;, text) text = re.sub(r&quot;(\\&apos;|\\’)ve&quot;, &quot; have&quot;, text) return textdef spacing_punctuation(text): &quot;&quot;&quot; add space before and after punctuation and symbols &quot;&quot;&quot; regular_punct = list(string.punctuation) extra_punct = [ &apos;,&apos;, &apos;.&apos;, &apos;&quot;&apos;, &apos;:&apos;, &apos;)&apos;, &apos;(&apos;, &apos;-&apos;, &apos;!&apos;, &apos;?&apos;, &apos;|&apos;, &apos;;&apos;, &quot;&apos;&quot;, &apos;$&apos;, &apos;&amp;&apos;, &apos;/&apos;, &apos;[&apos;, &apos;]&apos;, &apos;&gt;&apos;, &apos;%&apos;, &apos;=&apos;, &apos;#&apos;, &apos;*&apos;, &apos;+&apos;, &apos;\\\\&apos;, &apos;•&apos;, &apos;~&apos;, &apos;@&apos;, &apos;£&apos;, &apos;·&apos;, &apos;_&apos;, &apos;&#123;&apos;, &apos;&#125;&apos;, &apos;©&apos;, &apos;^&apos;, &apos;®&apos;, &apos;`&apos;, &apos;&lt;&apos;, &apos;→&apos;, &apos;°&apos;, &apos;€&apos;, &apos;™&apos;, &apos;›&apos;, &apos;♥&apos;, &apos;←&apos;, &apos;×&apos;, &apos;§&apos;, &apos;″&apos;, &apos;′&apos;, &apos;Â&apos;, &apos;█&apos;, &apos;½&apos;, &apos;à&apos;, &apos;…&apos;, &apos;“&apos;, &apos;★&apos;, &apos;”&apos;, &apos;–&apos;, &apos;●&apos;, &apos;â&apos;, &apos;►&apos;, &apos;−&apos;, &apos;¢&apos;, &apos;²&apos;, &apos;¬&apos;, &apos;░&apos;, &apos;¶&apos;, &apos;↑&apos;, &apos;±&apos;, &apos;¿&apos;, &apos;▾&apos;, &apos;═&apos;, &apos;¦&apos;, &apos;║&apos;, &apos;―&apos;, &apos;¥&apos;, &apos;▓&apos;, &apos;—&apos;, &apos;‹&apos;, &apos;─&apos;, &apos;▒&apos;, &apos;：&apos;, &apos;¼&apos;, &apos;⊕&apos;, &apos;▼&apos;, &apos;▪&apos;, &apos;†&apos;, &apos;■&apos;, &apos;’&apos;, &apos;▀&apos;, &apos;¨&apos;, &apos;▄&apos;, &apos;♫&apos;, &apos;☆&apos;, &apos;é&apos;, &apos;¯&apos;, &apos;♦&apos;, &apos;¤&apos;, &apos;▲&apos;, &apos;è&apos;, &apos;¸&apos;, &apos;¾&apos;, &apos;Ã&apos;, &apos;⋅&apos;, &apos;‘&apos;, &apos;∞&apos;, &apos;∙&apos;, &apos;）&apos;, &apos;↓&apos;, &apos;、&apos;, &apos;│&apos;, &apos;（&apos;, &apos;»&apos;, &apos;，&apos;, &apos;♪&apos;, &apos;╩&apos;, &apos;╚&apos;, &apos;³&apos;, &apos;・&apos;, &apos;╦&apos;, &apos;╣&apos;, &apos;╔&apos;, &apos;╗&apos;, &apos;▬&apos;, &apos;❤&apos;, &apos;ï&apos;, &apos;Ø&apos;, &apos;¹&apos;, &apos;≤&apos;, &apos;‡&apos;, &apos;√&apos;, &apos;«&apos;, &apos;»&apos;, &apos;´&apos;, &apos;º&apos;, &apos;¾&apos;, &apos;¡&apos;, &apos;§&apos;, &apos;£&apos;, &apos;₤&apos;] all_punct = &apos;&apos;.join(sorted(list(set(regular_punct + extra_punct)))) re_tok = re.compile(f&apos;([&#123;all_punct&#125;])&apos;) return re_tok.sub(r&apos; \\1 &apos;, text)def spacing_digit(text): &quot;&quot;&quot; add space before and after digits &quot;&quot;&quot; re_tok = re.compile(&apos;([0-9])&apos;) return re_tok.sub(r&apos; \\1 &apos;, text)def spacing_number(text): &quot;&quot;&quot; add space before and after numbers &quot;&quot;&quot; re_tok = re.compile(&apos;([0-9]&#123;1,&#125;)&apos;) return re_tok.sub(r&apos; \\1 &apos;, text)def remove_number(text): &quot;&quot;&quot; numbers are not toxic &quot;&quot;&quot; return re.sub(&apos;\\d+&apos;, &apos; &apos;, text)def remove_space(text): &quot;&quot;&quot; remove extra spaces and ending space if any &quot;&quot;&quot; text = re.sub(&apos;\\s+&apos;, &apos; &apos;, text) text = re.sub(&apos;\\s+$&apos;, &apos;&apos;, text) return text&quot;&quot;&quot;tokenizer&quot;&quot;&quot;def preprocess(text, remove_num=True): &quot;&quot;&quot; preprocess text into clean text for tokenization NOTE: 1. glove supports uppper case words 2. glove supports digit 3. glove supports punctuation 5. glove supports domains e.g. www.apple.com 6. glove supports misspelled words e.g. FUCKKK &quot;&quot;&quot; # # 1. normalize # text = normalize_unicode(text) # # 2. remove new line # text = remove_newline(text) # 3. de-contract text = decontracted(text) # 4. clean misspell text = clean_misspell(text) # 5. space misspell text = spacing_misspell(text) # 6. clean_latex text = clean_latex(text) # 7. space text = spacing_punctuation(text) # 8. handle number if remove_num: text = remove_number(text) else: text = spacing_digit(text) # 9. remove space text = remove_space(text) return text 调用preprocess(text) 就好，返回处理完后的文本","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"文本预处理","slug":"文本预处理","permalink":"http://renxingkai.github.io/tags/文本预处理/"}],"author":"CinKate"},{"title":"nltk---词性标注","slug":"nltk-postag","date":"2019-03-29T09:06:25.000Z","updated":"2020-05-17T16:11:59.994Z","comments":true,"path":"2019/03/29/nltk-postag/","link":"","permalink":"http://renxingkai.github.io/2019/03/29/nltk-postag/","excerpt":"","text":"1.POS标签器推荐使用nltk推荐的pos_tag()函数，基于Penn Treebank，以下代码展示了使用nltk获取句子POS标签的方法： sentence = &apos;The brown fox is quick and he is jumping over the lazy dog&apos;# recommended tagger based on PTBimport nltktokens = nltk.word_tokenize(sentence)tagged_sent = nltk.pos_tag(tokens, tagset=&apos;universal&apos;)print (tagged_sent) 输出： [(&apos;The&apos;, &apos;DET&apos;), (&apos;brown&apos;, &apos;ADJ&apos;), (&apos;fox&apos;, &apos;NOUN&apos;), (&apos;is&apos;, &apos;VERB&apos;), (&apos;quick&apos;, &apos;ADJ&apos;), (&apos;and&apos;, &apos;CONJ&apos;), (&apos;he&apos;, &apos;PRON&apos;), (&apos;is&apos;, &apos;VERB&apos;), (&apos;jumping&apos;, &apos;VERB&apos;), (&apos;over&apos;, &apos;ADP&apos;), (&apos;the&apos;, &apos;DET&apos;), (&apos;lazy&apos;, &apos;ADJ&apos;), (&apos;dog&apos;, &apos;NOUN&apos;)] 2.建立自己的POS标签器准备数据：# preparing the datafrom nltk.corpus import treebankdata = treebank.tagged_sents()train_data = data[:3500]test_data = data[3500:]print (train_data[0]) 输出：[(&apos;Pierre&apos;, &apos;NNP&apos;), (&apos;Vinken&apos;, &apos;NNP&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;61&apos;, &apos;CD&apos;), (&apos;years&apos;, &apos;NNS&apos;), (&apos;old&apos;, &apos;JJ&apos;), (&apos;,&apos;, &apos;,&apos;), (&apos;will&apos;, &apos;MD&apos;), (&apos;join&apos;, &apos;VB&apos;), (&apos;the&apos;, &apos;DT&apos;), (&apos;board&apos;, &apos;NN&apos;), (&apos;as&apos;, &apos;IN&apos;), (&apos;a&apos;, &apos;DT&apos;), (&apos;nonexecutive&apos;, &apos;JJ&apos;), (&apos;director&apos;, &apos;NN&apos;), (&apos;Nov.&apos;, &apos;NNP&apos;), (&apos;29&apos;, &apos;CD&apos;), (&apos;.&apos;, &apos;.&apos;)] 2.1DefaultTagger默认标签器首先我们试下从SequentialBackoffTagger基类继承的DefaultTagger，并为每个单词分配相同的用户输入POS标签。 # default taggerfrom nltk.tag import DefaultTaggerdt = DefaultTagger(&apos;NN&apos;)print(dt.evaluate(test_data))print(dt.tag(tokens)) 输出： 0.1454158195372253[(&apos;The&apos;, &apos;NN&apos;), (&apos;brown&apos;, &apos;NN&apos;), (&apos;fox&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;NN&apos;), (&apos;quick&apos;, &apos;NN&apos;), (&apos;and&apos;, &apos;NN&apos;), (&apos;he&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;NN&apos;), (&apos;jumping&apos;, &apos;NN&apos;), (&apos;over&apos;, &apos;NN&apos;), (&apos;the&apos;, &apos;NN&apos;), (&apos;lazy&apos;, &apos;NN&apos;), (&apos;dog&apos;, &apos;NN&apos;)] 默认获得了14%的准确率，由于给标签器输入的都是相同的标签（‘NN’），因此输出标签获得的都是名词。 2.2RegexpTagger正则表达式标签器# regex taggerfrom nltk.tag import RegexpTagger# define regex tag patternspatterns = [ (r&apos;.*ing$&apos;, &apos;VBG&apos;), # gerunds (r&apos;.*ed$&apos;, &apos;VBD&apos;), # simple past (r&apos;.*es$&apos;, &apos;VBZ&apos;), # 3rd singular present (r&apos;.*ould$&apos;, &apos;MD&apos;), # modals (r&apos;.*\\&apos;s$&apos;, &apos;NN$&apos;), # possessive nouns (r&apos;.*s$&apos;, &apos;NNS&apos;), # plural nouns (r&apos;^-?[0-9]+(.[0-9]+)?$&apos;, &apos;CD&apos;), # cardinal numbers (r&apos;.*&apos;, &apos;NN&apos;) # nouns (default) ... ]rt = RegexpTagger(patterns)print(rt.evaluate(test_data))print(rt.tag(tokens)) 输出： 0.24039113176493368[(&apos;The&apos;, &apos;NN&apos;), (&apos;brown&apos;, &apos;NN&apos;), (&apos;fox&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;NNS&apos;), (&apos;quick&apos;, &apos;NN&apos;), (&apos;and&apos;, &apos;NN&apos;), (&apos;he&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;NNS&apos;), (&apos;jumping&apos;, &apos;VBG&apos;), (&apos;over&apos;, &apos;NN&apos;), (&apos;the&apos;, &apos;NN&apos;), (&apos;lazy&apos;, &apos;NN&apos;), (&apos;dog&apos;, &apos;NN&apos;)] 准确率提高到了24%，还是有效果的~ 2.3一、二、三元标签器## N gram taggersfrom nltk.tag import UnigramTaggerfrom nltk.tag import BigramTaggerfrom nltk.tag import TrigramTaggerut = UnigramTagger(train_data)bt = BigramTagger(train_data)tt = TrigramTagger(train_data)print(ut.evaluate(test_data))print(ut.tag(tokens))print(bt.evaluate(test_data))print(bt.tag(tokens))print (tt.evaluate(test_data))print(tt.tag(tokens)) 输出： 0.8607803272340013[(&apos;The&apos;, &apos;DT&apos;), (&apos;brown&apos;, None), (&apos;fox&apos;, None), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;quick&apos;, &apos;JJ&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;he&apos;, &apos;PRP&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;jumping&apos;, &apos;VBG&apos;), (&apos;over&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;DT&apos;), (&apos;lazy&apos;, None), (&apos;dog&apos;, None)]0.13466937748087907[(&apos;The&apos;, &apos;DT&apos;), (&apos;brown&apos;, None), (&apos;fox&apos;, None), (&apos;is&apos;, None), (&apos;quick&apos;, None), (&apos;and&apos;, None), (&apos;he&apos;, None), (&apos;is&apos;, None), (&apos;jumping&apos;, None), (&apos;over&apos;, None), (&apos;the&apos;, None), (&apos;lazy&apos;, None), (&apos;dog&apos;, None)]0.08064672281924679[(&apos;The&apos;, &apos;DT&apos;), (&apos;brown&apos;, None), (&apos;fox&apos;, None), (&apos;is&apos;, None), (&apos;quick&apos;, None), (&apos;and&apos;, None), (&apos;he&apos;, None), (&apos;is&apos;, None), (&apos;jumping&apos;, None), (&apos;over&apos;, None), (&apos;the&apos;, None), (&apos;lazy&apos;, None), (&apos;dog&apos;, None)] 发现一元的准确率最高，达到了86%,二、三元准确率低的原因可能是在训练数据中观察到的二元词组和三元词组不一定会在测试数据中以相同的方式出现。 2.4包含标签列表的组合标签器及使用backoff标签器本质上，我们将创建一个标签器链，对于每一个标签器，吐过他不能标记输入的标识，则标签器的下一步将会回退到backoff标签器： def combined_tagger(train_data, taggers, backoff=None): for tagger in taggers: backoff = tagger(train_data, backoff=backoff) return backoff#backoff to regtaggerct = combined_tagger(train_data=train_data, taggers=[UnigramTagger, BigramTagger, TrigramTagger], backoff=rt)print(ct.evaluate(test_data)) print(ct.tag(tokens)) 输出： 0.9094781682641108[(&apos;The&apos;, &apos;DT&apos;), (&apos;brown&apos;, &apos;NN&apos;), (&apos;fox&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;quick&apos;, &apos;JJ&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;he&apos;, &apos;PRP&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;jumping&apos;, &apos;VBG&apos;), (&apos;over&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;DT&apos;), (&apos;lazy&apos;, &apos;NN&apos;), (&apos;dog&apos;, &apos;NN&apos;)] 准确率到了90% 2.5ClassifierBasedPOSTagger标签器(有监督分类算法)使用ClassifierBasedPOSTagger类中的classifier_builder参数中的有监督机器学习算法来训练标签器。 from nltk.classify import NaiveBayesClassifier, MaxentClassifierfrom nltk.tag.sequential import ClassifierBasedPOSTaggernbt = ClassifierBasedPOSTagger(train=train_data, classifier_builder=NaiveBayesClassifier.train)print(nbt.evaluate(test_data))print(nbt.tag(tokens)) 输出： 0.9306806079969019[(&apos;The&apos;, &apos;DT&apos;), (&apos;brown&apos;, &apos;JJ&apos;), (&apos;fox&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;quick&apos;, &apos;JJ&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;he&apos;, &apos;PRP&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;jumping&apos;, &apos;VBG&apos;), (&apos;over&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;DT&apos;), (&apos;lazy&apos;, &apos;JJ&apos;), (&apos;dog&apos;, &apos;VBG&apos;)] 有监督准确率达到了0.93 2.6Just For Fun!(MaxentClassifier)# try this out for fun!met = ClassifierBasedPOSTagger(train=train_data, classifier_builder=MaxentClassifier.train)print(met.evaluate(test_data)) print(met.tag(tokens)) 输出： ==&gt; Training (100 iterations) Iteration Log Likelihood Accuracy --------------------------------------- 1 -3.82864 0.007 2 -0.76176 0.957 Final nan 0.9840.9269048310581857[(&apos;The&apos;, &apos;DT&apos;), (&apos;brown&apos;, &apos;NN&apos;), (&apos;fox&apos;, &apos;NN&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;quick&apos;, &apos;JJ&apos;), (&apos;and&apos;, &apos;CC&apos;), (&apos;he&apos;, &apos;PRP&apos;), (&apos;is&apos;, &apos;VBZ&apos;), (&apos;jumping&apos;, &apos;VBG&apos;), (&apos;over&apos;, &apos;IN&apos;), (&apos;the&apos;, &apos;DT&apos;), (&apos;lazy&apos;, &apos;NN&apos;), (&apos;dog&apos;, &apos;NN&apos;)] MaxentClassifier准确率达到了0.92","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"文本预处理","slug":"文本预处理","permalink":"http://renxingkai.github.io/tags/文本预处理/"},{"name":"词性标注","slug":"词性标注","permalink":"http://renxingkai.github.io/tags/词性标注/"}],"author":"CinKate"},{"title":"nltk---分词与文本预处理","slug":"nltk-tokenize","date":"2019-03-28T15:08:16.000Z","updated":"2020-05-17T16:12:00.073Z","comments":true,"path":"2019/03/28/nltk-tokenize/","link":"","permalink":"http://renxingkai.github.io/2019/03/28/nltk-tokenize/","excerpt":"","text":"参考《text-analytics-with-python》中的第三章中的处理和理解文本对nltk等常用nlp包进行总结，以供之后复习与使用~ 1.tokenize(切分词(句子))首先，标识(token)是具有一定的句法语义且独立的最小文本成分， 1.1句子切分句子切分基本技术包括在句子之间寻找特定的分割符，例如句号(‘.’)，换行符(‘\\n’)或者分号(‘;’)等。在nltk中，主要关注以下句子切分器: nltk.sent_tokenize(默认句子切分器) nltk.tokenize.PunktSentenceTokenizer() nltk.tokenize.RegexpTokenizer()以下直接上代码： import nltkfrom nltk.corpus import gutenbergfrom pprint import pprint#载入语料alice=gutenberg.raw(fileids=&apos;carroll-alice.txt&apos;)sample_text = &apos;We will discuss briefly about the basic syntax,\\ structure and design philosophies. \\ There is a defined hierarchical syntax for Python code which you should remember \\ when writing code! Python is a really powerful programming language!&apos;# Total characters in Alice in Wonderlandprint(len(alice))# First 100 characters in the corpusprint(alice[0:100]) 输出： 144395[Alice&apos;s Adventures in Wonderland by Lewis Carroll 1865]CHAPTER I. Down the Rabbit-HoleAlice was 1.1.1默认分词器–nltk.sent_tokenize#默认分词器default_st = nltk.sent_tokenizealice_sentences = default_st(text=alice)sample_sentences = default_st(text=sample_text)print(&apos;Total sentences in sample_text:&apos;, len(sample_sentences))print(&apos;Sample text sentences :-&apos;)pprint(sample_sentences)print(&apos;\\nTotal sentences in alice:&apos;, len(alice_sentences))print(&apos;First 5 sentences in alice:-&apos;)pprint(alice_sentences[0:5]) 输出： Total sentences in sample_text: 3Sample text sentences :-[&apos;We will discuss briefly about the basic syntax, structure and design &apos; &apos;philosophies.&apos;, &apos;There is a defined hierarchical syntax for Python code which you should &apos; &apos;remember when writing code!&apos;, &apos;Python is a really powerful programming language!&apos;]Total sentences in alice: 1625First 5 sentences in alice:-[&quot;[Alice&apos;s Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.&quot;, &apos;Down the Rabbit-Hole\\n&apos; &apos;\\n&apos; &apos;Alice was beginning to get very tired of sitting by her sister on the\\n&apos; &apos;bank, and of having nothing to do: once or twice she had peeped into the\\n&apos; &apos;book her sister was reading, but it had no pictures or conversations in\\n&apos; &quot;it, &apos;and what is the use of a book,&apos; thought Alice &apos;without pictures or\\n&quot; &quot;conversation?&apos;&quot;, &apos;So she was considering in her own mind (as well as she could, for the\\n&apos; &apos;hot day made her feel very sleepy and stupid), whether the pleasure\\n&apos; &apos;of making a daisy-chain would be worth the trouble of getting up and\\n&apos; &apos;picking the daisies, when suddenly a White Rabbit with pink eyes ran\\n&apos; &apos;close by her.&apos;, &apos;There was nothing so VERY remarkable in that; nor did Alice think it so\\n&apos; &quot;VERY much out of the way to hear the Rabbit say to itself, &apos;Oh dear!&quot;, &apos;Oh dear!&apos;] 德语切分 ## 其他语言句子切分器from nltk.corpus import europarl_raw#德语german_text = europarl_raw.german.raw(fileids=&apos;ep-00-01-17.de&apos;)# 语料中的词数print(len(german_text))# 前100字符print(german_text[0:100]) 输出： 157171 Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit 使用默认分词器切分德语# 默认句子切分器german_sentences_def = default_st(text=german_text, language=&apos;german&apos;)# loading german text tokenizer into a PunktSentenceTokenizer instance german_tokenizer = nltk.data.load(resource_url=&apos;tokenizers/punkt/german.pickle&apos;)german_sentences = german_tokenizer.tokenize(german_text)# verify the type of german_tokenizer# should be PunktSentenceTokenizerprint(type(german_tokenizer))# check if results of both tokenizers match# should be Trueprint(german_sentences_def == german_sentences)# print first 5 sentences of the corpusfor sent in german_sentences[0:5]: print(sent) 输出: &lt;class &apos;nltk.tokenize.punkt.PunktSentenceTokenizer&apos;&gt;True Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .Wie Sie feststellen konnten , ist der gefürchtete &quot; Millenium-Bug &quot; nicht eingetreten .Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken . 1.1.2使用PunktSentenceTokenizer## using PunktSentenceTokenizer for sentence tokenizationpunkt_st = nltk.tokenize.PunktSentenceTokenizer()sample_sentences = punkt_st.tokenize(sample_text)pprint(sample_sentences) 输出: [&apos;We will discuss briefly about the basic syntax, structure and design &apos; &apos;philosophies.&apos;, &apos;There is a defined hierarchical syntax for Python code which you should &apos; &apos;remember when writing code!&apos;, &apos;Python is a really powerful programming language!&apos;] 1.1.3使用RegexpTokenizer#使用正则表达式做句子切分## using RegexpTokenizer for sentence tokenizationSENTENCE_TOKENS_PATTERN = r&apos;(?&lt;!\\w\\.\\w.)(?&lt;![A-Z][a-z]\\.)(?&lt;![A-Z]\\.)(?&lt;=\\.|\\?|\\!)\\s&apos;regex_st = nltk.tokenize.RegexpTokenizer( pattern=SENTENCE_TOKENS_PATTERN, gaps=True)sample_sentences = regex_st.tokenize(sample_text)pprint(sample_sentences) 输出： [&apos;We will discuss briefly about the basic syntax, structure and design &apos; &apos;philosophies.&apos;, &apos; There is a defined hierarchical syntax for Python code which you should &apos; &apos;remember when writing code!&apos;, &apos;Python is a really powerful programming language!&apos;] 1.2词语切分1.2.1默认分词器nltk.word_tokenize## 分词sentence = &quot;The brown fox wasn&apos;t that quick and he couldn&apos;t win the race&quot;# default word tokenizerdefault_wt = nltk.word_tokenizewords = default_wt(sentence)print(words) 输出： [&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;was&apos;, &quot;n&apos;t&quot;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &apos;could&apos;, &quot;n&apos;t&quot;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 1.2.2Treebank分词器# treebank word tokenizertreebank_wt = nltk.TreebankWordTokenizer()words = treebank_wt.tokenize(sentence)print(words) 输出： [&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;was&apos;, &quot;n&apos;t&quot;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &apos;could&apos;, &quot;n&apos;t&quot;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 1.2.3正则分词器RegexpTokenizer# 正则切分TOKEN_PATTERN = r&apos;\\w+&apos; regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN, gaps=False)words = regex_wt.tokenize(sentence)print(words) 输出： [&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;wasn&apos;, &apos;t&apos;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &apos;couldn&apos;, &apos;t&apos;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 设置正则模式: GAP_PATTERN = r&apos;\\s+&apos; regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN, gaps=True)words = regex_wt.tokenize(sentence)print(words) 输出： [&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &quot;wasn&apos;t&quot;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &quot;couldn&apos;t&quot;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 分词索引word_indices = list(regex_wt.span_tokenize(sentence))print(word_indices)print([sentence[start:end] for start, end in word_indices]) 输出：[(0, 3), (4, 9), (10, 13), (14, 20), (21, 25), (26, 31), (32, 35), (36, 38), (39, 47), (48, 51), (52, 55), (56, 60)][&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &quot;wasn&apos;t&quot;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &quot;couldn&apos;t&quot;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 1.2.4WordPunctTokenizer分词器# derived regex tokenizers(派生类执行分词)wordpunkt_wt = nltk.WordPunctTokenizer()words = wordpunkt_wt.tokenize(sentence)print(words) 输出： [&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;wasn&apos;, &quot;&apos;&quot;, &apos;t&apos;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &apos;couldn&apos;, &quot;&apos;&quot;, &apos;t&apos;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 1.2.5WhitespaceTokenizer分词器#WhitespaceTokenizer基于诸如缩进符、换行符及空格的空白字符将句子分割成单词whitespace_wt = nltk.WhitespaceTokenizer()words = whitespace_wt.tokenize(sentence)print(words) 输出： [&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &quot;wasn&apos;t&quot;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &quot;couldn&apos;t&quot;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;] 2.文本规范化文本规范化定义为这样的一一个过程，它包含一系列步骤， 依次是转换、清洗以及将文本数据标准化成可供NLP、分析系统和应用程序使用的格式。通常，文本切分本身也是文本规范化的一部分。除了文本切分以外，还有各种其他技术，包括文本清洗、大小写转换、词语校正、停用词删除、词干提取和词形还原。文本规范化也常常称为文本清洗或转换。 本节将讨论在文本规范化过程中使用的各种技术。在探索各种技术之前，请使用以下代码段来加载基本的依存关系以及将使用的语料库: import nltkimport reimport stringfrom pprint import pprintcorpus = [&quot;The brown fox wasn&apos;t that quick and he couldn&apos;t win the race&quot;, &quot;Hey that&apos;s a great deal! I just bought a phone for $199&quot;, &quot;@@You&apos;ll (learn) a **lot** in the book. Python is an amazing language!@@&quot;] 2.2文本清洗可以使用nltk中的clean_html()函数，或者BeautifulSoup库来解析HTML数据，还可以使用自定义的逻辑，包括正则表达式、xpath和lxml库来解析XML数据。 2.3文本切分def tokenize_text(text): sentences = nltk.sent_tokenize(text) word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences] return word_tokens token_list = [tokenize_text(text) for text in corpus]print(token_list) 输出： [[[&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;was&apos;, &quot;n&apos;t&quot;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &apos;could&apos;, &quot;n&apos;t&quot;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;]], [[&apos;Hey&apos;, &apos;that&apos;, &quot;&apos;s&quot;, &apos;a&apos;, &apos;great&apos;, &apos;deal&apos;, &apos;!&apos;], [&apos;I&apos;, &apos;just&apos;, &apos;bought&apos;, &apos;a&apos;, &apos;phone&apos;, &apos;for&apos;, &apos;$&apos;, &apos;199&apos;]], [[&apos;@&apos;, &apos;@&apos;, &apos;You&apos;, &quot;&apos;ll&quot;, &apos;(&apos;, &apos;learn&apos;, &apos;)&apos;, &apos;a&apos;, &apos;**lot**&apos;, &apos;in&apos;, &apos;the&apos;, &apos;book&apos;, &apos;.&apos;], [&apos;Python&apos;, &apos;is&apos;, &apos;an&apos;, &apos;amazing&apos;, &apos;language&apos;, &apos;!&apos;], [&apos;@&apos;, &apos;@&apos;]]] 2.4删除特殊字符在分词后删除特殊字符def remove_characters_after_tokenization(tokens): pattern = re.compile(&apos;[&#123;&#125;]&apos;.format(re.escape(string.punctuation))) filtered_tokens = [pattern.sub(&apos;&apos;, token) for token in tokens] return filtered_tokens filtered_list_1 = [[remove_characters_after_tokenization(tokens) for tokens in sentence_tokens] for sentence_tokens in token_list]pprint(filtered_list_1) 输出： [[[&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;was&apos;, &apos;nt&apos;, &apos;that&apos;, &apos;quick&apos;, &apos;and&apos;, &apos;he&apos;, &apos;could&apos;, &apos;nt&apos;, &apos;win&apos;, &apos;the&apos;, &apos;race&apos;]], [[&apos;Hey&apos;, &apos;that&apos;, &apos;s&apos;, &apos;a&apos;, &apos;great&apos;, &apos;deal&apos;, &apos;&apos;], [&apos;I&apos;, &apos;just&apos;, &apos;bought&apos;, &apos;a&apos;, &apos;phone&apos;, &apos;for&apos;, &apos;&apos;, &apos;199&apos;]], [[&apos;&apos;, &apos;&apos;, &apos;You&apos;, &apos;ll&apos;, &apos;&apos;, &apos;learn&apos;, &apos;&apos;, &apos;a&apos;, &apos;lot&apos;, &apos;in&apos;, &apos;the&apos;, &apos;book&apos;, &apos;&apos;], [&apos;Python&apos;, &apos;is&apos;, &apos;an&apos;, &apos;amazing&apos;, &apos;language&apos;, &apos;&apos;], [&apos;&apos;, &apos;&apos;]]] 在分词前删除特殊字符def remove_characters_before_tokenization(sentence, keep_apostrophes=False): sentence = sentence.strip() if keep_apostrophes: PATTERN = r&apos;[?|$|&amp;|*|%|@|(|)|~]&apos; filtered_sentence = re.sub(PATTERN, r&apos;&apos;, sentence) else: PATTERN = r&apos;[^a-zA-Z0-9 ]&apos; filtered_sentence = re.sub(PATTERN, r&apos;&apos;, sentence) return filtered_sentence filtered_list_2 = [remove_characters_before_tokenization(sentence) for sentence in corpus] print(filtered_list_2)cleaned_corpus = [remove_characters_before_tokenization(sentence, keep_apostrophes=True) for sentence in corpus]print(cleaned_corpus) 输出： [&apos;The brown fox wasnt that quick and he couldnt win the race&apos;, &apos;Hey thats a great deal I just bought a phone for 199&apos;, &apos;Youll learn a lot in the book Python is an amazing language&apos;][&quot;The brown fox wasn&apos;t that quick and he couldn&apos;t win the race&quot;, &quot;Hey that&apos;s a great deal! I just bought a phone for 199&quot;, &quot;You&apos;ll learn a lot in the book. Python is an amazing language!&quot;] 2.5扩展缩写词将is’nt 还原为is not等等… from contractions import contractions_dictdef expand_contractions(sentence, contraction_mapping): contractions_pattern = re.compile(&apos;(&#123;&#125;)&apos;.format(&apos;|&apos;.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL) def expand_match(contraction): match = contraction.group(0) first_char = match[0] expanded_contraction = contraction_mapping.get(match)\\ if contraction_mapping.get(match)\\ else contraction_mapping.get(match.lower()) expanded_contraction = first_char+expanded_contraction[1:] return expanded_contraction expanded_sentence = contractions_pattern.sub(expand_match, sentence) return expanded_sentence expanded_corpus = [expand_contractions(sentence, contractions_dict) for sentence in cleaned_corpus] print(expanded_corpus) 输出： [&apos;The brown fox was not that quick and he could not win the race&apos;, &apos;Hey that is a great deal! I just bought a phone for 199&apos;, &apos;You will learn a lot in the book. Python is an amazing language!&apos;] 2.6大小写转换# case conversion print(corpus[0].lower())print(corpus[0].upper()) 输出：the brown fox wasn&apos;t that quick and he couldn&apos;t win the raceTHE BROWN FOX WASN&apos;T THAT QUICK AND HE COULDN&apos;T WIN THE RACE 2.7删除停用词# removing stopwordsdef remove_stopwords(tokens): stopword_list = nltk.corpus.stopwords.words(&apos;english&apos;) filtered_tokens = [token for token in tokens if token not in stopword_list] return filtered_tokens expanded_corpus_tokens = [tokenize_text(text) for text in expanded_corpus] filtered_list_3 = [[remove_stopwords(tokens) for tokens in sentence_tokens] for sentence_tokens in expanded_corpus_tokens]print(filtered_list_3) 输出: [[[&apos;The&apos;, &apos;brown&apos;, &apos;fox&apos;, &apos;quick&apos;, &apos;could&apos;, &apos;win&apos;, &apos;race&apos;]], [[&apos;Hey&apos;, &apos;great&apos;, &apos;deal&apos;, &apos;!&apos;], [&apos;I&apos;, &apos;bought&apos;, &apos;phone&apos;, &apos;199&apos;]], [[&apos;You&apos;, &apos;learn&apos;, &apos;lot&apos;, &apos;book&apos;, &apos;.&apos;], [&apos;Python&apos;, &apos;amazing&apos;, &apos;language&apos;, &apos;!&apos;]]] 2.8词语校正删除重复的字符# removing repeated characterssample_sentence = &apos;My schooool is realllllyyy amaaazingggg&apos;sample_sentence_tokens = tokenize_text(sample_sentence)[0]from nltk.corpus import wordnetdef remove_repeated_characters(tokens): repeat_pattern = re.compile(r&apos;(\\w*)(\\w)\\2(\\w*)&apos;) match_substitution = r&apos;\\1\\2\\3&apos; def replace(old_word): if wordnet.synsets(old_word): return old_word new_word = repeat_pattern.sub(match_substitution, old_word) return replace(new_word) if new_word != old_word else new_word correct_tokens = [replace(word) for word in tokens] return correct_tokensprint(remove_repeated_characters(sample_sentence_tokens)) 输出:[&apos;My&apos;, &apos;school&apos;, &apos;is&apos;, &apos;really&apos;, &apos;amazing&apos;] 2.9词干提取2.9.1Port词干提取器# porter stemmerfrom nltk.stem import PorterStemmerps = PorterStemmer()print(ps.stem(&apos;jumping&apos;), ps.stem(&apos;jumps&apos;), ps.stem(&apos;jumped&apos;))print(ps.stem(&apos;lying&apos;))print(ps.stem(&apos;strange&apos;)) 输出： jump jump jumpliestrang 2.9.2LancasterStemmer词干提取器# lancaster stemmerfrom nltk.stem import LancasterStemmerls = LancasterStemmer()print(ls.stem(&apos;jumping&apos;), ls.stem(&apos;jumps&apos;), ls.stem(&apos;jumped&apos;))print (ls.stem(&apos;lying&apos;))print (ls.stem(&apos;strange&apos;)) 输出： jump jump jumplyingstrange 2.9.3RegexpStemmer正则词干提取器# regex stemmerfrom nltk.stem import RegexpStemmerrs = RegexpStemmer(&apos;ing$|s$|ed$&apos;, min=4)print( rs.stem(&apos;jumping&apos;), rs.stem(&apos;jumps&apos;), rs.stem(&apos;jumped&apos;))print (rs.stem(&apos;lying&apos;))print (rs.stem(&apos;strange&apos;)) 输出： jump jump jumplystrange 2.9.4SnowballStemmer词干提取器# snowball stemmerfrom nltk.stem import SnowballStemmerss = SnowballStemmer(&quot;german&quot;)print (&apos;Supported Languages:&apos;, SnowballStemmer.languages)# autobahnen -&gt; cars# autobahn -&gt; carss.stem(&apos;autobahnen&apos;)# springen -&gt; jumping# spring -&gt; jumpss.stem(&apos;springen&apos;) 输出： Supported Languages: (&apos;arabic&apos;, &apos;danish&apos;, &apos;dutch&apos;, &apos;english&apos;, &apos;finnish&apos;, &apos;french&apos;, &apos;german&apos;, &apos;hungarian&apos;, &apos;italian&apos;, &apos;norwegian&apos;, &apos;porter&apos;, &apos;portuguese&apos;, &apos;romanian&apos;, &apos;russian&apos;, &apos;spanish&apos;, &apos;swedish&apos;)Out[14]:&apos;spring&apos; 2.10词形还原# lemmatizationfrom nltk.stem import WordNetLemmatizerwnl = WordNetLemmatizer()# lemmatize nounsprint( wnl.lemmatize(&apos;cars&apos;, &apos;n&apos;))print (wnl.lemmatize(&apos;men&apos;, &apos;n&apos;))# lemmatize verbsprint (wnl.lemmatize(&apos;running&apos;, &apos;v&apos;))print (wnl.lemmatize(&apos;ate&apos;, &apos;v&apos;))# lemmatize adjectivesprint (wnl.lemmatize(&apos;saddest&apos;, &apos;a&apos;))print (wnl.lemmatize(&apos;fancier&apos;, &apos;a&apos;))# ineffective lemmatizationprint (wnl.lemmatize(&apos;ate&apos;, &apos;n&apos;))print (wnl.lemmatize(&apos;fancier&apos;, &apos;v&apos;)) 输出： carmenruneatsadfancyatefancier","categories":[{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"}],"tags":[{"name":"文本预处理","slug":"文本预处理","permalink":"http://renxingkai.github.io/tags/文本预处理/"},{"name":"分词","slug":"分词","permalink":"http://renxingkai.github.io/tags/分词/"}],"author":"CinKate"},{"title":"《活着》读后感","slug":"huozheduhougan","date":"2019-03-25T15:01:09.000Z","updated":"2020-05-17T16:11:59.457Z","comments":true,"path":"2019/03/25/huozheduhougan/","link":"","permalink":"http://renxingkai.github.io/2019/03/25/huozheduhougan/","excerpt":"","text":"之前在本科的时候只看过《活着》这部电影（葛优、巩俐主演），电影里已经很惨了，当时虽然看了把两遍电影，但是最大的感受是：虽然这部电影名为“活着”，可却不停地有人离开，也对福贵的悲痛人生感到痛惜，看着自己的亲人，一个个，一个个离开自己，世上也孤零零仅剩自己一个人，那种悲伤之情真的难以承受。 最近，在微信读书上看原著，可能由于年龄的增加，经历的事多了，以及作者的绝妙文笔，感觉读起来的画面感，不亚于看一部精彩的电影，甚至了超过了表演形式。 原著中，福贵的生平更惨，电影中他的孙子还能陪他一起在世上，原著中却真的只有他一个人走到了最后，亲手埋下了自己的儿子、女儿、妻子、女婿、孙子……从最初的悲恸不已，到后来的“心情平淡”，或许生活的残酷已经将这个男人摧残的遍体鳞伤，但他仍然坚强地活着，攒了两年钱却买了一头已然年暮的老牛，因为他知道活着的不易，他珍惜活着，他珍惜每一天。 可惜生活能留给他的，也仅剩了活着。“少年去游荡，中年想掘藏，老年做和尚。”经历了最惨痛的事情，才能看淡生活吧。 真的希望，活着的人能好好活着，每天清晨迎接初日，傍晚送走晚霞，日复一日，如此安好~ 二零一九年三月二十五日 于岳麓山下","categories":[{"name":"读书有感","slug":"读书有感","permalink":"http://renxingkai.github.io/categories/读书有感/"}],"tags":[{"name":"活着","slug":"活着","permalink":"http://renxingkai.github.io/tags/活着/"}],"author":"CinKate"},{"title":"Keras踩坑总结","slug":"kerestricks","date":"2019-03-23T17:04:49.000Z","updated":"2020-05-17T16:11:59.830Z","comments":true,"path":"2019/03/24/kerestricks/","link":"","permalink":"http://renxingkai.github.io/2019/03/24/kerestricks/","excerpt":"","text":"转载自：链接 Keras 是一个用 Python 编写的高级神经网络 API，它能够以 TensorFlow, CNTK, 或者 Theano 作为后端运行。Keras 的开发重点是支持快速的实验。能够以最小的时间把你的想法转换为实验结果，是做好研究的关键。本人是keras的忠实粉丝，可能是因为它实在是太简单易用了，不用多少代码就可以将自己的想法完全实现，但是在使用的过程中还是遇到了不少坑，本文做了一个归纳，供大家参考。 Keras 兼容的 Python 版本: Python 2.7-3.6。 详细教程请参阅Keras官方中文文档 1、Keras输出的loss，val这些值如何保存到文本中去：Keras中的fit函数会返回一个History对象，它的History.history属性会把之前的那些值全保存在里面，如果有验证集的话，也包含了验证集的这些指标变化情况，具体写法： hist=model.fit(train_set_x,train_set_y,batch_size=256,shuffle=True,nb_epoch=nb_epoch,validation_split=0.1)with open(&apos;log_sgd_big_32.txt&apos;,&apos;w&apos;) as f: f.write(str(hist.history)) 我觉得保存之前的loss，val这些值还是比较重要的，在之后的调参过程中有时候还是需要之前loss的结果作为参考的，特别是你自己添加了一些自己的loss的情况下，但是这样的写法会使整个文本的取名比较乱，所以其实可以考虑使用Aetros的插件，Aetros网址，这是一个基于Keras的一个管理工具，可以可视化你的网络结构，中间卷积结果的可视化，以及保存你以往跑的所有结果，还是很方便的，就是有些不稳定，有时候会崩。。。 history对象包含两个重要属性：epoch：训练的轮数history：它是一个字典，包含val_loss,val_acc,loss,acc四个key。 2、关于训练集，验证集和测试集：其实一开始我也没搞清楚这个问题，拿着测试集当验证集用，其实验证集是从训练集中抽取出来用于调参的，而测试集是和训练集无交集的，用于测试所选参数用于该模型的效果的，这个还是不要弄错了。。。在Keras中，验证集的划分只要在fit函数里设置validation_split的值就好了，这个对应了取训练集中百分之几的数据出来当做验证集。但由于shuffle是在validation _split之后执行的，所以如果一开始训练集没有shuffle的话，有可能使验证集全是负样本。测试集的使用只要在evaluate函数里设置就好了。 print model.evaluate（test_set_x，test_set_y ,batch_size=256） 这里注意evaluate和fit函数的默认batch_size都是32，自己记得修改。 总结： 验证集是在fit的时候通过validation_split参数自己从训练集中划分出来的； 测试集需要专门的使用evaluate去进行评价。 3、关于优化方法使用的问题之学习率调整开始总会纠结哪个优化方法好用，但是最好的办法就是试，无数次尝试后不难发现，Sgd的这种学习率非自适应的优化方法，调整学习率和初始化的方法会使它的结果有很大不同，但是由于收敛确实不快，总感觉不是很方便，我觉得之前一直使用Sgd的原因一方面是因为优化方法不多，其次是用Sgd都能有这么好的结果，说明你网络该有多好啊。其他的Adam，Adade，RMSprop结果都差不多，Nadam因为是adam的动量添加的版本，在收敛效果上会更出色。所以如果对结果不满意的话，就把这些方法换着来一遍吧。 （1）方法一：通过LearningRateScheduler实现学习率调整有很多初学者人会好奇怎么使sgd的学习率动态的变化，其实Keras里有个反馈函数叫LearningRateScheduler，具体使用如下： #使学习率指数下降def step_decay(epoch): initial_lrate = 0.01 drop = 0.5 epochs_drop = 10.0 lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop)) return lratelrate = LearningRateScheduler(step_decay)sgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False)model.fit(train_set_x, train_set_y, validation_split=0.1, nb_epoch=200, batch_size=256, callbacks=[lrate]) （2）方式二：最直接的调整学习率方式当然也可以直接在sgd声明函数中修改参数来直接修改学习率，学习率变化如下图： sgd = SGD(lr=learning_rate, decay=learning_rate/nb_epoch, momentum=0.9, nesterov=True) 具体可以参考这篇文章Using Learning Rate Schedules for Deep Learning Models in Python with Keras 除此之外，还有一种学利率调整方式，即 （3）方法三：通过ReduceLROnPlateau调整学习率keras.callbacks.ReduceLROnPlateau(monitor=&apos;val_loss&apos;, factor=0.1, patience=10, verbose=0, mode=&apos;auto&apos;, epsilon=0.0001, cooldown=0, min_lr=0) 当评价指标不在提升时，减少学习率。当学习停滞时，减少2倍或10倍的学习率常常能获得较好的效果。该回调函数检测指标的情况，如果在patience个epoch中看不到模型性能提升，则减少学习率 参数 monitor：被监测的量factor：每次减少学习率的因子，学习率将以lr = lr*factor的形式被减少patience：当patience个epoch过去而模型性能不提升时，学习率减少的动作会被触发mode：‘auto’，‘min’，‘max’之一，在min模式下，如果检测值触发学习率减少。在max模式下，当检测值不再上升则触发学习率减少。epsilon：阈值，用来确定是否进入检测值的“平原区”cooldown：学习率减少后，会经过cooldown个epoch才重新进行正常操作min_lr：学习率的下限 代码示例如下： from keras.callbacks import ReduceLROnPlateaureduce_lr = ReduceLROnPlateau(monitor=&apos;val_loss&apos;, patience=10, mode=&apos;auto&apos;)model.fit(train_x, train_y, batch_size=32, epochs=5, validation_split=0.1, callbacks=[reduce_lr]) 4、如何用 Keras 处理超过内存的数据集？你可以使用 model.train_on_batch(x，y) 和 model.test_on_batch(x，y) 进行批量训练与测试。请参阅 模型文档。 或者，你可以编写一个生成批处理训练数据的生成器，然后使用 model.fit_generator(data_generator，steps_per_epoch，epochs) 方法。 5、Batchnormalization层的放置问题：BN层是真的吊，简直神器，除了会使网络搭建的时间和每个epoch的时间延长一点之外，但是关于这个问题我看到了无数的说法，对于卷积和池化层的放法，又说放中间的，也有说池化层后面的，对于dropout层，有说放在它后面的，也有说放在它前面的，对于这个问题我的说法还是试！虽然麻烦。。。但是DL本来不就是一个偏工程性的学科吗。。。还有一点是需要注意的，就是BN层的参数问题，我一开始也没有注意到，仔细看BN层的参数： keras.layers.normalization.BatchNormalization(epsilon=1e-06, mode=0, axis=-1, momentum=0.9, weights=None, beta_init=&apos;zero&apos;, gamma_init=&apos;one&apos;) 参数mode：整数，指定规范化的模式，取0或10：按特征规范化，输入的各个特征图将独立被规范化。规范化的轴由参数axis指定。注意，如果输入是形如（samples，channels，rows，cols）的4D图像张量，则应设置规范化的轴为1，即沿着通道轴规范化。输入格式是‘tf’同理。1：按样本规范化，该模式默认输入为2D 我们大都使用的都是mode=0也就是按特征规范化，对于放置在卷积和池化之间或之后的4D张量，需要设置axis=1，而Dense层之后的BN层则直接使用默认值就好了。 6、在验证集的误差不再下降时，如何中断训练？你可以使用 EarlyStopping 回调： from keras.callbacks import EarlyStoppingearly_stopping = EarlyStopping(monitor=&apos;val_loss&apos;, patience=2)model.fit(x, y, validation_split=0.2, callbacks=[early_stopping]) 总结：关于callbacks参数的妙用 （1）查询每隔epoch之后的loss和acc （2）通过LearningRateScheduler实现衰减学习率或自定义衰减学习率 （3）通过EarlyStopping实现中断训练 （4）我们还可以自己定义回调函数，所为回调函数其实就是在训练完每一个epoch之后我们希望实现的操作。 7.如何「冻结」网络层？「冻结」一个层意味着将其排除在训练之外，即其权重将永远不会更新。这在微调模型或使用固定的词向量进行文本输入中很有用。有两种方式实现： 方式一：在构造层的时候传递一个bool类型trainable参数，如下： frozen_layer = Dense(32, trainable=False) 您可以将 trainable 参数（布尔值）传递给一个层的构造器，以将该层设置为不可训练的： 方式二：通过层对象的trainable属性去设置，如下： x = Input(shape=(32,))layer = Dense(32) #构造一个层layer.trainable = False #设置层的trainable属性y = layer(x) 注意：可以在实例化之后将网络层的 trainable 属性设置为 True 或 False。为了使之生效，在修改 trainable 属性之后，需要在模型上调用 compile()。及重新编译模型。 8.如何从 Sequential 模型中移除一个层？你可以通过调用模型的 .pop() 来删除 Sequential 模型中最后添加的层： model = Sequential()model.add(Dense(32, activation=&apos;relu&apos;, input_dim=784))model.add(Dense(32, activation=&apos;relu&apos;))print(len(model.layers)) # &quot;2&quot;model.pop()print(len(model.layers)) # &quot;1&quot;","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/tags/深度学习/"},{"name":"keras","slug":"keras","permalink":"http://renxingkai.github.io/tags/keras/"}],"author":"CinKate"},{"title":"BIDAF代码阅读","slug":"bidaf","date":"2019-03-21T11:31:10.000Z","updated":"2020-05-17T16:11:59.217Z","comments":true,"path":"2019/03/21/bidaf/","link":"","permalink":"http://renxingkai.github.io/2019/03/21/bidaf/","excerpt":"","text":"1.assert的用法 ： 主要用于检查条件，不符合就终止程序a=-1#报错assert a&gt;0,&quot;a超出范围&quot;#正常运行assert a&lt;0 2. 打开文件codecs.open()解决不同文件的编码问题，会将文件内容转为unicodeimport codecs, sys# 用codecs提供的open方法来指定打开的文件的语言编码，它会在读 取的时候自动转换为内部unicode bfile = codecs.open( &quot; dddd.txt &quot; , &apos; r &apos; , &quot; big5 &quot; )# bfile = open(&quot;dddd.txt&quot;, &apos;r&apos;) ss = bfile.read()bfile.close()# 输出，这个时候看到的就是转换后的结果。如果使用语言内建的open函数 来打开文件，这里看到的必定是乱码 以下是prepro.py文件的代码阅读与分析：import spacyimport jsonfrom tqdm import tqdmfrom collections import Counterimport randomimport codecsimport numpy as npimport osimport tensorflow as tf#加载模型nlp=spacy.blank(&apos;en&apos;)#对句子进行分词def word_tokenize(sent): doc=nlp(sent) return [token.text for token in doc]#常用的word2idx#此处输出spans形式：[(0, 1), (2, 6), (7, 8), (8, 9), (9, 10), (11, 15), (16, 18)]#意为取出该词当前所在的位置，并且结束长度+当前长度#两者之差即为该单词长度def convert_idx(text,tokens): current=0 spans=[] for token in tokens: current=text.find(token,current) if current&lt;0: print(&apos;Token &#123;&#125; cannot be found!&apos;.format(token)) raise Exception() #[(0, 1), (2, 6), (7, 8), (8, 9), (9, 10), (11, 15), (16, 18)] #取出该词当前所在的位置，并且结束长度+当前长度 #两者之差即为该单词长度 spans.append((current,current+len(token))) current+=len(token) return spans#预处理文件def process_file(filename,data_type=None,word_counter=None,char_counter=None): print(&quot;Generating &#123;&#125; examples...&quot;.format(data_type)) examples = [] eval_examples = &#123;&#125; total=0 with open(filename,&apos;r&apos;) as fh: source=json.load(fh) print(len(source[&apos;data&apos;])) #遍历每篇文章dev有48篇文章 for article in tqdm(source[&quot;data&quot;]): #遍历每篇文章的段落 for para in article[&apos;paragraphs&apos;]: #替换段落中的&apos;&apos;和`` context = para[&quot;context&quot;].replace(&quot;&apos;&apos;&quot;, &apos;&quot; &apos;).replace(&quot;``&quot;, &apos;&quot; &apos;) #并对段落进行分词,分词中还是带了标点和特殊符号，需要后面进行处理 context_tokens=word_tokenize(context) #[&apos;The&apos;, &apos;connection&apos;, &apos;between&apos;, &apos;macroscopic&apos;, &apos;nonconservative&apos;, &apos;forces&apos;, &apos;and&apos;, &apos;microscopic&apos;, &apos;conservative&apos;, &apos;forces&apos;, &apos;is&apos;, &apos;described&apos;, &apos;by&apos;, &apos;detailed&apos;, &apos;treatment&apos;, &apos;with&apos;, &apos;statistical&apos;, &apos;mechanics&apos;, &apos;.&apos;, &apos;In&apos;, &apos;macroscopic&apos;, &apos;closed&apos;, &apos;systems&apos;, &apos;,&apos;, &apos;nonconservative&apos;, &apos;forces&apos;, &apos;act&apos;, &apos;to&apos;, &apos;change&apos;, &apos;the&apos;, &apos;internal&apos;, &apos;energies&apos;, &apos;of&apos;, &apos;the&apos;, &apos;system&apos;, &apos;,&apos;, &apos;and&apos;, &apos;are&apos;, &apos;often&apos;, &apos;associated&apos;, &apos;with&apos;, &apos;the&apos;, &apos;transfer&apos;, &apos;of&apos;, &apos;heat&apos;, &apos;.&apos;, &apos;According&apos;, &apos;to&apos;, &apos;the&apos;, &apos;Second&apos;, &apos;law&apos;, &apos;of&apos;, &apos;thermodynamics&apos;, &apos;,&apos;, &apos;nonconservative&apos;, &apos;forces&apos;, &apos;necessarily&apos;, &apos;result&apos;, &apos;in&apos;, &apos;energy&apos;, &apos;transformations&apos;, &apos;within&apos;, &apos;closed&apos;, &apos;systems&apos;, &apos;from&apos;, &apos;ordered&apos;, &apos;to&apos;, &apos;more&apos;, &apos;random&apos;, &apos;conditions&apos;, &apos;as&apos;, &apos;entropy&apos;, &apos;increases&apos;, &apos;.&apos;] #获取每个单词的字符表示 context_chars = [list(token) for token in context_tokens] #word2idx 每个词开始的位置和结束的位置 spans = convert_idx(context, context_tokens) for token in context_tokens: #这儿加的是每个qas的长度？？ word_counter[token] += len(para[&quot;qas&quot;]) for char in token: #每个单词的字符这儿也加的是每个qas的长度 #Counter(&#123;&apos;e&apos;: 28293, &apos;a&apos;: 19610, &apos;n&apos;: 17317, &apos;t&apos;: 17071, &apos;r&apos;: 15443, &apos;o&apos;: 15358, &apos;i&apos;: # 14669, &apos;s&apos;: 14081, &apos;h&apos;: 11839, &apos;l&apos;: 9031, &apos;d&apos;: 8982, &apos;c&apos;: 6540, &apos;u&apos;: 5885, # &apos;w&apos;: 5806, &apos;f&apos;: 4516, &apos;g&apos;: 4463, &apos;p&apos;: 4372, &apos;m&apos;: 4165, &apos;,&apos;: 3116, &apos;y&apos;: 2842, &apos;b&apos;: 2321, &apos;v&apos;: 2152, # &apos;.&apos;: 2057, &apos;B&apos;: 2052, &apos;S&apos;: 1832, &apos;1&apos;: 1776, &apos;k&apos;: 1553, &apos;0&apos;: 1168, &apos;C&apos;: 1107, &apos;F&apos;: 963, &apos;T&apos;: 876, # &apos;2&apos;: 856, &apos;P&apos;: 836, &apos;I&apos;: 819, &apos;5&apos;: 798, &apos;N&apos;: 766, &apos;L&apos;: 741, &apos;X&apos;: 714, &apos;M&apos;: 672, &apos;4&apos;: 662, &apos;3&apos;: 636, # &apos;A&apos;: 619, &apos;9&apos;: 584, &quot;&apos;&quot;: 552, &apos;-&apos;: 523, &apos;7&apos;: 488, &apos;D&apos;: 470, &apos;–&apos;: 415, &apos;(&apos;: 412, &apos;)&apos;: 412, &apos;8&apos;: 380, # &apos;6&apos;: 371, &apos;V&apos;: 352, &apos;O&apos;: 272, &apos;J&apos;: 268, &apos;j&apos;: 249, &apos;q&apos;: 235, &apos;&quot;&apos;: 222, &apos;G&apos;: 221, &apos;x&apos;: 220, &apos;E&apos;: 177, # &apos;R&apos;: 173, &apos;W&apos;: 168, &apos;K&apos;: 159, &apos;H&apos;: 117, &apos;U&apos;: 108, &apos;z&apos;: 107, &apos;½&apos;: 81, &apos;:&apos;: 81, &apos;;&apos;: 63, &apos;$&apos;: 49, &apos;#&apos;: 30, # &apos;é&apos;: 26, &apos;/&apos;: 21, &apos;Q&apos;: 15&#125;) char_counter[char] += len(para[&quot;qas&quot;]) #遍历qas for qa in para[&quot;qas&quot;]: total += 1 #替换问题&apos;&apos; `` ques = qa[&quot;question&quot;].replace( &quot;&apos;&apos;&quot;, &apos;&quot; &apos;).replace(&quot;``&quot;, &apos;&quot; &apos;) #对问题进行分词 ques_tokens = word_tokenize(ques) #取出问题中的字符 ques_chars = [list(token) for token in ques_tokens] #遍历问题每个词 for token in ques_tokens: #此处真的正确 word_counter[token] += 1 for char in token: char_counter[char] += 1 y1s, y2s = [], [] answer_texts = [] #遍历答案文本 for answer in qa[&quot;answers&quot;]: #答案文本 answer_text = answer[&quot;text&quot;] #开始位置 answer_start = answer[&apos;answer_start&apos;] answer_end = answer_start + len(answer_text) answer_texts.append(answer_text) answer_span = [] #加入答案span answer_span for idx, span in enumerate(spans): if not (answer_end &lt;= span[0] or answer_start &gt;= span[1]): answer_span.append(idx) y1, y2 = answer_span[0], answer_span[-1] y1s.append(y1) y2s.append(y2) example = &#123;&quot;context_tokens&quot;: context_tokens, &quot;context_chars&quot;: context_chars, &quot;ques_tokens&quot;: ques_tokens, &quot;ques_chars&quot;: ques_chars, &quot;y1s&quot;: y1s, &quot;y2s&quot;: y2s, &quot;id&quot;: total&#125; examples.append(example) #未分词结果 eval_examples[str(total)] = &#123; &quot;context&quot;: context, &quot;spans&quot;: spans, &quot;answers&quot;: answer_texts, &quot;uuid&quot;: qa[&quot;id&quot;]&#125; random.shuffle(examples) print(&quot;&#123;&#125; questions in total&quot;.format(len(examples))) return examples, eval_examples#获取词向量def get_embedding(counter, data_type, limit=-1, emb_file=None, size=None, vec_size=None, token2idx_dict=None): print(&quot;Generating &#123;&#125; embedding...&quot;.format(data_type)) embedding_dict=&#123;&#125; #过滤掉低频词，仅取出频率较高的词 filtered_elements=[k for k,v in counter.items() if v&gt;limit] #判断词向量文件是否为空 if emb_file is not None: assert size is not None#如果size为空直接退出程序 assert vec_size is not None#如果vec_size为空直接退出程序 #读取词向量 with codecs.open(emb_file, &quot;r&quot;, encoding=&quot;utf-8&quot;) as fh: # 依次遍历词向量每一行 for line in tqdm(fh, total=size): #分开词和向量 array = line.split() #取出开头的单词 word=&quot;&quot;.join(array[0:-vec_size]) #取出单词对应的词向量 vector=list(map(float,array[-vec_size:])) #词向量的单词在counter单词中，并且 在文本中的单词数目&gt;limit if word in counter and counter[word]&gt;limit: embedding_dict[word]=vector print(&quot;&#123;&#125; / &#123;&#125; tokens have corresponding &#123;&#125; embedding vector&quot;.format( len(embedding_dict), len(filtered_elements), data_type)) #如果词向量文件为空 else: assert vec_size is not None #遍历所有过滤的词 for token in filtered_elements: #对每个单词进行随机初始化向量 embedding_dict[token]=[np.random.normal(scale=0.01) for _ in range(vec_size)] print(&quot;&#123;&#125; tokens have corresponding embedding vector&quot;.format( len(filtered_elements))) #处理OOV词 NULL = &quot;--NULL--&quot; OOV = &quot;--OOV--&quot; #从下标2索引开始，过滤掉NULL和OOV 创建token2_idx_dict token2idx_dict=&#123;token:idx for idx,token in enumerate(embedding_dict.keys(),2)&#125; if token2idx_dict is None else token2idx_dict #NULL OOV 设置token2idx token2idx_dict[NULL] = 0 token2idx_dict[OOV] = 1 #NULL OOV设置embedding_dict embedding_dict[NULL] = [0. for _ in range(vec_size)] embedding_dict[OOV] = [0. for _ in range(vec_size)] #id2embedding 单词id对应的词向量 id2emb_dict=&#123;idx:embedding_dict[token] for token,idx in token2idx_dict.items() &#125; #获取词向量矩阵 emb_mat=[id2emb_dict[idx] for idx in range(id2emb_dict)] #仅返回 词向量矩阵，token2idx_dict return emb_mat, token2idx_dict#构建文本特征question paragraph answer and so ondef build_features(config, examples, data_type, out_file, word2idx_dict, char2idx_dict, is_test=False): #文章长度 para_limit=config.test_para_limit if is_test else config.para_limit #问题长度 ques_limit = config.test_ques_limit if is_test else config.ques_limit #字符限制长度 char_limit = config.char_limit #过滤文章和问题长度函数 def filter_func(example, is_test=False): return len(example[&quot;context_tokens&quot;]) &gt; para_limit or len(example[&quot;ques_tokens&quot;]) &gt; ques_limit print(&quot;Processing &#123;&#125; examples...&quot;.format(data_type)) writer = tf.python_io.TFRecordWriter(out_file) total = 0 total_ = 0 meta = &#123;&#125; #处理文章 for example in tqdm(examples): total_+=1 #过滤长度大于限制值的文章 if filter_func(example, is_test): continue total += 1 #段落ids context_idxs = np.zeros([para_limit], dtype=np.int32) #段落id char对应的矩阵 context_char_idxs = np.zeros([para_limit, char_limit], dtype=np.int32) ##问题ids ques_idxs = np.zeros([ques_limit], dtype=np.int32) ##问题id char对应的矩阵 ques_char_idxs = np.zeros([ques_limit, char_limit], dtype=np.int32) #段落长度 y1 = np.zeros([para_limit], dtype=np.float32) y2 = np.zeros([para_limit], dtype=np.float32) #获取单词 def _get_word(word): for each in (word, word.lower(), word.capitalize(), word.upper()): if each in word2idx_dict: #返回每个单词对应的id return word2idx_dict[each] return 1 #获取字符 def _get_char(char): if char in char2idx_dict: # 返回每个字符对应的id return char2idx_dict[char] return 1 #为每个文章内容获取对应的ids context_tokens为已经分好词的文章 for i, token in enumerate(example[&quot;context_tokens&quot;]): context_idxs[i] = _get_word(token) # 为每个问题内容获取对应的ids ques_tokens为已经分好词的问题 for i, token in enumerate(example[&quot;ques_tokens&quot;]): ques_idxs[i] = _get_word(token) # 为每个文章内容获取对应的chars for i, token in enumerate(example[&quot;context_chars&quot;]): for j, char in enumerate(token): #不能超出char的限制 if j == char_limit: break #赋值char 不够的用0填充 context_char_idxs[i, j] = _get_char(char) # 为每个问题内容获取对应的chars for i, token in enumerate(example[&quot;ques_chars&quot;]): for j, char in enumerate(token): #不能超出char的限制 if j == char_limit: break # 赋值char 不够的用0填充 ques_char_idxs[i, j] = _get_char(char) #开始，结束位置 start, end = example[&quot;y1s&quot;][-1], example[&quot;y2s&quot;][-1] y1[start], y2[end] = 1.0, 1.0 #构建tensorflow 记录 record = tf.train.Example(features=tf.train.Features(feature=&#123; &quot;context_idxs&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_idxs.tostring()])), &quot;ques_idxs&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_idxs.tostring()])), &quot;context_char_idxs&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_char_idxs.tostring()])), &quot;ques_char_idxs&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_char_idxs.tostring()])), &quot;y1&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[y1.tostring()])), &quot;y2&quot;: tf.train.Feature(bytes_list=tf.train.BytesList(value=[y2.tostring()])), &quot;id&quot;: tf.train.Feature(int64_list=tf.train.Int64List(value=[example[&quot;id&quot;]])) &#125;)) writer.write(record.SerializeToString()) print(&quot;Build &#123;&#125; / &#123;&#125; instances of features in total&quot;.format(total, total_)) meta[&quot;total&quot;] = total writer.close() return meta#保存文件def save(filename, obj, message=None): if message is not None: print(&quot;Saving &#123;&#125;...&quot;.format(message)) with open(filename, &quot;w&quot;) as fh: json.dump(obj, fh)# 预处理文件def prepro(config): #单词，字符计数器 word_counter, char_counter = Counter(), Counter() #处理训练集 train_examples, train_eval = process_file( config.train_file, &quot;train&quot;, word_counter, char_counter) #处理验证集 dev_examples, dev_eval = process_file( config.dev_file, &quot;dev&quot;, word_counter, char_counter) #处理测试集 test_examples, test_eval = process_file( config.test_file, &quot;test&quot;, word_counter, char_counter) #词向量文件 word_emb_file = config.fasttext_file if config.fasttext else config.glove_word_file #字符向量文件 char_emb_file = config.glove_char_file if config.pretrained_char else None #字符向量大小 char_emb_size = config.glove_char_size if config.pretrained_char else None #字符向量维度 char_emb_dim = config.glove_dim if config.pretrained_char else config.char_dim #word2idx字典 word2idx_dict = None #如果存在word2idx字典 则直接导入 if os.path.isfile(config.word2idx_file): with open(config.word2idx_file, &quot;r&quot;) as fh: word2idx_dict = json.load(fh) #构建词向量矩阵 word_emb_mat, word2idx_dict = get_embedding(word_counter, &quot;word&quot;, emb_file=word_emb_file, size=config.glove_word_size, vec_size=config.glove_dim, token2idx_dict=word2idx_dict) #构建字符向量矩阵 char2idx_dict = None # 如果存在char2idx字典 则直接导入 if os.path.isfile(config.char2idx_file): with open(config.char2idx_file, &quot;r&quot;) as fh: char2idx_dict = json.load(fh) # 构建字符向量矩阵 char_emb_mat, char2idx_dict = get_embedding( char_counter, &quot;char&quot;, emb_file=char_emb_file, size=char_emb_size, vec_size=char_emb_dim, token2idx_dict=char2idx_dict) #对训练集、验证集、测试集构建特征 build_features(config, train_examples, &quot;train&quot;, config.train_record_file, word2idx_dict, char2idx_dict) dev_meta = build_features(config, dev_examples, &quot;dev&quot;, config.dev_record_file, word2idx_dict, char2idx_dict) test_meta = build_features(config, test_examples, &quot;test&quot;, config.test_record_file, word2idx_dict, char2idx_dict, is_test=True) #对预处理的文件进行保存 save(config.word_emb_file, word_emb_mat, message=&quot;word embedding&quot;) save(config.char_emb_file, char_emb_mat, message=&quot;char embedding&quot;) save(config.train_eval_file, train_eval, message=&quot;train eval&quot;) save(config.dev_eval_file, dev_eval, message=&quot;dev eval&quot;) save(config.test_eval_file, test_eval, message=&quot;test eval&quot;) save(config.dev_meta, dev_meta, message=&quot;dev meta&quot;) save(config.word2idx_file, word2idx_dict, message=&quot;word2idx&quot;) save(config.char2idx_file, char2idx_dict, message=&quot;char2idx&quot;) save(config.test_meta, test_meta, message=&quot;test meta&quot;)","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/categories/深度学习/"}],"tags":[{"name":"阅读理解","slug":"阅读理解","permalink":"http://renxingkai.github.io/tags/阅读理解/"},{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/tags/深度学习/"}],"author":"CinKate"},{"title":"神经网络调参的一些tips","slug":"nntuningparameter","date":"2019-03-19T20:10:50.000Z","updated":"2020-05-17T16:12:00.003Z","comments":true,"path":"2019/03/20/nntuningparameter/","link":"","permalink":"http://renxingkai.github.io/2019/03/20/nntuningparameter/","excerpt":"","text":"参考建议调整超参数思想为控制变量法，并且按照学习率、批处理大小和隐藏层设计的顺序进行。以下是一些建议： 使用一个规模比较小的数据集一般都会把数据集分为训练集、测试集、验证集三份，训练集和验证集也被称为开发数据集，有的数据集不设验证集，是因为数据量小，通常可以用训练集调整超参数。 如果有验证集，验证集的数据不宜过多，因为数据越多，越需要多次迭代才能看到超参数的效果，所需要的时间就越长，在寻找一组最佳参数阶段，需要比较不同参数下损失变化的曲线和精度的值。 调整学习率控制其他超参数不变，改变学习率，比如从0.0001开始，顺序选择0.001、0.01、0.005、0.1和0.5，然后比较在不同的学习率下损失函数的曲线增长或减小的幅度，我们可以找到一个区间，也就是在这个区间内，损失函数的波形是稳定下降的，不会发生振荡，则取这个区间最小的值就可以。 调整batch_size控制其他超参数不变，改变批处理大小，可以依次选择20、50、100、200，然后比较在不同的批处理大小下能使准确率变化最陡的值，准确率变化越陡，证明参数学习收敛越快。 调整隐藏层设计控制其他超参数不变，改变隐藏层的层数或每层神经元的多少，选择能获得最高准确率的值。 超参数的调整主要还是需要自己做大量的实验，得出较好的“经验”，这样调起来会更得心应手一些~","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/categories/深度学习/"}],"tags":[{"name":"神经网络","slug":"神经网络","permalink":"http://renxingkai.github.io/tags/神经网络/"},{"name":"调参","slug":"调参","permalink":"http://renxingkai.github.io/tags/调参/"}],"author":"CinKate"},{"title":"第一篇博文","slug":"firstpage","date":"2019-03-19T14:30:40.000Z","updated":"2020-05-17T16:11:59.814Z","comments":true,"path":"2019/03/19/firstpage/","link":"","permalink":"http://renxingkai.github.io/2019/03/19/firstpage/","excerpt":"今天，是我对github.io的一次尝试，也是我第一次建立自己的博客，希望记录下学习、生活、成长的事迹，多学、多产出，其实更好~","text":"今天，是我对github.io的一次尝试，也是我第一次建立自己的博客，希望记录下学习、生活、成长的事迹，多学、多产出，其实更好~ 分享一首自己很喜欢的词： 扬州慢·淮左名都宋代：姜夔淳熙丙申至日，予过维扬。夜雪初霁，荠麦弥望。入其城，则四顾萧条，寒水自碧，暮色渐起，戍角悲吟。予怀怆然，感慨今昔，因自度此曲。千岩老人以为有“黍离”之悲也。 淮左名都，竹西佳处，解鞍少驻初程。过春风十里。尽荠麦青青。自胡马窥江去后，废池乔木，犹厌言兵。渐黄昏，清角吹寒。都在空城。杜郎俊赏，算而今、重到须惊。纵豆蔻词工，青楼梦好，难赋深情。二十四桥仍在，波心荡、冷月无声。念桥边红药，年年知为谁生。 接下来的日子，锻炼自己的耐力，Always learn from the dalao~坐看天边云卷云舒~","categories":[{"name":"杂谈","slug":"杂谈","permalink":"http://renxingkai.github.io/categories/杂谈/"}],"tags":[{"name":"第一次建博客","slug":"第一次建博客","permalink":"http://renxingkai.github.io/tags/第一次建博客/"},{"name":"That's a great day","slug":"That-s-a-great-day","permalink":"http://renxingkai.github.io/tags/That-s-a-great-day/"}],"author":"CinKate"}],"categories":[{"name":"文本生成","slug":"文本生成","permalink":"http://renxingkai.github.io/categories/文本生成/"},{"name":"图神经网络","slug":"图神经网络","permalink":"http://renxingkai.github.io/categories/图神经网络/"},{"name":"机器学习","slug":"机器学习","permalink":"http://renxingkai.github.io/categories/机器学习/"},{"name":"搜索","slug":"搜索","permalink":"http://renxingkai.github.io/categories/搜索/"},{"name":"后台学习","slug":"后台学习","permalink":"http://renxingkai.github.io/categories/后台学习/"},{"name":"自然语言处理","slug":"自然语言处理","permalink":"http://renxingkai.github.io/categories/自然语言处理/"},{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/categories/深度学习/"},{"name":"读书有感","slug":"读书有感","permalink":"http://renxingkai.github.io/categories/读书有感/"},{"name":"杂谈","slug":"杂谈","permalink":"http://renxingkai.github.io/categories/杂谈/"}],"tags":[{"name":"文本风格迁移","slug":"文本风格迁移","permalink":"http://renxingkai.github.io/tags/文本风格迁移/"},{"name":"GraphEmbedding","slug":"GraphEmbedding","permalink":"http://renxingkai.github.io/tags/GraphEmbedding/"},{"name":"评估指标","slug":"评估指标","permalink":"http://renxingkai.github.io/tags/评估指标/"},{"name":"search","slug":"search","permalink":"http://renxingkai.github.io/tags/search/"},{"name":"git","slug":"git","permalink":"http://renxingkai.github.io/tags/git/"},{"name":"docker","slug":"docker","permalink":"http://renxingkai.github.io/tags/docker/"},{"name":"词向量相关","slug":"词向量相关","permalink":"http://renxingkai.github.io/tags/词向量相关/"},{"name":"阅读理解","slug":"阅读理解","permalink":"http://renxingkai.github.io/tags/阅读理解/"},{"name":"关键词抽取","slug":"关键词抽取","permalink":"http://renxingkai.github.io/tags/关键词抽取/"},{"name":"深度学习","slug":"深度学习","permalink":"http://renxingkai.github.io/tags/深度学习/"},{"name":"文本预处理","slug":"文本预处理","permalink":"http://renxingkai.github.io/tags/文本预处理/"},{"name":"词性标注","slug":"词性标注","permalink":"http://renxingkai.github.io/tags/词性标注/"},{"name":"分词","slug":"分词","permalink":"http://renxingkai.github.io/tags/分词/"},{"name":"活着","slug":"活着","permalink":"http://renxingkai.github.io/tags/活着/"},{"name":"keras","slug":"keras","permalink":"http://renxingkai.github.io/tags/keras/"},{"name":"神经网络","slug":"神经网络","permalink":"http://renxingkai.github.io/tags/神经网络/"},{"name":"调参","slug":"调参","permalink":"http://renxingkai.github.io/tags/调参/"},{"name":"第一次建博客","slug":"第一次建博客","permalink":"http://renxingkai.github.io/tags/第一次建博客/"},{"name":"That's a great day","slug":"That-s-a-great-day","permalink":"http://renxingkai.github.io/tags/That-s-a-great-day/"}]}